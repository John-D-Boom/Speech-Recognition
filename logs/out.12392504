Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.2)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.248184351921082
  batch 100 loss: 3.178938603401184
  batch 150 loss: 3.0957636404037476
  batch 200 loss: 2.960883836746216
  batch 250 loss: 2.898955135345459
  batch 300 loss: 2.7134326171875
  batch 350 loss: 2.5514321660995485
  batch 400 loss: 2.5087995386123656
  batch 450 loss: 2.484430637359619
  batch 500 loss: 2.343005108833313
  batch 550 loss: 2.2780299496650698
  batch 600 loss: 2.1551477670669557
  batch 650 loss: 2.0819764018058775
  batch 700 loss: 2.231765041351318
  batch 750 loss: 2.031522626876831
  batch 800 loss: 1.9671851754188538
  batch 850 loss: 1.904916260242462
  batch 900 loss: 1.8736775279045106
LOSS train 1.87368 valid 1.84845, valid PER 70.29%
EPOCH 2:
  batch 50 loss: 1.8644779419898987
  batch 100 loss: 1.8087550616264343
  batch 150 loss: 1.7022443056106566
  batch 200 loss: 1.7443542432785035
  batch 250 loss: 1.7164420080184937
  batch 300 loss: 1.703977427482605
  batch 350 loss: 1.6761895203590393
  batch 400 loss: 1.6487426471710205
  batch 450 loss: 1.652364284992218
  batch 500 loss: 1.6101948857307433
  batch 550 loss: 1.5853535437583923
  batch 600 loss: 1.6036743330955505
  batch 650 loss: 1.5321571731567383
  batch 700 loss: 1.5835961246490478
  batch 750 loss: 1.5454953932762145
  batch 800 loss: 1.4928221487998963
  batch 850 loss: 1.5172894668579102
  batch 900 loss: 1.4697062969207764
LOSS train 1.46971 valid 1.44749, valid PER 49.75%
EPOCH 3:
  batch 50 loss: 1.4125312972068786
  batch 100 loss: 1.4808733177185058
  batch 150 loss: 1.465549716949463
  batch 200 loss: 1.3822166156768798
  batch 250 loss: 1.4346188998222351
  batch 300 loss: 1.430924472808838
  batch 350 loss: 1.4287246870994568
  batch 400 loss: 1.3911076283454895
  batch 450 loss: 1.3706578636169433
  batch 500 loss: 1.3520855832099914
  batch 550 loss: 1.3697106945514679
  batch 600 loss: 1.3151009202003479
  batch 650 loss: 1.3142786395549775
  batch 700 loss: 1.3288205862045288
  batch 750 loss: 1.3649869275093078
  batch 800 loss: 1.3489555287361146
  batch 850 loss: 1.324884569644928
  batch 900 loss: 1.316145691871643
LOSS train 1.31615 valid 1.26390, valid PER 41.76%
EPOCH 4:
  batch 50 loss: 1.3150007498264313
  batch 100 loss: 1.2522243237495423
  batch 150 loss: 1.27172389626503
  batch 200 loss: 1.264447340965271
  batch 250 loss: 1.2619838738441467
  batch 300 loss: 1.2614668345451354
  batch 350 loss: 1.2707636761665344
  batch 400 loss: 1.2231593680381776
  batch 450 loss: 1.2226904690265656
  batch 500 loss: 1.2986527049541474
  batch 550 loss: 1.2276240634918212
  batch 600 loss: 1.1818470561504364
  batch 650 loss: 1.2693194591999053
  batch 700 loss: 1.2870167243480681
  batch 750 loss: 1.2126065289974213
  batch 800 loss: 1.2297949957847596
  batch 850 loss: 1.20851567029953
  batch 900 loss: 1.2253559803962708
LOSS train 1.22536 valid 1.18288, valid PER 37.08%
EPOCH 5:
  batch 50 loss: 1.2062940204143524
  batch 100 loss: 1.1810845482349395
  batch 150 loss: 1.176820890903473
  batch 200 loss: 1.2140328013896942
  batch 250 loss: 1.181946622133255
  batch 300 loss: 1.2048409843444825
  batch 350 loss: 1.1741537654399872
  batch 400 loss: 1.161845347881317
  batch 450 loss: 1.141038364171982
  batch 500 loss: 1.138795223236084
  batch 550 loss: 1.175149167776108
  batch 600 loss: 1.194803249835968
  batch 650 loss: 1.17039635181427
  batch 700 loss: 1.1533749854564668
  batch 750 loss: 1.1335875880718231
  batch 800 loss: 1.1885131394863129
  batch 850 loss: 1.2132298600673677
  batch 900 loss: 1.1464795446395875
LOSS train 1.14648 valid 1.11959, valid PER 36.50%
EPOCH 6:
  batch 50 loss: 1.1104093325138091
  batch 100 loss: 1.1501798367500304
  batch 150 loss: 1.1144261014461518
  batch 200 loss: 1.085700523853302
  batch 250 loss: 1.1395781064033508
  batch 300 loss: 1.1497465705871581
  batch 350 loss: 1.1524450337886811
  batch 400 loss: 1.128403786420822
  batch 450 loss: 1.1334451234340668
  batch 500 loss: 1.0825296938419342
  batch 550 loss: 1.155573365688324
  batch 600 loss: 1.092327754497528
  batch 650 loss: 1.0780321621894837
  batch 700 loss: 1.0805987095832825
  batch 750 loss: 1.120502780675888
  batch 800 loss: 1.1364339971542359
  batch 850 loss: 1.1493149590492249
  batch 900 loss: 1.1358982050418853
LOSS train 1.13590 valid 1.12738, valid PER 35.96%
EPOCH 7:
  batch 50 loss: 1.0848252427577973
  batch 100 loss: 1.10876047372818
  batch 150 loss: 1.061426181793213
  batch 200 loss: 1.0815733826160432
  batch 250 loss: 1.113180102109909
  batch 300 loss: 1.0979885053634644
  batch 350 loss: 1.1142497038841248
  batch 400 loss: 1.0839841425418855
  batch 450 loss: 1.0921333754062652
  batch 500 loss: 1.0798630952835082
  batch 550 loss: 1.0594250524044038
  batch 600 loss: 1.0880780875682832
  batch 650 loss: 1.0214689075946808
  batch 700 loss: 1.1180791807174684
  batch 750 loss: 1.0690404403209686
  batch 800 loss: 1.0572990047931672
  batch 850 loss: 1.061153885126114
  batch 900 loss: 1.0461804676055908
LOSS train 1.04618 valid 1.14489, valid PER 35.68%
EPOCH 8:
  batch 50 loss: 1.0682279181480407
  batch 100 loss: 1.0239628553390503
  batch 150 loss: 1.0704675149917602
  batch 200 loss: 1.0356123435497284
  batch 250 loss: 1.0515933704376221
  batch 300 loss: 1.0194490039348603
  batch 350 loss: 1.0447938108444215
  batch 400 loss: 1.025810157060623
  batch 450 loss: 1.0584864556789397
  batch 500 loss: 1.039327790737152
  batch 550 loss: 1.0570501852035523
  batch 600 loss: 1.0285582888126372
  batch 650 loss: 1.0461600780487061
  batch 700 loss: 1.036502206325531
  batch 750 loss: 1.055528403520584
  batch 800 loss: 1.0621706461906433
  batch 850 loss: 1.0118130600452424
  batch 900 loss: 1.0310067248344421
LOSS train 1.03101 valid 1.05161, valid PER 32.67%
EPOCH 9:
  batch 50 loss: 0.9861470592021943
  batch 100 loss: 0.9663937842845917
  batch 150 loss: 0.9888562309741974
  batch 200 loss: 0.9494476056098938
  batch 250 loss: 0.9560937738418579
  batch 300 loss: 0.9810502016544342
  batch 350 loss: 0.9533071279525757
  batch 400 loss: 1.0297966980934143
  batch 450 loss: 1.0320938646793365
  batch 500 loss: 1.0120097386837006
  batch 550 loss: 1.0192080044746399
  batch 600 loss: 1.0557688689231872
  batch 650 loss: 1.0233798944950103
  batch 700 loss: 1.0209480345249176
  batch 750 loss: 1.0527808392047882
  batch 800 loss: 1.1284497046470643
  batch 850 loss: 1.0643489134311677
  batch 900 loss: 1.056575608253479
LOSS train 1.05658 valid 1.03573, valid PER 32.70%
EPOCH 10:
  batch 50 loss: 1.0173079991340637
  batch 100 loss: 1.0234460365772247
  batch 150 loss: 1.0566857969760894
  batch 200 loss: 0.9666635727882386
  batch 250 loss: 1.0278263521194457
  batch 300 loss: 1.0205359542369843
  batch 350 loss: 1.0291697883605957
  batch 400 loss: 0.9692599439620971
  batch 450 loss: 0.989236878156662
  batch 500 loss: 0.99340984582901
  batch 550 loss: 1.006910982131958
  batch 600 loss: 0.9655037105083466
  batch 650 loss: 1.0310011100769043
  batch 700 loss: 1.0163579201698303
  batch 750 loss: 1.023055168390274
  batch 800 loss: 1.0082920932769774
  batch 850 loss: 0.9975138211250305
  batch 900 loss: 0.9904971265792847
LOSS train 0.99050 valid 1.03321, valid PER 34.09%
EPOCH 11:
  batch 50 loss: 0.9564109921455384
  batch 100 loss: 0.9828799271583557
  batch 150 loss: 0.9576439428329467
  batch 200 loss: 0.9214869296550751
  batch 250 loss: 0.9559796047210694
  batch 300 loss: 0.9934928143024444
  batch 350 loss: 1.004827275276184
  batch 400 loss: 0.95223592877388
  batch 450 loss: 0.9644434142112732
  batch 500 loss: 0.9769880104064942
  batch 550 loss: 0.980253723859787
  batch 600 loss: 0.978614866733551
  batch 650 loss: 0.9900972652435303
  batch 700 loss: 1.0443621504306793
  batch 750 loss: 0.9705918526649475
  batch 800 loss: 0.9876793777942657
  batch 850 loss: 0.9590216267108918
  batch 900 loss: 0.9888054418563843
LOSS train 0.98881 valid 0.99322, valid PER 31.67%
EPOCH 12:
  batch 50 loss: 0.893603435754776
  batch 100 loss: 0.927573938369751
  batch 150 loss: 1.0182896113395692
  batch 200 loss: 0.9578412842750549
  batch 250 loss: 0.9537319970130921
  batch 300 loss: 1.0093619334697723
  batch 350 loss: 0.957089956998825
  batch 400 loss: 0.9696333992481232
  batch 450 loss: 0.9725822365283966
  batch 500 loss: 0.9944313144683838
  batch 550 loss: 0.9678950738906861
  batch 600 loss: 0.9562262785434723
  batch 650 loss: 0.9561260104179382
  batch 700 loss: 0.9363398563861847
  batch 750 loss: 0.9688082730770111
  batch 800 loss: 0.9398694705963134
  batch 850 loss: 0.9484973740577698
  batch 900 loss: 0.9571006965637207
LOSS train 0.95710 valid 0.99212, valid PER 31.35%
EPOCH 13:
  batch 50 loss: 0.9126125967502594
  batch 100 loss: 0.9166396391391755
  batch 150 loss: 0.9215192234516144
  batch 200 loss: 0.8979491794109344
  batch 250 loss: 0.9133679306507111
  batch 300 loss: 0.9408762454986572
  batch 350 loss: 0.9109284746646881
  batch 400 loss: 0.9671827340126038
  batch 450 loss: 0.962233647108078
  batch 500 loss: 0.934793484210968
  batch 550 loss: 0.9812334501743316
  batch 600 loss: 0.9447856199741363
  batch 650 loss: 0.9369437420368194
  batch 700 loss: 0.965393933057785
  batch 750 loss: 0.9447314965724946
  batch 800 loss: 0.971172672510147
  batch 850 loss: 0.9388738048076629
  batch 900 loss: 0.9416903924942016
LOSS train 0.94169 valid 0.97621, valid PER 30.85%
EPOCH 14:
  batch 50 loss: 0.9070203495025635
  batch 100 loss: 0.8729882872104645
  batch 150 loss: 0.8983440482616425
  batch 200 loss: 0.9085721909999848
  batch 250 loss: 0.8981835567951202
  batch 300 loss: 0.8927123844623566
  batch 350 loss: 0.9012464022636414
  batch 400 loss: 0.9548088467121124
  batch 450 loss: 0.8810060560703278
  batch 500 loss: 0.9746979451179505
  batch 550 loss: 0.9703682029247284
  batch 600 loss: 0.9622433543205261
  batch 650 loss: 0.9429429626464844
  batch 700 loss: 0.9609115219116211
  batch 750 loss: 0.943444412946701
  batch 800 loss: 0.932828677892685
  batch 850 loss: 0.9780129718780518
  batch 900 loss: 0.9244319880008698
LOSS train 0.92443 valid 0.99585, valid PER 31.40%
EPOCH 15:
  batch 50 loss: 0.8978134548664093
  batch 100 loss: 0.912596470117569
  batch 150 loss: 0.8878735935688019
  batch 200 loss: 0.9250289332866669
  batch 250 loss: 0.9145310699939728
  batch 300 loss: 0.8950876390933991
  batch 350 loss: 0.9013494503498077
  batch 400 loss: 0.9174986255168914
  batch 450 loss: 0.9143938291072845
  batch 500 loss: 0.8817022824287415
  batch 550 loss: 0.9168117463588714
  batch 600 loss: 0.9727920234203339
  batch 650 loss: 0.9220829832553864
  batch 700 loss: 0.9146456551551819
  batch 750 loss: 0.9327109551429749
  batch 800 loss: 0.9399786150455475
  batch 850 loss: 0.9090722525119781
  batch 900 loss: 0.898303837776184
LOSS train 0.89830 valid 1.01030, valid PER 32.14%
EPOCH 16:
  batch 50 loss: 0.8916738355159759
  batch 100 loss: 0.8589657008647918
  batch 150 loss: 0.8840425801277161
  batch 200 loss: 0.9052869069576264
  batch 250 loss: 0.8797014451026917
  batch 300 loss: 0.9119080567359924
  batch 350 loss: 0.8984253084659577
  batch 400 loss: 0.8784513807296753
  batch 450 loss: 0.9051854169368744
  batch 500 loss: 0.9022908473014831
  batch 550 loss: 0.8733075141906739
  batch 600 loss: 0.8993459057807922
  batch 650 loss: 0.9076530408859252
  batch 700 loss: 0.8570670509338378
  batch 750 loss: 0.8950093102455139
  batch 800 loss: 0.8825982213020325
  batch 850 loss: 0.9135136902332306
  batch 900 loss: 0.8955871629714965
LOSS train 0.89559 valid 0.96928, valid PER 30.34%
EPOCH 17:
  batch 50 loss: 0.9007652306556702
  batch 100 loss: 0.8308159041404725
  batch 150 loss: 0.8971807634830475
  batch 200 loss: 0.8697108650207519
  batch 250 loss: 0.9025229442119599
  batch 300 loss: 0.8622969114780425
  batch 350 loss: 0.8691681587696075
  batch 400 loss: 0.889424592256546
  batch 450 loss: 0.8562591779232025
  batch 500 loss: 0.8779890811443329
  batch 550 loss: 0.8844063723087311
  batch 600 loss: 0.9164045369625091
  batch 650 loss: 0.8570582592487335
  batch 700 loss: 0.8823880624771118
  batch 750 loss: 0.8343757033348084
  batch 800 loss: 0.8922839748859406
  batch 850 loss: 0.8616533267498017
  batch 900 loss: 0.8705422353744506
LOSS train 0.87054 valid 0.94582, valid PER 29.65%
EPOCH 18:
  batch 50 loss: 0.8404926812648773
  batch 100 loss: 0.8621804928779602
  batch 150 loss: 0.896338266134262
  batch 200 loss: 0.8612001359462738
  batch 250 loss: 0.8170804738998413
  batch 300 loss: 0.8567836821079254
  batch 350 loss: 0.8214683544635772
  batch 400 loss: 0.8405331921577454
  batch 450 loss: 0.9600324928760529
  batch 500 loss: 0.9010237157344818
  batch 550 loss: 0.9232562685012817
  batch 600 loss: 0.8947160851955414
  batch 650 loss: 0.855747116804123
  batch 700 loss: 0.8582113087177277
  batch 750 loss: 0.8951291942596435
  batch 800 loss: 0.9150823962688446
  batch 850 loss: 0.8681691634654999
  batch 900 loss: 0.8680961430072784
LOSS train 0.86810 valid 0.95911, valid PER 29.75%
EPOCH 19:
  batch 50 loss: 0.8396716928482055
  batch 100 loss: 0.8494769287109375
  batch 150 loss: 0.8157289111614228
  batch 200 loss: 0.8227021265029907
  batch 250 loss: 0.8915241622924804
  batch 300 loss: 0.9159958112239838
  batch 350 loss: 0.9148028433322907
  batch 400 loss: 0.878611171245575
  batch 450 loss: 0.8180586516857147
  batch 500 loss: 0.841449476480484
  batch 550 loss: 0.8913169026374816
  batch 600 loss: 0.8813696646690369
  batch 650 loss: 0.8748017227649689
  batch 700 loss: 0.8838850152492523
  batch 750 loss: 0.8416471791267395
  batch 800 loss: 0.8225384378433227
  batch 850 loss: 0.862086888551712
  batch 900 loss: 0.8453345370292663
LOSS train 0.84533 valid 0.96099, valid PER 30.38%
EPOCH 20:
  batch 50 loss: 0.7957842946052551
  batch 100 loss: 0.8086563336849213
  batch 150 loss: 0.8280951964855194
  batch 200 loss: 0.8676832914352417
  batch 250 loss: 0.8743956971168518
  batch 300 loss: 0.8426070916652679
  batch 350 loss: 0.8386547607183457
  batch 400 loss: 0.8746236455440521
  batch 450 loss: 0.8695361471176147
  batch 500 loss: 0.8653603374958039
  batch 550 loss: 0.8440709841251374
  batch 600 loss: 0.822852543592453
  batch 650 loss: 0.903899530172348
  batch 700 loss: 0.8404229259490967
  batch 750 loss: 0.8399585914611817
  batch 800 loss: 0.8541299998760223
  batch 850 loss: 0.8673858165740966
  batch 900 loss: 0.8561409831047058
LOSS train 0.85614 valid 1.00468, valid PER 31.12%
Training finished in 6.0 minutes.
Model saved to checkpoints/20230116_143335/model_17
Loading model from checkpoints/20230116_143335/model_17
SUB: 15.88%, DEL: 13.87%, INS: 1.86%, COR: 70.25%, PER: 31.61%
