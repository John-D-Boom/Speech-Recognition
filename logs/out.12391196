Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.9)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.055208497047424
  batch 100 loss: 3.2269785022735595
  batch 150 loss: 3.124275498390198
  batch 200 loss: 2.994671468734741
  batch 250 loss: 2.9113637590408326
  batch 300 loss: 2.787547035217285
  batch 350 loss: 2.7284633350372314
  batch 400 loss: 2.651392407417297
  batch 450 loss: 2.5742970180511473
  batch 500 loss: 2.4977057123184205
  batch 550 loss: 2.4543594121932983
  batch 600 loss: 2.442641749382019
  batch 650 loss: 2.3879265642166136
  batch 700 loss: 2.395662603378296
  batch 750 loss: 2.3887375259399413
  batch 800 loss: 2.3620865726470948
  batch 850 loss: 2.347043514251709
  batch 900 loss: 2.339077115058899
max grad: tensor(4.1814)
LOSS train 2.33908 valid 2.49077, valid PER 83.88%
EPOCH 2:
  batch 50 loss: 2.332173089981079
  batch 100 loss: 2.334883942604065
  batch 150 loss: 2.2341759967803956
  batch 200 loss: 2.2642579531669615
  batch 250 loss: 2.2711630773544313
  batch 300 loss: 2.2628587007522585
  batch 350 loss: 2.2479243421554567
  batch 400 loss: 2.201829743385315
  batch 450 loss: 2.2430819845199585
  batch 500 loss: 2.2251027274131774
  batch 550 loss: 2.2347802352905273
  batch 600 loss: 2.19402277469635
  batch 650 loss: 2.1634181261062624
  batch 700 loss: 2.2041190385818483
  batch 750 loss: 2.173341293334961
  batch 800 loss: 2.131416358947754
  batch 850 loss: 2.1787124109268188
  batch 900 loss: 2.13487024307251
max grad: tensor(1.4368)
LOSS train 2.13487 valid 2.18170, valid PER 80.37%
EPOCH 3:
  batch 50 loss: 2.101357181072235
  batch 100 loss: 2.1571419072151183
  batch 150 loss: 2.1461207556724546
  batch 200 loss: 2.0993630576133726
  batch 250 loss: 2.112449860572815
  batch 300 loss: 2.1251502442359924
  batch 350 loss: 2.1237455368041993
  batch 400 loss: 2.106404914855957
  batch 450 loss: 2.101419804096222
  batch 500 loss: 2.062961754798889
  batch 550 loss: 2.078772714138031
  batch 600 loss: 2.0545642805099487
  batch 650 loss: 2.0526043248176573
  batch 700 loss: 2.057859168052673
  batch 750 loss: 2.0887439012527467
  batch 800 loss: 2.0919978833198547
  batch 850 loss: 2.059170446395874
  batch 900 loss: 2.051052050590515
max grad: tensor(1.2967)
LOSS train 2.05105 valid 2.05349, valid PER 77.95%
EPOCH 4:
  batch 50 loss: 2.0670941853523255
  batch 100 loss: 2.002517538070679
  batch 150 loss: 2.0322025299072264
  batch 200 loss: 2.0298560094833373
  batch 250 loss: 2.0626627373695374
  batch 300 loss: 2.017834279537201
  batch 350 loss: 2.0217104530334473
  batch 400 loss: 1.9993034648895263
  batch 450 loss: 2.0101255774497986
  batch 500 loss: 2.051747760772705
  batch 550 loss: 1.9650740647315978
  batch 600 loss: 1.9612412643432617
  batch 650 loss: 2.0202522039413453
  batch 700 loss: 2.044943814277649
  batch 750 loss: 2.0012091994285583
  batch 800 loss: 1.9790062451362609
  batch 850 loss: 1.9632582354545594
  batch 900 loss: 1.9779324698448182
max grad: tensor(1.5143)
LOSS train 1.97793 valid 1.99666, valid PER 77.19%
EPOCH 5:
  batch 50 loss: 1.998823971748352
  batch 100 loss: 1.9692276859283446
  batch 150 loss: 1.9668572688102721
  batch 200 loss: 2.0151851916313173
  batch 250 loss: 1.9530630779266358
  batch 300 loss: 1.976342763900757
  batch 350 loss: 1.953564088344574
  batch 400 loss: 1.9439397144317627
  batch 450 loss: 1.9552508211135864
  batch 500 loss: 1.9097129487991333
  batch 550 loss: 1.9540038800239563
  batch 600 loss: 1.962788426876068
  batch 650 loss: 1.9463048791885376
  batch 700 loss: 1.9478690004348755
  batch 750 loss: 1.9299304819107055
  batch 800 loss: 1.9943487095832824
  batch 850 loss: 1.9306674027442932
  batch 900 loss: 1.9210791969299317
max grad: tensor(1.2275)
LOSS train 1.92108 valid 1.94760, valid PER 75.04%
EPOCH 6:
  batch 50 loss: 1.9234280896186828
  batch 100 loss: 1.9193567872047423
  batch 150 loss: 1.9025682497024536
  batch 200 loss: 1.8646355748176575
  batch 250 loss: 1.9165269589424134
  batch 300 loss: 1.9457096838951111
  batch 350 loss: 1.9200274229049683
  batch 400 loss: 1.9040958881378174
  batch 450 loss: 1.9217773795127868
  batch 500 loss: 1.8891212034225464
  batch 550 loss: 1.9157104086875916
  batch 600 loss: 1.8802809047698974
  batch 650 loss: 1.8750640726089478
  batch 700 loss: 1.864205436706543
  batch 750 loss: 1.9003512835502625
  batch 800 loss: 1.9205001664161683
  batch 850 loss: 1.8940034914016723
  batch 900 loss: 1.9031437158584594
max grad: tensor(1.3033)
LOSS train 1.90314 valid 1.86031, valid PER 74.22%
EPOCH 7:
  batch 50 loss: 1.874517138004303
  batch 100 loss: 1.9231615018844606
  batch 150 loss: 1.8733050441741943
  batch 200 loss: 1.8339235520362853
  batch 250 loss: 1.8902491998672486
  batch 300 loss: 1.8761387729644776
  batch 350 loss: 1.8919371271133423
  batch 400 loss: 1.8507289242744447
  batch 450 loss: 1.8946613025665284
  batch 500 loss: 1.8847377395629883
  batch 550 loss: 1.8247430276870729
  batch 600 loss: 1.8678951454162598
  batch 650 loss: 1.8363964557647705
  batch 700 loss: 1.8855175638198853
  batch 750 loss: 1.84940838098526
  batch 800 loss: 1.86147399187088
  batch 850 loss: 1.833977587223053
  batch 900 loss: 1.8468689632415771
max grad: tensor(3.6761)
LOSS train 1.84687 valid 1.92702, valid PER 73.24%
EPOCH 8:
  batch 50 loss: 1.8728593492507934
  batch 100 loss: 1.8227623987197876
  batch 150 loss: 1.8882879400253296
  batch 200 loss: 1.8418840193748474
  batch 250 loss: 1.8204116058349609
  batch 300 loss: 1.820023717880249
  batch 350 loss: 1.8163001680374145
  batch 400 loss: 1.8098187112808228
  batch 450 loss: 1.8946554088592529
  batch 500 loss: 1.8751927566528321
  batch 550 loss: 1.8314930415153503
  batch 600 loss: 1.7937223172187806
  batch 650 loss: 1.8076928520202638
  batch 700 loss: 1.839537091255188
  batch 750 loss: 1.8383235931396484
  batch 800 loss: 1.8249265694618224
  batch 850 loss: 1.798674294948578
  batch 900 loss: 1.818765299320221
max grad: tensor(1.1313)
LOSS train 1.81877 valid 1.75782, valid PER 70.54%
EPOCH 9:
  batch 50 loss: 1.8296132588386536
  batch 100 loss: 1.7944955897331238
  batch 150 loss: 1.8262021517753602
  batch 200 loss: 1.7804228639602662
  batch 250 loss: 1.778224356174469
  batch 300 loss: 1.834713752269745
  batch 350 loss: 1.8177770471572876
  batch 400 loss: 1.848461663722992
  batch 450 loss: 1.8076630282402038
  batch 500 loss: 1.783856644630432
  batch 550 loss: 1.7994083285331726
  batch 600 loss: 1.828551003932953
  batch 650 loss: 1.8093975067138672
  batch 700 loss: 1.790479407310486
  batch 750 loss: 1.7973651170730591
  batch 800 loss: 1.826109573841095
  batch 850 loss: 1.8200939464569093
  batch 900 loss: 1.7765180969238281
max grad: tensor(1.0636)
LOSS train 1.77652 valid 1.70164, valid PER 70.75%
EPOCH 10:
  batch 50 loss: 1.7725390815734863
  batch 100 loss: 1.8034398651123047
  batch 150 loss: 1.8057424235343933
  batch 200 loss: 1.7928987622261048
  batch 250 loss: 1.7982820248603821
  batch 300 loss: 1.7829099535942077
  batch 350 loss: 1.7938291263580322
  batch 400 loss: 1.7392433190345764
  batch 450 loss: 1.7460035133361815
  batch 500 loss: 1.7673850226402283
  batch 550 loss: 1.7553570175170898
  batch 600 loss: 1.7589346146583558
  batch 650 loss: 1.780939531326294
  batch 700 loss: 1.7856378769874572
  batch 750 loss: 1.790966670513153
  batch 800 loss: 1.7853080224990845
  batch 850 loss: 1.7825933074951172
  batch 900 loss: 1.7404085445404052
max grad: tensor(1.1255)
LOSS train 1.74041 valid 1.78675, valid PER 71.22%
EPOCH 11:
  batch 50 loss: 1.7939035677909851
  batch 100 loss: 1.7630512475967408
  batch 150 loss: 1.7361452984809875
  batch 200 loss: 1.712231571674347
  batch 250 loss: 1.735881142616272
  batch 300 loss: 1.7474264264106751
  batch 350 loss: 1.7925754261016846
  batch 400 loss: 1.7508283591270446
  batch 450 loss: 1.7444771146774292
  batch 500 loss: 1.7356026935577393
  batch 550 loss: 1.7492565655708312
  batch 600 loss: 1.7432043647766113
  batch 650 loss: 1.7665657997131348
  batch 700 loss: 1.8376392436027527
  batch 750 loss: 1.7519137835502625
  batch 800 loss: 1.7604122543334961
  batch 850 loss: 1.7245018839836121
  batch 900 loss: 1.7767343091964722
max grad: tensor(1.0203)
LOSS train 1.77673 valid 1.67356, valid PER 66.99%
EPOCH 12:
  batch 50 loss: 1.7284932875633239
  batch 100 loss: 1.6947217607498168
  batch 150 loss: 1.7236214423179625
  batch 200 loss: 1.750880765914917
  batch 250 loss: 1.720813729763031
  batch 300 loss: 1.7810464930534362
  batch 350 loss: 1.7376281762123107
  batch 400 loss: 1.7164504909515381
  batch 450 loss: 1.7421328210830689
  batch 500 loss: 1.733072316646576
  batch 550 loss: 1.7584330773353576
  batch 600 loss: 1.754376401901245
  batch 650 loss: 1.7453634905815125
  batch 700 loss: 1.7121886491775513
  batch 750 loss: 1.7607768082618713
  batch 800 loss: 1.7069203853607178
  batch 850 loss: 1.7441270208358766
  batch 900 loss: 1.7197345495224
max grad: tensor(1.1580)
LOSS train 1.71973 valid 1.65092, valid PER 65.82%
EPOCH 13:
  batch 50 loss: 1.708933310508728
  batch 100 loss: 1.7276599597930908
  batch 150 loss: 1.7478813862800597
  batch 200 loss: 1.6862748670578003
  batch 250 loss: 1.7342738270759583
  batch 300 loss: 1.739951386451721
  batch 350 loss: 1.706393587589264
  batch 400 loss: 1.7267899751663207
  batch 450 loss: 1.7157305526733397
  batch 500 loss: 1.7086136102676392
  batch 550 loss: 1.7632358002662658
  batch 600 loss: 1.7252764916419983
  batch 650 loss: 1.7233837676048278
  batch 700 loss: 1.7288473176956176
  batch 750 loss: 1.6664750480651855
  batch 800 loss: 1.7274058485031127
  batch 850 loss: 1.7273747611045838
  batch 900 loss: 1.7173874807357787
max grad: tensor(1.3028)
LOSS train 1.71739 valid 1.61872, valid PER 65.54%
EPOCH 14:
  batch 50 loss: 1.6972118616104126
  batch 100 loss: 1.6790463519096375
  batch 150 loss: 1.7082433533668517
  batch 200 loss: 1.6906788921356202
  batch 250 loss: 1.6868899369239807
  batch 300 loss: 1.6808285093307496
  batch 350 loss: 1.6757403421401977
  batch 400 loss: 1.7167737364768982
  batch 450 loss: 1.6909507083892823
  batch 500 loss: 1.6831785392761232
  batch 550 loss: 1.7168041586875915
  batch 600 loss: 1.6867318391799926
  batch 650 loss: 1.723850464820862
  batch 700 loss: 1.723434295654297
  batch 750 loss: 1.7158912849426269
  batch 800 loss: 1.7119159817695617
  batch 850 loss: 1.730200505256653
  batch 900 loss: 1.703688781261444
max grad: tensor(1.1116)
LOSS train 1.70369 valid 1.63761, valid PER 66.49%
EPOCH 15:
  batch 50 loss: 1.6642927312850953
  batch 100 loss: 1.707269821166992
  batch 150 loss: 1.6698482394218446
  batch 200 loss: 1.6963614511489868
  batch 250 loss: 1.7161373496055603
  batch 300 loss: 1.698378975391388
  batch 350 loss: 1.6514022302627565
  batch 400 loss: 1.6906156349182129
  batch 450 loss: 1.6968954825401306
  batch 500 loss: 1.7108995580673219
  batch 550 loss: 1.7260763621330262
  batch 600 loss: 1.715309329032898
  batch 650 loss: 1.66632310628891
  batch 700 loss: 1.6855236196517944
  batch 750 loss: 1.7061399817466736
  batch 800 loss: 1.6876431107521057
  batch 850 loss: 1.669098241329193
  batch 900 loss: 1.6258895611763
max grad: tensor(2.0489)
LOSS train 1.62589 valid 1.64446, valid PER 66.54%
EPOCH 16:
  batch 50 loss: 1.6597614121437072
  batch 100 loss: 1.6735662508010865
  batch 150 loss: 1.6710823678970337
  batch 200 loss: 1.6830294060707092
  batch 250 loss: 1.6549918675422668
  batch 300 loss: 1.7342485666275025
  batch 350 loss: 1.6856265211105346
  batch 400 loss: 1.6629801344871522
  batch 450 loss: 1.702955105304718
  batch 500 loss: 1.681429727077484
  batch 550 loss: 1.6581478476524354
  batch 600 loss: 1.6781968522071837
  batch 650 loss: 1.6566913628578186
  batch 700 loss: 1.6430442643165588
  batch 750 loss: 1.693701114654541
  batch 800 loss: 1.6571984791755676
  batch 850 loss: 1.6655865931510925
  batch 900 loss: 1.6802086853981018
max grad: tensor(1.2525)
LOSS train 1.68021 valid 1.60969, valid PER 64.11%
EPOCH 17:
  batch 50 loss: 1.6473716282844544
  batch 100 loss: 1.6122654128074645
  batch 150 loss: 1.6949866819381714
  batch 200 loss: 1.6504449796676637
  batch 250 loss: 1.6342479944229127
  batch 300 loss: 1.6640394520759583
  batch 350 loss: 1.6489146542549133
  batch 400 loss: 1.7001400637626647
  batch 450 loss: 1.6695881724357604
  batch 500 loss: 1.6523531579971313
  batch 550 loss: 1.6568068170547485
  batch 600 loss: 1.6908900189399718
  batch 650 loss: 1.65715749502182
  batch 700 loss: 1.6661238074302673
  batch 750 loss: 1.607742714881897
  batch 800 loss: 1.6908828449249267
  batch 850 loss: 1.647155888080597
  batch 900 loss: 1.6897163319587707
max grad: tensor(1.2740)
LOSS train 1.68972 valid 1.56236, valid PER 60.81%
EPOCH 18:
  batch 50 loss: 1.6643519163131715
  batch 100 loss: 1.6267454624176025
  batch 150 loss: 1.675934555530548
  batch 200 loss: 1.6584148073196412
  batch 250 loss: 1.6476112842559814
  batch 300 loss: 1.6324798250198365
  batch 350 loss: 1.6293159437179565
  batch 400 loss: 1.6502669525146485
  batch 450 loss: 1.719365074634552
  batch 500 loss: 1.655334861278534
  batch 550 loss: 1.7160068917274476
  batch 600 loss: 1.6532112860679626
  batch 650 loss: 1.6241588592529297
  batch 700 loss: 1.6094174814224242
  batch 750 loss: 1.650665624141693
  batch 800 loss: 1.6569613075256349
  batch 850 loss: 1.6532644724845886
  batch 900 loss: 1.642083923816681
max grad: tensor(2.6388)
LOSS train 1.64208 valid 1.58445, valid PER 63.39%
EPOCH 19:
  batch 50 loss: 1.6703731560707091
  batch 100 loss: 1.6378105092048645
  batch 150 loss: 1.6292360949516296
  batch 200 loss: 1.6274466466903688
  batch 250 loss: 1.6475144624710083
  batch 300 loss: 1.6170545434951782
  batch 350 loss: 1.6649436306953431
  batch 400 loss: 1.6582374691963195
  batch 450 loss: 1.6121459579467774
  batch 500 loss: 1.6360667634010315
  batch 550 loss: 1.6611094880104065
  batch 600 loss: 1.6387543964385987
  batch 650 loss: 1.6341688084602355
  batch 700 loss: 1.6557198476791382
  batch 750 loss: 1.6220828318595886
  batch 800 loss: 1.6239883756637574
  batch 850 loss: 1.6419447827339173
  batch 900 loss: 1.6296415042877197
max grad: tensor(1.6997)
LOSS train 1.62964 valid 1.50343, valid PER 60.03%
EPOCH 20:
  batch 50 loss: 1.600893337726593
  batch 100 loss: 1.600907151699066
  batch 150 loss: 1.623367109298706
  batch 200 loss: 1.631612319946289
  batch 250 loss: 1.6344970893859863
  batch 300 loss: 1.646896984577179
  batch 350 loss: 1.6294024801254272
  batch 400 loss: 1.6152118039131165
  batch 450 loss: 1.6457265329360962
  batch 500 loss: 1.6296886491775513
  batch 550 loss: 1.642991282939911
  batch 600 loss: 1.5926549243927002
  batch 650 loss: 1.6235669732093811
  batch 700 loss: 1.6386967492103577
  batch 750 loss: 1.6401221799850463
  batch 800 loss: 1.6440461921691893
  batch 850 loss: 1.6332632613182068
  batch 900 loss: 1.6028846502304077
max grad: tensor(1.7208)
LOSS train 1.60288 valid 1.55097, valid PER 60.10%
Training finished in 8.0 minutes.
Model saved to checkpoints/20230116_141117/model_19
Loading model from checkpoints/20230116_141117/model_19
SUB: 5.19%, DEL: 56.78%, INS: 0.04%, COR: 38.04%, PER: 62.00%
