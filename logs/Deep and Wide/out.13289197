Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=256, concat=1, lr=0.002, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.2, beta1=0.9, beta2=0.999)
Total number of model parameters is 2172968
EPOCH 1:
  batch 50 loss: 4.864369101524353
  batch 100 loss: 3.0498994064331053
  batch 150 loss: 2.8697336387634276
  batch 200 loss: 2.704238748550415
  batch 250 loss: 2.5711856126785277
  batch 300 loss: 2.3490700912475586
  batch 350 loss: 2.151857395172119
  batch 400 loss: 2.0651140093803404
  batch 450 loss: 1.8898044419288635
  batch 500 loss: 1.7571717381477356
  batch 550 loss: 1.6928640270233155
  batch 600 loss: 1.6499054956436157
  batch 650 loss: 1.546050271987915
  batch 700 loss: 1.561541006565094
  batch 750 loss: 1.464729390144348
  batch 800 loss: 1.480124852657318
  batch 850 loss: 1.4316342544555665
  batch 900 loss: 1.3788559126853943
LOSS train 1.37886 valid 1.34908, valid PER 43.60%
EPOCH 2:
  batch 50 loss: 1.3542083477973939
  batch 100 loss: 1.3414863753318786
  batch 150 loss: 1.2972332763671874
  batch 200 loss: 1.2704302716255187
  batch 250 loss: 1.2955684399604797
  batch 300 loss: 1.2079547214508057
  batch 350 loss: 1.2239468836784362
  batch 400 loss: 1.1951061391830444
  batch 450 loss: 1.1731010782718658
  batch 500 loss: 1.1871036660671235
  batch 550 loss: 1.151038647890091
  batch 600 loss: 1.1353804647922516
  batch 650 loss: 1.1612545382976531
  batch 700 loss: 1.2330918323993683
  batch 750 loss: 1.157095605134964
  batch 800 loss: 1.1147676253318786
  batch 850 loss: 1.0929003036022187
  batch 900 loss: 1.0706589603424073
LOSS train 1.07066 valid 1.04180, valid PER 32.84%
EPOCH 3:
  batch 50 loss: 0.9768276226520538
  batch 100 loss: 1.0774276638031006
  batch 150 loss: 1.1157829535007477
  batch 200 loss: 1.042882696390152
  batch 250 loss: 1.0347142493724824
  batch 300 loss: 1.0470571267604827
  batch 350 loss: 1.0329247605800629
  batch 400 loss: 1.0428979218006134
  batch 450 loss: 1.1237534725666045
  batch 500 loss: 1.080951793193817
  batch 550 loss: 1.0647808349132537
  batch 600 loss: 1.0045948719978333
  batch 650 loss: 1.0335472595691682
  batch 700 loss: 1.0251734101772307
  batch 750 loss: 1.013275672197342
  batch 800 loss: 1.0420831453800201
  batch 850 loss: 0.992164603471756
  batch 900 loss: 1.044462229013443
LOSS train 1.04446 valid 0.99588, valid PER 31.73%
EPOCH 4:
  batch 50 loss: 0.9819446325302124
  batch 100 loss: 0.9369006204605103
  batch 150 loss: 0.9774585258960724
  batch 200 loss: 0.9524183154106141
  batch 250 loss: 0.9231297719478607
  batch 300 loss: 0.9528113222122192
  batch 350 loss: 0.9162581622600555
  batch 400 loss: 0.9041635608673095
  batch 450 loss: 0.8927161777019501
  batch 500 loss: 0.9698766744136811
  batch 550 loss: 0.9163778328895569
  batch 600 loss: 0.8975010561943054
  batch 650 loss: 0.9791300570964814
  batch 700 loss: 0.9377767181396485
  batch 750 loss: 0.9352807486057282
  batch 800 loss: 0.9019314694404602
  batch 850 loss: 0.897112501859665
  batch 900 loss: 0.9543979525566101
LOSS train 0.95440 valid 0.94682, valid PER 28.62%
EPOCH 5:
  batch 50 loss: 0.8463893699645996
  batch 100 loss: 0.8095760262012481
  batch 150 loss: 0.8548478829860687
  batch 200 loss: 0.8789146995544433
  batch 250 loss: 0.8348175919055939
  batch 300 loss: 0.8862025785446167
  batch 350 loss: 0.8311952602863312
  batch 400 loss: 0.830951783657074
  batch 450 loss: 0.8403261435031891
  batch 500 loss: 0.8039513421058655
  batch 550 loss: 0.8613750970363617
  batch 600 loss: 0.8406324392557144
  batch 650 loss: 0.8546748578548431
  batch 700 loss: 0.862429404258728
  batch 750 loss: 0.8329175198078156
  batch 800 loss: 0.858397730588913
  batch 850 loss: 0.8395012199878693
  batch 900 loss: 0.8218170571327209
LOSS train 0.82182 valid 0.89518, valid PER 27.70%
EPOCH 6:
  batch 50 loss: 0.7693235659599305
  batch 100 loss: 0.79866161942482
  batch 150 loss: 0.7821087288856506
  batch 200 loss: 0.7567981958389283
  batch 250 loss: 0.7591198575496674
  batch 300 loss: 0.7859003484249115
  batch 350 loss: 0.8027851474285126
  batch 400 loss: 0.7774816131591797
  batch 450 loss: 0.7968627965450287
  batch 500 loss: 0.7540747505426407
  batch 550 loss: 0.7824317812919617
  batch 600 loss: 0.7521661335229873
  batch 650 loss: 0.7301731216907501
  batch 700 loss: 0.7702930963039398
  batch 750 loss: 0.8031679916381836
  batch 800 loss: 0.8102322328090668
  batch 850 loss: 0.8060524630546569
  batch 900 loss: 0.7869882702827453
LOSS train 0.78699 valid 0.85956, valid PER 26.43%
EPOCH 7:
  batch 50 loss: 0.6972992426156998
  batch 100 loss: 0.7255871272087098
  batch 150 loss: 0.6782950460910797
  batch 200 loss: 0.7041320210695267
  batch 250 loss: 0.8049031400680542
  batch 300 loss: 0.7390775382518768
  batch 350 loss: 0.7641979503631592
  batch 400 loss: 0.7193235445022583
  batch 450 loss: 0.7233454072475434
  batch 500 loss: 0.7387959957122803
  batch 550 loss: 0.7244549334049225
  batch 600 loss: 0.7322976458072662
  batch 650 loss: 0.7266445887088776
  batch 700 loss: 0.769660005569458
  batch 750 loss: 0.7436816358566284
  batch 800 loss: 0.7426839250326157
  batch 850 loss: 0.7135655927658081
  batch 900 loss: 0.7495127141475677
LOSS train 0.74951 valid 0.85381, valid PER 26.58%
EPOCH 8:
  batch 50 loss: 0.6704271852970123
  batch 100 loss: 0.6570222109556199
  batch 150 loss: 0.6718360662460328
  batch 200 loss: 0.6603811591863632
  batch 250 loss: 0.672285121679306
  batch 300 loss: 0.6411324745416641
  batch 350 loss: 0.6707972455024719
  batch 400 loss: 0.6745144039392471
  batch 450 loss: 0.7042820310592651
  batch 500 loss: 0.6653297996520996
  batch 550 loss: 0.7126837491989135
  batch 600 loss: 0.6478196054697036
  batch 650 loss: 0.6716655242443085
  batch 700 loss: 0.6856186217069626
  batch 750 loss: 0.7122464764118195
  batch 800 loss: 0.7421268391609191
  batch 850 loss: 0.6831287050247192
  batch 900 loss: 0.6884292435646057
LOSS train 0.68843 valid 0.81872, valid PER 25.08%
EPOCH 9:
  batch 50 loss: 0.5924776607751846
  batch 100 loss: 0.596182934641838
  batch 150 loss: 0.617643312215805
  batch 200 loss: 0.6189748966693878
  batch 250 loss: 0.6030989289283752
  batch 300 loss: 0.6197517490386963
  batch 350 loss: 0.618079771399498
  batch 400 loss: 0.6420005202293396
  batch 450 loss: 0.6412460601329804
  batch 500 loss: 0.6390887421369552
  batch 550 loss: 0.6298742324113846
  batch 600 loss: 0.6548813039064407
  batch 650 loss: 0.6795113372802735
  batch 700 loss: 0.6383465701341628
  batch 750 loss: 0.6537353420257568
  batch 800 loss: 0.7131130057573318
  batch 850 loss: 0.6655677688121796
  batch 900 loss: 0.6328516757488251
LOSS train 0.63285 valid 0.81901, valid PER 24.68%
EPOCH 10:
  batch 50 loss: 0.569718508720398
  batch 100 loss: 0.5571942955255509
  batch 150 loss: 0.5895012545585633
  batch 200 loss: 0.5773731690645217
  batch 250 loss: 0.5888517630100251
  batch 300 loss: 0.616115602850914
  batch 350 loss: 0.5921261888742447
  batch 400 loss: 0.5868773001432419
  batch 450 loss: 0.5851886022090912
  batch 500 loss: 0.5994602847099304
  batch 550 loss: 0.5872841531038284
  batch 600 loss: 0.6085368877649308
  batch 650 loss: 0.6257707673311234
  batch 700 loss: 0.6385599189996719
  batch 750 loss: 0.6359325355291366
  batch 800 loss: 0.6190765273571014
  batch 850 loss: 0.6206709432601929
  batch 900 loss: 0.6231602054834365
LOSS train 0.62316 valid 0.83378, valid PER 24.48%
EPOCH 11:
  batch 50 loss: 0.5781471687555313
  batch 100 loss: 0.543650341629982
  batch 150 loss: 0.5426619338989258
  batch 200 loss: 0.5164688014984131
  batch 250 loss: 0.5358917129039764
  batch 300 loss: 0.5244736391305923
  batch 350 loss: 0.5852811676263809
  batch 400 loss: 0.5572998625040054
  batch 450 loss: 0.5777517133951187
  batch 500 loss: 0.5738291507959365
  batch 550 loss: 0.603162157535553
  batch 600 loss: 0.5585618925094604
  batch 650 loss: 0.5755697917938233
  batch 700 loss: 0.624224163889885
  batch 750 loss: 0.5797817146778107
  batch 800 loss: 0.5772977155447007
  batch 850 loss: 0.5745365923643112
  batch 900 loss: 0.6001761668920517
LOSS train 0.60018 valid 0.78897, valid PER 23.36%
EPOCH 12:
  batch 50 loss: 0.4902902090549469
  batch 100 loss: 0.504549371600151
  batch 150 loss: 0.5154242146015168
  batch 200 loss: 0.5045267343521118
  batch 250 loss: 0.5215768945217133
  batch 300 loss: 0.554960607290268
  batch 350 loss: 0.5228491467237473
  batch 400 loss: 0.5326578444242478
  batch 450 loss: 0.5297821348905564
  batch 500 loss: 0.5793689906597137
  batch 550 loss: 0.5721167534589767
  batch 600 loss: 0.5657941341400147
  batch 650 loss: 0.5506668096780777
  batch 700 loss: 0.5412379032373429
  batch 750 loss: 0.5691764587163926
  batch 800 loss: 0.5614962148666381
  batch 850 loss: 0.5748466747999191
  batch 900 loss: 0.573377240896225
LOSS train 0.57338 valid 0.80281, valid PER 23.49%
EPOCH 13:
  batch 50 loss: 0.4870426678657532
  batch 100 loss: 0.4773169958591461
  batch 150 loss: 0.49047323286533356
  batch 200 loss: 0.47033353269100187
  batch 250 loss: 0.47751994013786314
  batch 300 loss: 0.5211456581950188
  batch 350 loss: 0.5006189134716987
  batch 400 loss: 0.5044475769996644
  batch 450 loss: 0.5403881084918976
  batch 500 loss: 0.5395522904396057
  batch 550 loss: 0.5809829819202423
  batch 600 loss: 0.5794388777017594
  batch 650 loss: 0.5561064040660858
  batch 700 loss: 0.5677491241693496
  batch 750 loss: 0.5279274368286133
  batch 800 loss: 0.5690851825475692
  batch 850 loss: 0.5206346702575684
  batch 900 loss: 0.5528223323822021
LOSS train 0.55282 valid 0.82282, valid PER 23.66%
EPOCH 14:
  batch 50 loss: 0.4752746221423149
  batch 100 loss: 0.4632753014564514
  batch 150 loss: 0.47585204243659973
  batch 200 loss: 0.48784513533115387
  batch 250 loss: 0.48199783623218534
  batch 300 loss: 0.4886084425449371
  batch 350 loss: 0.5358829134702683
  batch 400 loss: 0.529239090681076
  batch 450 loss: 0.48900812983512876
  batch 500 loss: 0.49714244484901426
  batch 550 loss: 0.5199273902177811
  batch 600 loss: 0.5070825731754303
  batch 650 loss: 0.5147291964292526
  batch 700 loss: 0.5017069453001022
  batch 750 loss: 0.5184958964586258
  batch 800 loss: 0.5055649754405022
  batch 850 loss: 0.5234790587425232
  batch 900 loss: 0.5127530527114869
LOSS train 0.51275 valid 0.80307, valid PER 23.09%
EPOCH 15:
  batch 50 loss: 0.4446724236011505
  batch 100 loss: 0.45014175653457644
  batch 150 loss: 0.4529706686735153
  batch 200 loss: 0.4443320894241333
  batch 250 loss: 0.4737274616956711
  batch 300 loss: 0.46141368806362154
  batch 350 loss: 0.45760589838027954
  batch 400 loss: 0.48921113073825834
  batch 450 loss: 0.4615770214796066
  batch 500 loss: 0.46856719374656675
  batch 550 loss: 0.481250918507576
  batch 600 loss: 0.5129970133304596
  batch 650 loss: 0.49343362867832186
  batch 700 loss: 0.4847959977388382
  batch 750 loss: 0.4999539250135422
  batch 800 loss: 0.47395382285118104
  batch 850 loss: 0.48154881715774533
  batch 900 loss: 0.448669952750206
LOSS train 0.44867 valid 0.83540, valid PER 23.68%
EPOCH 16:
  batch 50 loss: 0.42744343757629394
  batch 100 loss: 0.4054131829738617
  batch 150 loss: 0.3982720068097115
  batch 200 loss: 0.43112588763237
  batch 250 loss: 0.43080571889877317
  batch 300 loss: 0.4276525914669037
  batch 350 loss: 0.41965204894542696
  batch 400 loss: 0.46236811637878417
  batch 450 loss: 0.4494385829567909
  batch 500 loss: 0.43882009625434876
  batch 550 loss: 0.40397982358932494
  batch 600 loss: 0.47611371874809266
  batch 650 loss: 0.4674909853935242
  batch 700 loss: 0.4432433295249939
  batch 750 loss: 0.4710239404439926
  batch 800 loss: 0.4698426026105881
  batch 850 loss: 0.4720126098394394
  batch 900 loss: 0.48488088488578795
LOSS train 0.48488 valid 0.82671, valid PER 23.32%
EPOCH 17:
  batch 50 loss: 0.41741198480129243
  batch 100 loss: 0.3698604902625084
  batch 150 loss: 0.3923715707659721
  batch 200 loss: 0.3532823571562767
  batch 250 loss: 0.3920102849602699
  batch 300 loss: 0.3783682259917259
  batch 350 loss: 0.3911563101410866
  batch 400 loss: 0.4037840601801872
  batch 450 loss: 0.3886357414722443
  batch 500 loss: 0.4343691611289978
  batch 550 loss: 0.4190180832147598
  batch 600 loss: 0.45956719785928724
  batch 650 loss: 0.4284890866279602
  batch 700 loss: 0.4494482088088989
  batch 750 loss: 0.4286676728725433
  batch 800 loss: 0.4556299716234207
  batch 850 loss: 0.44567114770412447
  batch 900 loss: 0.47941442728042605
LOSS train 0.47941 valid 0.82777, valid PER 23.10%
EPOCH 18:
  batch 50 loss: 0.36986211359500887
  batch 100 loss: 0.3910976427793503
  batch 150 loss: 0.43171824008226395
  batch 200 loss: 0.4806814777851105
  batch 250 loss: 0.4660062283277512
  batch 300 loss: 0.4128362011909485
  batch 350 loss: 0.40083400428295135
  batch 400 loss: 0.4300117415189743
  batch 450 loss: 0.45871430575847627
  batch 500 loss: 0.4537765860557556
  batch 550 loss: 0.47769595324993136
  batch 600 loss: 0.443776581287384
  batch 650 loss: 0.43548533499240877
  batch 700 loss: 0.4180439746379852
  batch 750 loss: 0.4363092291355133
  batch 800 loss: 0.44424047946929934
  batch 850 loss: 0.4382552430033684
  batch 900 loss: 0.4356745207309723
LOSS train 0.43567 valid 0.84304, valid PER 23.28%
EPOCH 19:
  batch 50 loss: 0.36378467112779617
  batch 100 loss: 0.3559733334183693
  batch 150 loss: 0.3543152180314064
  batch 200 loss: 0.3848501953482628
  batch 250 loss: 0.38536291182041166
  batch 300 loss: 0.4241662535071373
  batch 350 loss: 0.420624635219574
  batch 400 loss: 0.4044798830151558
  batch 450 loss: 0.37281119376420974
  batch 500 loss: 0.38112287372350695
  batch 550 loss: 0.4344478511810303
  batch 600 loss: 0.44165757566690445
  batch 650 loss: 0.4521447312831879
  batch 700 loss: 0.4196446165442467
  batch 750 loss: 0.4104014253616333
  batch 800 loss: 0.38713729768991473
  batch 850 loss: 0.4308655858039856
  batch 900 loss: 0.4525188422203064
LOSS train 0.45252 valid 0.87876, valid PER 23.93%
EPOCH 20:
  batch 50 loss: 0.37714600533246995
  batch 100 loss: 0.40322482138872145
  batch 150 loss: 0.38021644681692124
  batch 200 loss: 0.41303936183452605
  batch 250 loss: 0.38741817951202395
  batch 300 loss: 0.3862511074542999
  batch 350 loss: 0.35286311954259875
  batch 400 loss: 0.383954176902771
  batch 450 loss: 0.3724293947219849
  batch 500 loss: 0.39627590000629426
  batch 550 loss: 0.3984262055158615
  batch 600 loss: 0.3838876983523369
  batch 650 loss: 0.41399055659770967
  batch 700 loss: 0.38821567803621293
  batch 750 loss: 0.414163701236248
  batch 800 loss: 0.4256418639421463
  batch 850 loss: 0.42020296573638916
  batch 900 loss: 0.418958460688591
LOSS train 0.41896 valid 0.85000, valid PER 22.90%
[1.3788559126853943, 1.0706589603424073, 1.044462229013443, 0.9543979525566101, 0.8218170571327209, 0.7869882702827453, 0.7495127141475677, 0.6884292435646057, 0.6328516757488251, 0.6231602054834365, 0.6001761668920517, 0.573377240896225, 0.5528223323822021, 0.5127530527114869, 0.448669952750206, 0.48488088488578795, 0.47941442728042605, 0.4356745207309723, 0.4525188422203064, 0.418958460688591]
[1.3490785360336304, 1.0417958498001099, 0.9958763718605042, 0.9468241930007935, 0.8951783776283264, 0.8595598340034485, 0.8538148999214172, 0.8187242746353149, 0.8190096616744995, 0.833780825138092, 0.7889745831489563, 0.8028061389923096, 0.8228182196617126, 0.8030701875686646, 0.8354024887084961, 0.826705276966095, 0.8277702331542969, 0.8430351614952087, 0.8787622451782227, 0.8499962687492371]
Training finished in 16.0 minutes.
Model saved to checkpoints/20230125_135645/model_11
Loading model from checkpoints/20230125_135645/model_11
SUB: 16.27%, DEL: 7.76%, INS: 2.29%, COR: 75.98%, PER: 26.32%
