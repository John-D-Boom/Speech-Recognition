Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=3, fbank_dims=23, model_dims=256, concat=1, lr=0.002, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.2, beta1=0.9, beta2=0.999)
Total number of model parameters is 3749928
EPOCH 1:
  batch 50 loss: 4.525326867103576
  batch 100 loss: 3.2667945575714112
  batch 150 loss: 3.0381105756759643
  batch 200 loss: 2.741015229225159
  batch 250 loss: 2.479400782585144
  batch 300 loss: 2.232721469402313
  batch 350 loss: 1.9989757013320923
  batch 400 loss: 1.9212384271621703
  batch 450 loss: 1.7679904890060425
  batch 500 loss: 1.6513997364044188
  batch 550 loss: 1.5430995988845826
  batch 600 loss: 1.4946643829345703
  batch 650 loss: 1.421677360534668
  batch 700 loss: 1.4090313005447388
  batch 750 loss: 1.3533497309684754
  batch 800 loss: 1.3521617484092712
  batch 850 loss: 1.2943358218669891
  batch 900 loss: 1.2505000984668733
LOSS train 1.25050 valid 1.27448, valid PER 40.10%
EPOCH 2:
  batch 50 loss: 1.2363176333904267
  batch 100 loss: 1.2205410671234131
  batch 150 loss: 1.141792368888855
  batch 200 loss: 1.1544470691680908
  batch 250 loss: 1.1730996119976043
  batch 300 loss: 1.1687766921520233
  batch 350 loss: 1.1765374231338501
  batch 400 loss: 1.1334622621536254
  batch 450 loss: 1.1102079355716705
  batch 500 loss: 1.1487740099430084
  batch 550 loss: 1.1036363518238068
  batch 600 loss: 1.054189351797104
  batch 650 loss: 1.0629173815250397
  batch 700 loss: 1.0680614495277405
  batch 750 loss: 1.0624928426742555
  batch 800 loss: 1.0219218266010284
  batch 850 loss: 1.0289380478858947
  batch 900 loss: 1.0324619579315186
LOSS train 1.03246 valid 1.02294, valid PER 32.18%
EPOCH 3:
  batch 50 loss: 0.9193393194675445
  batch 100 loss: 0.9741558289527893
  batch 150 loss: 1.0617900061607362
  batch 200 loss: 0.9904644417762757
  batch 250 loss: 1.0117434346675873
  batch 300 loss: 0.9745602703094483
  batch 350 loss: 0.9950659310817719
  batch 400 loss: 0.9703731262683868
  batch 450 loss: 0.947020605802536
  batch 500 loss: 0.9575526583194732
  batch 550 loss: 0.9771017837524414
  batch 600 loss: 0.9288771116733551
  batch 650 loss: 0.9733123373985291
  batch 700 loss: 0.9554122984409332
  batch 750 loss: 1.0398880290985106
  batch 800 loss: 1.0626126945018768
  batch 850 loss: 0.9549946665763855
  batch 900 loss: 0.9644775676727295
LOSS train 0.96448 valid 1.02274, valid PER 31.88%
EPOCH 4:
  batch 50 loss: 0.966314902305603
  batch 100 loss: 0.8935784626007081
  batch 150 loss: 0.9185828006267548
  batch 200 loss: 0.9012150573730469
  batch 250 loss: 0.8983067202568055
  batch 300 loss: 0.9195442008972168
  batch 350 loss: 0.8914828813076019
  batch 400 loss: 0.8929034614562988
  batch 450 loss: 0.8801484417915344
  batch 500 loss: 0.9323263025283813
  batch 550 loss: 0.8494884276390076
  batch 600 loss: 0.8559198880195618
  batch 650 loss: 0.9199783051013947
  batch 700 loss: 0.8944445431232453
  batch 750 loss: 0.8575901305675506
  batch 800 loss: 0.860443412065506
  batch 850 loss: 0.875566279888153
  batch 900 loss: 0.9209255313873291
LOSS train 0.92093 valid 0.92448, valid PER 28.77%
EPOCH 5:
  batch 50 loss: 0.8237067627906799
  batch 100 loss: 0.8127461385726928
  batch 150 loss: 0.8592948484420776
  batch 200 loss: 0.8539748024940491
  batch 250 loss: 0.8020858037471771
  batch 300 loss: 0.8675711762905121
  batch 350 loss: 0.8405996513366699
  batch 400 loss: 0.8185760879516601
  batch 450 loss: 0.8011256897449494
  batch 500 loss: 0.8033135372400284
  batch 550 loss: 0.8276337289810181
  batch 600 loss: 0.8504220795631409
  batch 650 loss: 0.8286121416091919
  batch 700 loss: 0.8191718411445618
  batch 750 loss: 0.7991433990001678
  batch 800 loss: 0.8618073427677154
  batch 850 loss: 0.8784708988666534
  batch 900 loss: 0.8137561666965485
LOSS train 0.81376 valid 0.88067, valid PER 27.58%
EPOCH 6:
  batch 50 loss: 0.7846688032150269
  batch 100 loss: 0.7866643834114074
  batch 150 loss: 0.7712762153148651
  batch 200 loss: 0.7506825160980225
  batch 250 loss: 0.7555356621742249
  batch 300 loss: 0.762890533208847
  batch 350 loss: 0.77280064702034
  batch 400 loss: 0.7235766577720643
  batch 450 loss: 0.8005859541893006
  batch 500 loss: 0.7244210320711136
  batch 550 loss: 0.771770726442337
  batch 600 loss: 0.7785766524076462
  batch 650 loss: 0.7632924377918243
  batch 700 loss: 0.8008132207393647
  batch 750 loss: 0.7860687744617462
  batch 800 loss: 0.7756363689899445
  batch 850 loss: 0.805393192768097
  batch 900 loss: 0.7787801241874694
LOSS train 0.77878 valid 0.81976, valid PER 25.40%
EPOCH 7:
  batch 50 loss: 0.6828661894798279
  batch 100 loss: 0.7141965240240097
  batch 150 loss: 0.6549053490161896
  batch 200 loss: 0.6845819389820099
  batch 250 loss: 0.7479694008827209
  batch 300 loss: 0.6843681782484055
  batch 350 loss: 0.7331924003362655
  batch 400 loss: 0.6742138564586639
  batch 450 loss: 0.6799870604276657
  batch 500 loss: 0.6559634321928024
  batch 550 loss: 0.7023818284273148
  batch 600 loss: 0.7207159608602524
  batch 650 loss: 0.7887008190155029
  batch 700 loss: 0.8044414210319519
  batch 750 loss: 0.7587222498655319
  batch 800 loss: 0.7440164268016816
  batch 850 loss: 0.7059275263547897
  batch 900 loss: 0.7085730791091919
LOSS train 0.70857 valid 0.80135, valid PER 24.87%
EPOCH 8:
  batch 50 loss: 0.643942397236824
  batch 100 loss: 0.6370074617862701
  batch 150 loss: 0.6629552435874939
  batch 200 loss: 0.6537198197841644
  batch 250 loss: 0.6606572532653808
  batch 300 loss: 0.6177389454841614
  batch 350 loss: 0.6262221413850785
  batch 400 loss: 0.6556873792409896
  batch 450 loss: 0.704667004942894
  batch 500 loss: 0.6939087128639221
  batch 550 loss: 0.7065284341573715
  batch 600 loss: 0.6685338419675827
  batch 650 loss: 0.6771941083669663
  batch 700 loss: 0.6927238321304321
  batch 750 loss: 0.6872475415468215
  batch 800 loss: 0.7353962516784668
  batch 850 loss: 0.6851581239700317
  batch 900 loss: 0.693095788359642
LOSS train 0.69310 valid 0.79501, valid PER 24.76%
EPOCH 9:
  batch 50 loss: 0.6174853712320327
  batch 100 loss: 0.58717464864254
  batch 150 loss: 0.6350063443183899
  batch 200 loss: 0.6379413515329361
  batch 250 loss: 0.6104467689990998
  batch 300 loss: 0.6427010381221772
  batch 350 loss: 0.5988902997970581
  batch 400 loss: 0.6732520008087158
  batch 450 loss: 0.6860197842121124
  batch 500 loss: 0.6469467037916183
  batch 550 loss: 0.6526201176643371
  batch 600 loss: 0.6719004052877426
  batch 650 loss: 0.6796357697248459
  batch 700 loss: 0.6381341326236725
  batch 750 loss: 0.6584975808858872
  batch 800 loss: 0.6767782986164093
  batch 850 loss: 0.689174370765686
  batch 900 loss: 0.632065759897232
LOSS train 0.63207 valid 0.78838, valid PER 24.54%
EPOCH 10:
  batch 50 loss: 0.5733030676841736
  batch 100 loss: 0.5715069967508316
  batch 150 loss: 0.6039209240674972
  batch 200 loss: 0.5720425271987915
  batch 250 loss: 0.5698976343870163
  batch 300 loss: 0.5728969752788544
  batch 350 loss: 0.576877156496048
  batch 400 loss: 0.5692608582973481
  batch 450 loss: 0.5841447168588638
  batch 500 loss: 0.5797363984584808
  batch 550 loss: 0.6300637191534042
  batch 600 loss: 0.6166218358278275
  batch 650 loss: 0.6244900596141815
  batch 700 loss: 0.6462548565864563
  batch 750 loss: 0.6506732130050659
  batch 800 loss: 0.6293643420934677
  batch 850 loss: 0.6299100637435913
  batch 900 loss: 0.6318411523103714
LOSS train 0.63184 valid 0.78257, valid PER 24.07%
EPOCH 11:
  batch 50 loss: 0.5578416460752487
  batch 100 loss: 0.5445211780071259
  batch 150 loss: 0.5784878146648407
  batch 200 loss: 0.5185731047391892
  batch 250 loss: 0.553853548169136
  batch 300 loss: 0.5156143653392792
  batch 350 loss: 0.5776252073049545
  batch 400 loss: 0.5875313866138459
  batch 450 loss: 0.5984818041324615
  batch 500 loss: 0.5702404356002808
  batch 550 loss: 0.604333963394165
  batch 600 loss: 0.5844349932670593
  batch 650 loss: 0.6128882282972336
  batch 700 loss: 0.6310197174549103
  batch 750 loss: 0.5868047064542771
  batch 800 loss: 0.6085789453983307
  batch 850 loss: 0.5956400811672211
  batch 900 loss: 0.619883953332901
LOSS train 0.61988 valid 0.78901, valid PER 24.46%
EPOCH 12:
  batch 50 loss: 0.5466210180521012
  batch 100 loss: 0.4992627799510956
  batch 150 loss: 0.5500006061792374
  batch 200 loss: 0.563087802529335
  batch 250 loss: 0.5521031707525254
  batch 300 loss: 0.5773814630508423
  batch 350 loss: 0.5389761698246002
  batch 400 loss: 0.5964452755451203
  batch 450 loss: 0.5803711235523223
  batch 500 loss: 0.5732004022598267
  batch 550 loss: 0.5547069633007049
  batch 600 loss: 0.5587875932455063
  batch 650 loss: 0.5495410519838333
  batch 700 loss: 0.5521290343999863
  batch 750 loss: 0.5612358129024506
  batch 800 loss: 0.5312848383188248
  batch 850 loss: 0.5667182219028473
  batch 900 loss: 0.5860932171344757
LOSS train 0.58609 valid 0.78008, valid PER 23.10%
EPOCH 13:
  batch 50 loss: 0.4947155100107193
  batch 100 loss: 0.49356462687253955
  batch 150 loss: 0.49734398126602175
  batch 200 loss: 0.47942413449287413
  batch 250 loss: 0.5106132113933564
  batch 300 loss: 0.5384843093156815
  batch 350 loss: 0.5030158585309983
  batch 400 loss: 0.4895055115222931
  batch 450 loss: 0.5107833832502365
  batch 500 loss: 0.5024566984176636
  batch 550 loss: 0.5658495771884918
  batch 600 loss: 0.5585644489526749
  batch 650 loss: 0.5474080538749695
  batch 700 loss: 0.561771473288536
  batch 750 loss: 0.5266386532783508
  batch 800 loss: 0.5423705857992173
  batch 850 loss: 0.5321689677238465
  batch 900 loss: 0.5734745037555694
LOSS train 0.57347 valid 0.76039, valid PER 22.97%
EPOCH 14:
  batch 50 loss: 0.4756784200668335
  batch 100 loss: 0.47722873151302336
  batch 150 loss: 0.49905753016471865
  batch 200 loss: 0.4926674735546112
  batch 250 loss: 0.48237609982490537
  batch 300 loss: 0.47712558925151827
  batch 350 loss: 0.5102772909402847
  batch 400 loss: 0.5470970404148102
  batch 450 loss: 0.5772374832630157
  batch 500 loss: 0.521030238866806
  batch 550 loss: 0.5476536226272583
  batch 600 loss: 0.556902796626091
  batch 650 loss: 0.5557475769519806
  batch 700 loss: 0.5536028099060059
  batch 750 loss: 0.5380497938394546
  batch 800 loss: 0.594777165055275
  batch 850 loss: 0.5893856817483902
  batch 900 loss: 0.5600986927747726
LOSS train 0.56010 valid 0.78145, valid PER 23.40%
EPOCH 15:
  batch 50 loss: 0.4599076062440872
  batch 100 loss: 0.4635429736971855
  batch 150 loss: 0.4656468850374222
  batch 200 loss: 0.4697243815660477
  batch 250 loss: 0.4904012095928192
  batch 300 loss: 0.49017037510871886
  batch 350 loss: 0.4640208625793457
  batch 400 loss: 0.4840127545595169
  batch 450 loss: 0.4927284038066864
  batch 500 loss: 0.48827900886535647
  batch 550 loss: 0.5490222191810608
  batch 600 loss: 0.547284836769104
  batch 650 loss: 0.5111042594909668
  batch 700 loss: 0.5295363050699234
  batch 750 loss: 0.5259037232398986
  batch 800 loss: 0.4849888637661934
  batch 850 loss: 0.4917601764202118
  batch 900 loss: 0.5012697160243988
LOSS train 0.50127 valid 0.79636, valid PER 23.45%
EPOCH 16:
  batch 50 loss: 0.44490762054920197
  batch 100 loss: 0.43203909635543825
  batch 150 loss: 0.43619919538497925
  batch 200 loss: 0.45171008974313737
  batch 250 loss: 0.4578782749176025
  batch 300 loss: 0.46759645998477933
  batch 350 loss: 0.48808430939912795
  batch 400 loss: 0.4656000256538391
  batch 450 loss: 0.46852274060249327
  batch 500 loss: 0.4629862093925476
  batch 550 loss: 0.46048456788063047
  batch 600 loss: 0.4986147749423981
  batch 650 loss: 0.49846018821001054
  batch 700 loss: 0.4531391894817352
  batch 750 loss: 0.46546588122844695
  batch 800 loss: 0.4678146040439606
  batch 850 loss: 0.494622055888176
  batch 900 loss: 0.48772303462028505
LOSS train 0.48772 valid 0.78608, valid PER 22.86%
EPOCH 17:
  batch 50 loss: 0.44168114602565767
  batch 100 loss: 0.3898231440782547
  batch 150 loss: 0.4343626439571381
  batch 200 loss: 0.4014643782377243
  batch 250 loss: 0.4262810754776001
  batch 300 loss: 0.39563146770000457
  batch 350 loss: 0.4445073878765106
  batch 400 loss: 0.44541327297687533
  batch 450 loss: 0.4523404157161713
  batch 500 loss: 0.4792514616250992
  batch 550 loss: 0.44221905410289764
  batch 600 loss: 0.4862547394633293
  batch 650 loss: 0.4577878785133362
  batch 700 loss: 0.4766585314273834
  batch 750 loss: 0.46261723458766935
  batch 800 loss: 0.4603110921382904
  batch 850 loss: 0.46300615549087526
  batch 900 loss: 0.4386237809062004
LOSS train 0.43862 valid 0.76288, valid PER 21.83%
EPOCH 18:
  batch 50 loss: 0.36388336986303327
  batch 100 loss: 0.37038420379161835
  batch 150 loss: 0.4284885996580124
  batch 200 loss: 0.3880334573984146
  batch 250 loss: 0.41481960982084276
  batch 300 loss: 0.42116709411144254
  batch 350 loss: 0.41113343119621276
  batch 400 loss: 0.3989210441708565
  batch 450 loss: 0.43002652764320376
  batch 500 loss: 0.44763175785541537
  batch 550 loss: 0.454729779958725
  batch 600 loss: 0.4570214486122131
  batch 650 loss: 0.44048950642347334
  batch 700 loss: 0.45190893948078154
  batch 750 loss: 0.44261758387088773
  batch 800 loss: 0.47190321385860445
  batch 850 loss: 0.4382737463712692
  batch 900 loss: 0.46604799091815946
LOSS train 0.46605 valid 0.79028, valid PER 22.51%
EPOCH 19:
  batch 50 loss: 0.420247882604599
  batch 100 loss: 0.4034669029712677
  batch 150 loss: 0.3845522630214691
  batch 200 loss: 0.37373954623937605
  batch 250 loss: 0.4135055774450302
  batch 300 loss: 0.41646628141403197
  batch 350 loss: 0.4647106236219406
  batch 400 loss: 0.42387823104858396
  batch 450 loss: 0.41238358795642854
  batch 500 loss: 0.44171490520238876
  batch 550 loss: 0.46151759803295134
  batch 600 loss: 0.4414239251613617
  batch 650 loss: 0.45898307859897614
  batch 700 loss: 0.47753658652305603
  batch 750 loss: 0.43335706293582915
  batch 800 loss: 0.4367009103298187
  batch 850 loss: 0.4617481243610382
  batch 900 loss: 0.484122799038887
LOSS train 0.48412 valid 0.81726, valid PER 23.48%
EPOCH 20:
  batch 50 loss: 0.401420314013958
  batch 100 loss: 0.38586132794618605
  batch 150 loss: 0.3748757070302963
  batch 200 loss: 0.4175300571322441
  batch 250 loss: 0.39642566710710525
  batch 300 loss: 0.4006509327888489
  batch 350 loss: 0.3718327811360359
  batch 400 loss: 0.3985172185301781
  batch 450 loss: 0.3984460884332657
  batch 500 loss: 0.4113933777809143
  batch 550 loss: 0.4240577441453934
  batch 600 loss: 0.43590972900390623
  batch 650 loss: 0.40958244204521177
  batch 700 loss: 0.4298563948273659
  batch 750 loss: 0.4506838074326515
  batch 800 loss: 0.4737907737493515
  batch 850 loss: 0.46319457709789275
  batch 900 loss: 0.47343077182769777
LOSS train 0.47343 valid 0.79841, valid PER 22.85%
[1.2505000984668733, 1.0324619579315186, 0.9644775676727295, 0.9209255313873291, 0.8137561666965485, 0.7787801241874694, 0.7085730791091919, 0.693095788359642, 0.632065759897232, 0.6318411523103714, 0.619883953332901, 0.5860932171344757, 0.5734745037555694, 0.5600986927747726, 0.5012697160243988, 0.48772303462028505, 0.4386237809062004, 0.46604799091815946, 0.484122799038887, 0.47343077182769777]
[1.274476408958435, 1.022940993309021, 1.0227441787719727, 0.924480140209198, 0.8806713819503784, 0.8197618126869202, 0.8013469576835632, 0.7950105667114258, 0.7883772253990173, 0.7825720906257629, 0.7890107035636902, 0.7800758481025696, 0.7603908181190491, 0.7814544439315796, 0.7963576912879944, 0.7860802412033081, 0.7628830671310425, 0.7902785539627075, 0.8172627687454224, 0.7984113693237305]
Training finished in 21.0 minutes.
Model saved to checkpoints/20230125_135644/model_13
Loading model from checkpoints/20230125_135644/model_13
SUB: 15.39%, DEL: 6.87%, INS: 2.22%, COR: 77.74%, PER: 24.48%
