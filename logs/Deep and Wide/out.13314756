Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=4, fbank_dims=23, model_dims=256, concat=1, lr=0.002, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.5, beta1=0.9, beta2=0.999)
Total number of model parameters is 5326888
EPOCH 1:
  batch 50 loss: 4.464115891456604
  batch 100 loss: 3.3146359586715697
  batch 150 loss: 3.281757049560547
  batch 200 loss: 3.270930709838867
  batch 250 loss: 3.1723547840118407
  batch 300 loss: 2.9516116523742677
  batch 350 loss: 2.879772415161133
  batch 400 loss: 2.8420780181884764
  batch 450 loss: 2.7404237508773805
  batch 500 loss: 2.611980481147766
  batch 550 loss: 2.5002538919448853
  batch 600 loss: 2.4409091901779174
  batch 650 loss: 2.335108070373535
  batch 700 loss: 2.335925736427307
  batch 750 loss: 2.3072867369651795
  batch 800 loss: 2.2287250399589538
  batch 850 loss: 2.192702531814575
  batch 900 loss: 2.102302496433258
LOSS train 2.10230 valid 2.01449, valid PER 62.03%
EPOCH 2:
  batch 50 loss: 2.0750403141975404
  batch 100 loss: 2.0048234391212465
  batch 150 loss: 1.918916153907776
  batch 200 loss: 1.8850827836990356
  batch 250 loss: 1.8789812564849853
  batch 300 loss: 1.8528105401992798
  batch 350 loss: 1.8026064658164977
  batch 400 loss: 1.7581631326675415
  batch 450 loss: 1.758616726398468
  batch 500 loss: 1.7497030305862427
  batch 550 loss: 1.7279083132743835
  batch 600 loss: 1.695730357170105
  batch 650 loss: 1.643664128780365
  batch 700 loss: 1.6470830297470094
  batch 750 loss: 1.6190467834472657
  batch 800 loss: 1.5605227851867676
  batch 850 loss: 1.534797282218933
  batch 900 loss: 1.5340253686904908
LOSS train 1.53403 valid 1.42190, valid PER 44.01%
EPOCH 3:
  batch 50 loss: 1.4525639295578003
  batch 100 loss: 1.515104055404663
  batch 150 loss: 1.5251708483695985
  batch 200 loss: 1.4689719462394715
  batch 250 loss: 1.4557910394668578
  batch 300 loss: 1.482876522541046
  batch 350 loss: 1.4654421949386596
  batch 400 loss: 1.4221463084220887
  batch 450 loss: 1.397027416229248
  batch 500 loss: 1.3849999403953552
  batch 550 loss: 1.3524408614635468
  batch 600 loss: 1.32309463262558
  batch 650 loss: 1.3205094289779664
  batch 700 loss: 1.3045475101470947
  batch 750 loss: 1.3488320541381835
  batch 800 loss: 1.3502542340755463
  batch 850 loss: 1.2976207542419433
  batch 900 loss: 1.3028852903842927
LOSS train 1.30289 valid 1.24880, valid PER 39.46%
EPOCH 4:
  batch 50 loss: 1.2800613415241242
  batch 100 loss: 1.2268870484828949
  batch 150 loss: 1.2521449756622314
  batch 200 loss: 1.2348717069625854
  batch 250 loss: 1.2371143221855163
  batch 300 loss: 1.2489588785171508
  batch 350 loss: 1.206110121011734
  batch 400 loss: 1.1942367935180664
  batch 450 loss: 1.191883591413498
  batch 500 loss: 1.2504272413253785
  batch 550 loss: 1.155571459531784
  batch 600 loss: 1.1552105617523194
  batch 650 loss: 1.244316828250885
  batch 700 loss: 1.2231191384792328
  batch 750 loss: 1.214087039232254
  batch 800 loss: 1.1895189332962035
  batch 850 loss: 1.1429708456993104
  batch 900 loss: 1.1877546405792236
LOSS train 1.18775 valid 1.10609, valid PER 34.88%
EPOCH 5:
  batch 50 loss: 1.1216399109363555
  batch 100 loss: 1.0956702506542206
  batch 150 loss: 1.160549864768982
  batch 200 loss: 1.185115031003952
  batch 250 loss: 1.1013307094573974
  batch 300 loss: 1.1365783953666686
  batch 350 loss: 1.0965099775791167
  batch 400 loss: 1.0684280109405517
  batch 450 loss: 1.1049132418632508
  batch 500 loss: 1.0840920209884644
  batch 550 loss: 1.146100434064865
  batch 600 loss: 1.0949969983100891
  batch 650 loss: 1.1619802474975587
  batch 700 loss: 1.1121658289432526
  batch 750 loss: 1.1106106042861938
  batch 800 loss: 1.1643779790401458
  batch 850 loss: 1.1416275596618652
  batch 900 loss: 1.0827633345127106
LOSS train 1.08276 valid 1.07632, valid PER 33.56%
EPOCH 6:
  batch 50 loss: 1.0634927773475646
  batch 100 loss: 1.0737969672679901
  batch 150 loss: 1.0626367485523225
  batch 200 loss: 1.0184029459953308
  batch 250 loss: 1.1020955777168273
  batch 300 loss: 1.1043507909774781
  batch 350 loss: 1.0846334779262543
  batch 400 loss: 1.0527103805541993
  batch 450 loss: 1.0606405019760132
  batch 500 loss: 1.0273714876174926
  batch 550 loss: 1.0461783230304718
  batch 600 loss: 1.022289981842041
  batch 650 loss: 1.0283089327812194
  batch 700 loss: 1.031106880903244
  batch 750 loss: 1.0749451112747193
  batch 800 loss: 1.04296182513237
  batch 850 loss: 1.0907311058044433
  batch 900 loss: 1.0849815320968628
LOSS train 1.08498 valid 0.99178, valid PER 31.39%
EPOCH 7:
  batch 50 loss: 0.9779467236995697
  batch 100 loss: 1.0280394911766053
  batch 150 loss: 0.9889506256580353
  batch 200 loss: 0.9908091998100281
  batch 250 loss: 1.0371740806102752
  batch 300 loss: 0.9943460714817047
  batch 350 loss: 1.0394407379627228
  batch 400 loss: 1.0174139487743377
  batch 450 loss: 1.0216976594924927
  batch 500 loss: 0.9858927428722382
  batch 550 loss: 0.969789320230484
  batch 600 loss: 1.0064614379405976
  batch 650 loss: 0.9727111709117889
  batch 700 loss: 1.039330962896347
  batch 750 loss: 0.9885155463218689
  batch 800 loss: 0.9754917860031128
  batch 850 loss: 0.9595625054836273
  batch 900 loss: 0.971430447101593
LOSS train 0.97143 valid 0.97454, valid PER 30.86%
EPOCH 8:
  batch 50 loss: 1.010505075454712
  batch 100 loss: 0.9652030420303345
  batch 150 loss: 0.9709013485908509
  batch 200 loss: 0.9552803766727448
  batch 250 loss: 0.9312636995315552
  batch 300 loss: 0.9313769936561584
  batch 350 loss: 0.9786530554294586
  batch 400 loss: 0.9204221153259278
  batch 450 loss: 0.9746090078353882
  batch 500 loss: 0.9543309676647186
  batch 550 loss: 0.9523342704772949
  batch 600 loss: 0.9030820143222809
  batch 650 loss: 0.9287206482887268
  batch 700 loss: 0.9452745699882508
  batch 750 loss: 0.9526457977294922
  batch 800 loss: 0.9644360172748566
  batch 850 loss: 0.958302743434906
  batch 900 loss: 0.9617966270446777
LOSS train 0.96180 valid 0.98609, valid PER 30.48%
EPOCH 9:
  batch 50 loss: 0.937364319562912
  batch 100 loss: 0.9032047986984253
  batch 150 loss: 0.9187400698661804
  batch 200 loss: 0.927877722978592
  batch 250 loss: 0.8695548629760742
  batch 300 loss: 0.9283342897891999
  batch 350 loss: 0.9060902428627015
  batch 400 loss: 0.938426764011383
  batch 450 loss: 0.932381649017334
  batch 500 loss: 0.8863467037677765
  batch 550 loss: 0.9075758159160614
  batch 600 loss: 0.9365699827671051
  batch 650 loss: 0.8991140413284302
  batch 700 loss: 0.881661604642868
  batch 750 loss: 0.9006842017173767
  batch 800 loss: 0.9416272282600403
  batch 850 loss: 0.9324251258373261
  batch 900 loss: 0.9049562239646911
LOSS train 0.90496 valid 0.90140, valid PER 28.28%
EPOCH 10:
  batch 50 loss: 0.8720453798770904
  batch 100 loss: 0.8929091906547546
  batch 150 loss: 0.8969499695301056
  batch 200 loss: 0.8606718218326569
  batch 250 loss: 0.8602504551410675
  batch 300 loss: 0.8429943764209747
  batch 350 loss: 0.8991012978553772
  batch 400 loss: 0.8697233963012695
  batch 450 loss: 0.9057677817344666
  batch 500 loss: 0.8821173644065857
  batch 550 loss: 0.888821576833725
  batch 600 loss: 0.8947249853610992
  batch 650 loss: 0.9156339895725251
  batch 700 loss: 0.9050587755441666
  batch 750 loss: 0.9455473363399506
  batch 800 loss: 0.9015623342990875
  batch 850 loss: 0.8749479019641876
  batch 900 loss: 0.8595833659172059
LOSS train 0.85958 valid 0.88253, valid PER 27.53%
EPOCH 11:
  batch 50 loss: 0.862301470041275
  batch 100 loss: 0.8207827293872834
  batch 150 loss: 0.8615541088581086
  batch 200 loss: 0.809036762714386
  batch 250 loss: 0.8186237835884094
  batch 300 loss: 0.8253340828418732
  batch 350 loss: 0.8942280173301697
  batch 400 loss: 0.8609884083271027
  batch 450 loss: 0.8682577073574066
  batch 500 loss: 0.844688116312027
  batch 550 loss: 0.8660504233837127
  batch 600 loss: 0.8603221487998962
  batch 650 loss: 0.9022606492042542
  batch 700 loss: 0.9591655468940735
  batch 750 loss: 0.8718441164493561
  batch 800 loss: 0.8706087064743042
  batch 850 loss: 0.8329758751392364
  batch 900 loss: 0.8837644016742706
LOSS train 0.88376 valid 0.87147, valid PER 27.33%
EPOCH 12:
  batch 50 loss: 0.8096287822723389
  batch 100 loss: 0.8224463820457458
  batch 150 loss: 0.8173180484771728
  batch 200 loss: 0.8423776090145111
  batch 250 loss: 0.8342588889598846
  batch 300 loss: 0.8386079621315002
  batch 350 loss: 0.8079282176494599
  batch 400 loss: 0.8275821745395661
  batch 450 loss: 0.8123047399520874
  batch 500 loss: 0.8346423518657684
  batch 550 loss: 0.8343214130401612
  batch 600 loss: 0.8451729834079742
  batch 650 loss: 0.8446577870845795
  batch 700 loss: 0.8417834031581879
  batch 750 loss: 0.8452711009979248
  batch 800 loss: 0.8201704120635986
  batch 850 loss: 0.8219161820411682
  batch 900 loss: 0.8436703372001648
LOSS train 0.84367 valid 0.86431, valid PER 26.16%
EPOCH 13:
  batch 50 loss: 0.7886473333835602
  batch 100 loss: 0.7967485284805298
  batch 150 loss: 0.81245924949646
  batch 200 loss: 0.7728612005710602
  batch 250 loss: 0.7923906290531159
  batch 300 loss: 0.8379922276735305
  batch 350 loss: 0.7834115874767303
  batch 400 loss: 0.7864466297626496
  batch 450 loss: 0.8389177232980728
  batch 500 loss: 0.814955837726593
  batch 550 loss: 0.8720509338378907
  batch 600 loss: 0.8432297646999359
  batch 650 loss: 0.8295382022857666
  batch 700 loss: 0.8658739423751831
  batch 750 loss: 0.8092423641681671
  batch 800 loss: 0.8307007145881653
  batch 850 loss: 0.8143395626544953
  batch 900 loss: 0.8196671599149704
LOSS train 0.81967 valid 0.82752, valid PER 26.06%
EPOCH 14:
  batch 50 loss: 0.7703783744573593
  batch 100 loss: 0.7787274301052094
  batch 150 loss: 0.7913510501384735
  batch 200 loss: 0.7586282324790955
  batch 250 loss: 0.7713822019100189
  batch 300 loss: 0.7864579606056213
  batch 350 loss: 0.7816706025600433
  batch 400 loss: 0.7929929745197296
  batch 450 loss: 0.7777917742729187
  batch 500 loss: 0.7705263030529023
  batch 550 loss: 0.8182963180541992
  batch 600 loss: 0.8018354368209839
  batch 650 loss: 0.8169666743278503
  batch 700 loss: 0.8146005916595459
  batch 750 loss: 0.8224172854423523
  batch 800 loss: 0.7985900694131851
  batch 850 loss: 0.8380423808097839
  batch 900 loss: 0.817461347579956
LOSS train 0.81746 valid 0.84818, valid PER 25.90%
EPOCH 15:
  batch 50 loss: 0.7548238450288772
  batch 100 loss: 0.7780803227424622
  batch 150 loss: 0.7871670579910278
  batch 200 loss: 0.7901379001140595
  batch 250 loss: 0.8117755925655366
  batch 300 loss: 0.7971261310577392
  batch 350 loss: 0.7942965936660766
  batch 400 loss: 0.7848712599277496
  batch 450 loss: 0.7679149889945984
  batch 500 loss: 0.7695378625392914
  batch 550 loss: 0.8447170543670655
  batch 600 loss: 0.8254683458805084
  batch 650 loss: 0.7700238555669785
  batch 700 loss: 0.7668914777040482
  batch 750 loss: 0.7730917322635651
  batch 800 loss: 0.776194338798523
  batch 850 loss: 0.7390715426206589
  batch 900 loss: 0.7519093078374862
LOSS train 0.75191 valid 0.85069, valid PER 26.69%
EPOCH 16:
  batch 50 loss: 0.7636211144924164
  batch 100 loss: 0.7444827234745026
  batch 150 loss: 0.7586956632137298
  batch 200 loss: 0.7630734777450562
  batch 250 loss: 0.7610208940505981
  batch 300 loss: 0.7776657819747925
  batch 350 loss: 0.7486245828866959
  batch 400 loss: 0.746702002286911
  batch 450 loss: 0.7665114188194275
  batch 500 loss: 0.7496828436851501
  batch 550 loss: 0.7359644532203674
  batch 600 loss: 0.7650793766975403
  batch 650 loss: 0.7699346518516541
  batch 700 loss: 0.7255315792560577
  batch 750 loss: 0.7682029497623444
  batch 800 loss: 0.7797568702697754
  batch 850 loss: 0.784186965227127
  batch 900 loss: 0.7818745052814484
LOSS train 0.78187 valid 0.81590, valid PER 25.09%
EPOCH 17:
  batch 50 loss: 0.7339893114566803
  batch 100 loss: 0.7032853996753693
  batch 150 loss: 0.771340423822403
  batch 200 loss: 0.7840389323234558
  batch 250 loss: 0.7842228031158447
  batch 300 loss: 0.7511835098266602
  batch 350 loss: 0.7538551074266434
  batch 400 loss: 0.7717788100242615
  batch 450 loss: 0.7396204477548599
  batch 500 loss: 0.7551969516277314
  batch 550 loss: 0.7481505811214447
  batch 600 loss: 0.7613700813055039
  batch 650 loss: 0.7711601042747498
  batch 700 loss: 0.7780812096595764
  batch 750 loss: 0.7379870587587356
  batch 800 loss: 0.7722703635692596
  batch 850 loss: 0.7515152752399444
  batch 900 loss: 0.7353806298971176
LOSS train 0.73538 valid 0.80064, valid PER 24.40%
EPOCH 18:
  batch 50 loss: 0.7193239158391953
  batch 100 loss: 0.6935217088460922
  batch 150 loss: 0.743130441904068
  batch 200 loss: 0.7239484912157059
  batch 250 loss: 0.6998622953891754
  batch 300 loss: 0.7194825464487076
  batch 350 loss: 0.6840215808153153
  batch 400 loss: 0.6782290893793106
  batch 450 loss: 0.7138129639625549
  batch 500 loss: 0.7147944456338883
  batch 550 loss: 0.7581112653017044
  batch 600 loss: 0.7217729806900024
  batch 650 loss: 0.7170443767309189
  batch 700 loss: 0.7044568157196045
  batch 750 loss: 0.7257615733146667
  batch 800 loss: 0.7529410809278488
  batch 850 loss: 0.730219088792801
  batch 900 loss: 0.7097801673412323
LOSS train 0.70978 valid 0.78538, valid PER 24.45%
EPOCH 19:
  batch 50 loss: 0.7006390595436096
  batch 100 loss: 0.6963335198163986
  batch 150 loss: 0.675767098069191
  batch 200 loss: 0.676949252486229
  batch 250 loss: 0.7348729568719864
  batch 300 loss: 0.775615821480751
  batch 350 loss: 0.7457908260822296
  batch 400 loss: 0.7005514204502106
  batch 450 loss: 0.6819389545917511
  batch 500 loss: 0.6890982782840729
  batch 550 loss: 0.7323220402002335
  batch 600 loss: 0.7394968169927597
  batch 650 loss: 0.7480997025966645
  batch 700 loss: 0.7705090606212616
  batch 750 loss: 0.7187363868951797
  batch 800 loss: 0.7116380953788757
  batch 850 loss: 0.7124482560157775
  batch 900 loss: 0.69724436044693
LOSS train 0.69724 valid 0.78813, valid PER 23.72%
EPOCH 20:
  batch 50 loss: 0.6458850198984146
  batch 100 loss: 0.6837240678071975
  batch 150 loss: 0.6963159656524658
  batch 200 loss: 0.7008679646253586
  batch 250 loss: 0.7138154065608978
  batch 300 loss: 0.6944736915826798
  batch 350 loss: 0.6937022840976715
  batch 400 loss: 0.715892287492752
  batch 450 loss: 0.6809085273742675
  batch 500 loss: 0.7095277988910675
  batch 550 loss: 0.713568885922432
  batch 600 loss: 0.6883467078208924
  batch 650 loss: 0.7118796026706695
  batch 700 loss: 0.7103818976879119
  batch 750 loss: 0.7104185128211975
  batch 800 loss: 0.7204438883066178
  batch 850 loss: 0.7342244952917099
  batch 900 loss: 0.6960416531562805
LOSS train 0.69604 valid 0.80855, valid PER 25.10%
[2.102302496433258, 1.5340253686904908, 1.3028852903842927, 1.1877546405792236, 1.0827633345127106, 1.0849815320968628, 0.971430447101593, 0.9617966270446777, 0.9049562239646911, 0.8595833659172059, 0.8837644016742706, 0.8436703372001648, 0.8196671599149704, 0.817461347579956, 0.7519093078374862, 0.7818745052814484, 0.7353806298971176, 0.7097801673412323, 0.69724436044693, 0.6960416531562805]
[2.0144925117492676, 1.421898365020752, 1.2487971782684326, 1.1060903072357178, 1.0763230323791504, 0.9917848706245422, 0.9745434522628784, 0.9860860109329224, 0.9013993144035339, 0.8825294375419617, 0.8714688420295715, 0.8643086552619934, 0.8275225758552551, 0.8481836318969727, 0.8506912589073181, 0.8159043192863464, 0.8006365895271301, 0.7853800654411316, 0.7881265878677368, 0.8085452318191528]
Training finished in 28.0 minutes.
Model saved to checkpoints/20230125_193852/model_18
Loading model from checkpoints/20230125_193852/model_18
SUB: 15.85%, DEL: 7.81%, INS: 1.71%, COR: 76.34%, PER: 25.37%
