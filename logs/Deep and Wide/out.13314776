Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=4, fbank_dims=23, model_dims=256, concat=1, lr=0.002, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.7, beta1=0.9, beta2=0.999)
Total number of model parameters is 5326888
EPOCH 1:
  batch 50 loss: 4.5000496053695676
  batch 100 loss: 3.3195169878005983
  batch 150 loss: 3.2840125513076783
  batch 200 loss: 3.27204619884491
  batch 250 loss: 3.1916468238830564
  batch 300 loss: 3.0331475591659545
  batch 350 loss: 2.981771721839905
  batch 400 loss: 2.9421731138229372
  batch 450 loss: 2.89719425201416
  batch 500 loss: 2.810296115875244
  batch 550 loss: 2.7422948980331423
  batch 600 loss: 2.701909427642822
  batch 650 loss: 2.633422198295593
  batch 700 loss: 2.609358854293823
  batch 750 loss: 2.5183678150177
  batch 800 loss: 2.438854970932007
  batch 850 loss: 2.3405981492996215
  batch 900 loss: 2.273066272735596
LOSS train 2.27307 valid 2.16423, valid PER 71.57%
EPOCH 2:
  batch 50 loss: 2.2283690071105955
  batch 100 loss: 2.1802618432044985
  batch 150 loss: 2.07267498254776
  batch 200 loss: 2.062928681373596
  batch 250 loss: 2.028799753189087
  batch 300 loss: 1.925054361820221
  batch 350 loss: 1.9030935215950011
  batch 400 loss: 1.8453306651115418
  batch 450 loss: 1.8117958831787109
  batch 500 loss: 1.820486557483673
  batch 550 loss: 1.7583171725273132
  batch 600 loss: 1.730479645729065
  batch 650 loss: 1.6849925494194031
  batch 700 loss: 1.700346164703369
  batch 750 loss: 1.6583388090133666
  batch 800 loss: 1.613411419391632
  batch 850 loss: 1.640108013153076
  batch 900 loss: 1.589465537071228
LOSS train 1.58947 valid 1.51413, valid PER 50.14%
EPOCH 3:
  batch 50 loss: 1.5224436831474304
  batch 100 loss: 1.5918325591087341
  batch 150 loss: 1.5936735272407532
  batch 200 loss: 1.5259140920639038
  batch 250 loss: 1.507193579673767
  batch 300 loss: 1.5073925948143005
  batch 350 loss: 1.5287913393974304
  batch 400 loss: 1.4975755953788756
  batch 450 loss: 1.4614382791519165
  batch 500 loss: 1.4632682776451111
  batch 550 loss: 1.463235468864441
  batch 600 loss: 1.395589165687561
  batch 650 loss: 1.4173095512390137
  batch 700 loss: 1.4177273535728454
  batch 750 loss: 1.4085721278190613
  batch 800 loss: 1.4653170776367188
  batch 850 loss: 1.392432794570923
  batch 900 loss: 1.3721299409866332
LOSS train 1.37213 valid 1.30045, valid PER 39.99%
EPOCH 4:
  batch 50 loss: 1.3662716841697693
  batch 100 loss: 1.2964012408256531
  batch 150 loss: 1.3318094778060914
  batch 200 loss: 1.3352494263648986
  batch 250 loss: 1.3507602739334106
  batch 300 loss: 1.3243973135948182
  batch 350 loss: 1.3065690636634826
  batch 400 loss: 1.279595308303833
  batch 450 loss: 1.2690327763557434
  batch 500 loss: 1.3263790822029113
  batch 550 loss: 1.2431715369224547
  batch 600 loss: 1.2509088349342345
  batch 650 loss: 1.325660502910614
  batch 700 loss: 1.3308995962142944
  batch 750 loss: 1.2911508500576019
  batch 800 loss: 1.2713380801677703
  batch 850 loss: 1.2340847671031951
  batch 900 loss: 1.2304547023773194
LOSS train 1.23045 valid 1.19285, valid PER 35.66%
EPOCH 5:
  batch 50 loss: 1.2460531461238862
  batch 100 loss: 1.187159653902054
  batch 150 loss: 1.238033540248871
  batch 200 loss: 1.2737028980255127
  batch 250 loss: 1.1933530795574188
  batch 300 loss: 1.2059978246688843
  batch 350 loss: 1.1825452554225921
  batch 400 loss: 1.1428331446647644
  batch 450 loss: 1.1808220100402833
  batch 500 loss: 1.1613519835472106
  batch 550 loss: 1.206555314064026
  batch 600 loss: 1.2017301094532014
  batch 650 loss: 1.1792458140850066
  batch 700 loss: 1.1690595662593841
  batch 750 loss: 1.1617445647716522
  batch 800 loss: 1.2135072815418244
  batch 850 loss: 1.1924332940578461
  batch 900 loss: 1.1715435552597047
LOSS train 1.17154 valid 1.13409, valid PER 33.62%
EPOCH 6:
  batch 50 loss: 1.1376307260990144
  batch 100 loss: 1.1533738100528717
  batch 150 loss: 1.127104914188385
  batch 200 loss: 1.0984445023536682
  batch 250 loss: 1.1382068991661072
  batch 300 loss: 1.152320476770401
  batch 350 loss: 1.1532912790775298
  batch 400 loss: 1.0977428591251372
  batch 450 loss: 1.147568130493164
  batch 500 loss: 1.0979183340072631
  batch 550 loss: 1.1354788386821746
  batch 600 loss: 1.118742299079895
  batch 650 loss: 1.0774027180671693
  batch 700 loss: 1.1114147424697876
  batch 750 loss: 1.120503830909729
  batch 800 loss: 1.123305207490921
  batch 850 loss: 1.114858158826828
  batch 900 loss: 1.1174525272846223
LOSS train 1.11745 valid 1.05618, valid PER 31.77%
EPOCH 7:
  batch 50 loss: 1.0585185706615448
  batch 100 loss: 1.0808634316921235
  batch 150 loss: 1.0510785138607026
  batch 200 loss: 1.0886916542053222
  batch 250 loss: 1.1409834027290344
  batch 300 loss: 1.0761032128334045
  batch 350 loss: 1.10470933675766
  batch 400 loss: 1.070420480966568
  batch 450 loss: 1.0787362217903138
  batch 500 loss: 1.0678045392036437
  batch 550 loss: 1.043592631816864
  batch 600 loss: 1.0626355481147767
  batch 650 loss: 1.0311428964138032
  batch 700 loss: 1.0845705473423004
  batch 750 loss: 1.0284448778629303
  batch 800 loss: 1.0443152749538422
  batch 850 loss: 1.0395829248428345
  batch 900 loss: 1.048703145980835
LOSS train 1.04870 valid 1.03413, valid PER 30.92%
EPOCH 8:
  batch 50 loss: 1.0620608174800872
  batch 100 loss: 0.9796261143684387
  batch 150 loss: 1.075239017009735
  batch 200 loss: 1.0613499104976654
  batch 250 loss: 1.0320219111442566
  batch 300 loss: 1.030298752784729
  batch 350 loss: 1.0265835785865784
  batch 400 loss: 1.038504240512848
  batch 450 loss: 1.09980877161026
  batch 500 loss: 1.0356865859031676
  batch 550 loss: 1.0460225462913513
  batch 600 loss: 1.014303971529007
  batch 650 loss: 1.0288781321048737
  batch 700 loss: 1.05014594912529
  batch 750 loss: 1.0689496850967408
  batch 800 loss: 1.0295752429962157
  batch 850 loss: 1.0151376247406005
  batch 900 loss: 1.0494917023181916
LOSS train 1.04949 valid 0.97963, valid PER 29.55%
EPOCH 9:
  batch 50 loss: 0.9829093205928803
  batch 100 loss: 0.9818481600284577
  batch 150 loss: 1.0037204551696777
  batch 200 loss: 1.0085662043094634
  batch 250 loss: 0.9933503437042236
  batch 300 loss: 0.9898833060264587
  batch 350 loss: 0.9743726909160614
  batch 400 loss: 1.023975808620453
  batch 450 loss: 1.0033554315567017
  batch 500 loss: 0.9954205346107483
  batch 550 loss: 0.9973760271072387
  batch 600 loss: 0.9868423247337341
  batch 650 loss: 0.9962940526008606
  batch 700 loss: 0.9827347993850708
  batch 750 loss: 0.9816733074188232
  batch 800 loss: 1.0157347571849824
  batch 850 loss: 1.0113970351219177
  batch 900 loss: 0.9886569046974182
LOSS train 0.98866 valid 0.97295, valid PER 29.12%
EPOCH 10:
  batch 50 loss: 0.963465541601181
  batch 100 loss: 1.0021965205669403
  batch 150 loss: 0.9985518431663514
  batch 200 loss: 0.9648719835281372
  batch 250 loss: 0.9424747252464294
  batch 300 loss: 0.9383644104003906
  batch 350 loss: 0.9408655905723572
  batch 400 loss: 0.9072930824756622
  batch 450 loss: 0.920466502904892
  batch 500 loss: 0.9500323748588562
  batch 550 loss: 0.990923763513565
  batch 600 loss: 0.9496741962432861
  batch 650 loss: 0.9963534200191497
  batch 700 loss: 1.0263710629940033
  batch 750 loss: 1.011358072757721
  batch 800 loss: 0.969627388715744
  batch 850 loss: 0.9617611718177795
  batch 900 loss: 0.9534660685062408
LOSS train 0.95347 valid 0.96517, valid PER 28.85%
EPOCH 11:
  batch 50 loss: 0.9633437204360962
  batch 100 loss: 0.9521150767803193
  batch 150 loss: 0.9471152019500733
  batch 200 loss: 0.8993782579898835
  batch 250 loss: 0.9136422181129455
  batch 300 loss: 0.9048475992679595
  batch 350 loss: 0.963115508556366
  batch 400 loss: 0.9309995722770691
  batch 450 loss: 0.94315190076828
  batch 500 loss: 0.9342110455036163
  batch 550 loss: 0.9501720488071441
  batch 600 loss: 0.9375122618675232
  batch 650 loss: 0.9598085463047028
  batch 700 loss: 0.9914200580120087
  batch 750 loss: 0.9626697850227356
  batch 800 loss: 0.9449244248867035
  batch 850 loss: 0.9307401919364929
  batch 900 loss: 0.95945445895195
LOSS train 0.95945 valid 0.95712, valid PER 27.96%
EPOCH 12:
  batch 50 loss: 0.9081142354011535
  batch 100 loss: 0.8784235823154449
  batch 150 loss: 0.9178385031223297
  batch 200 loss: 0.926101130247116
  batch 250 loss: 0.9181640589237213
  batch 300 loss: 0.936805181503296
  batch 350 loss: 0.8968041002750397
  batch 400 loss: 0.9093193280696868
  batch 450 loss: 0.8956798553466797
  batch 500 loss: 0.9215582823753357
  batch 550 loss: 0.8983392733335495
  batch 600 loss: 0.90914945602417
  batch 650 loss: 0.9261151683330536
  batch 700 loss: 0.9023665356636047
  batch 750 loss: 0.914757958650589
  batch 800 loss: 0.8762082064151764
  batch 850 loss: 0.910764548778534
  batch 900 loss: 0.92006831407547
LOSS train 0.92007 valid 0.94237, valid PER 27.93%
EPOCH 13:
  batch 50 loss: 0.8816682136058808
  batch 100 loss: 0.8779645442962647
  batch 150 loss: 0.900402227640152
  batch 200 loss: 0.8591713070869446
  batch 250 loss: 0.9013747251033783
  batch 300 loss: 0.9246811318397522
  batch 350 loss: 0.8974478328227997
  batch 400 loss: 0.9060391104221344
  batch 450 loss: 0.9031956017017364
  batch 500 loss: 0.9051674509048462
  batch 550 loss: 0.9577504563331604
  batch 600 loss: 0.9598659992218017
  batch 650 loss: 0.9107252740859986
  batch 700 loss: 0.9178007054328918
  batch 750 loss: 0.8839477968215942
  batch 800 loss: 0.910893098115921
  batch 850 loss: 0.8805556064844131
  batch 900 loss: 0.8791805267333984
LOSS train 0.87918 valid 0.90608, valid PER 26.96%
EPOCH 14:
  batch 50 loss: 0.8600893318653107
  batch 100 loss: 0.8602602696418762
  batch 150 loss: 0.8682288587093353
  batch 200 loss: 0.8573931932449341
  batch 250 loss: 0.8778476130962372
  batch 300 loss: 0.8510179948806763
  batch 350 loss: 0.8836097633838653
  batch 400 loss: 0.8944521152973175
  batch 450 loss: 0.8887503099441528
  batch 500 loss: 0.8887049329280853
  batch 550 loss: 0.9120046603679657
  batch 600 loss: 0.8729932522773742
  batch 650 loss: 0.9185639238357544
  batch 700 loss: 0.8895039296150208
  batch 750 loss: 0.8655773913860321
  batch 800 loss: 0.8682504725456238
  batch 850 loss: 0.9426642847061157
  batch 900 loss: 0.8933212482929229
LOSS train 0.89332 valid 0.86429, valid PER 25.53%
EPOCH 15:
  batch 50 loss: 0.8001817893981934
  batch 100 loss: 0.844319099187851
  batch 150 loss: 0.8567628121376037
  batch 200 loss: 0.8818049561977387
  batch 250 loss: 0.9039056921005248
  batch 300 loss: 0.8563227903842926
  batch 350 loss: 0.8651024222373962
  batch 400 loss: 0.85499032497406
  batch 450 loss: 0.8605684554576873
  batch 500 loss: 0.8401960837841034
  batch 550 loss: 0.9132577764987946
  batch 600 loss: 0.9010705268383026
  batch 650 loss: 0.8585214042663574
  batch 700 loss: 0.8488854956626892
  batch 750 loss: 0.8993680667877197
  batch 800 loss: 0.8538655126094818
  batch 850 loss: 0.8780563569068909
  batch 900 loss: 0.8123863106966018
LOSS train 0.81239 valid 0.87583, valid PER 26.01%
EPOCH 16:
  batch 50 loss: 0.8418194341659546
  batch 100 loss: 0.8241258394718171
  batch 150 loss: 0.8662376427650451
  batch 200 loss: 0.8898577404022217
  batch 250 loss: 0.8734661388397217
  batch 300 loss: 0.8768097698688507
  batch 350 loss: 0.9073540472984314
  batch 400 loss: 0.8551416754722595
  batch 450 loss: 0.8722989201545716
  batch 500 loss: 0.8514190638065338
  batch 550 loss: 0.8466556322574615
  batch 600 loss: 0.8549538516998291
  batch 650 loss: 0.8445569789409637
  batch 700 loss: 0.8173588764667511
  batch 750 loss: 0.8424510824680328
  batch 800 loss: 0.8336081957817078
  batch 850 loss: 0.8338355910778046
  batch 900 loss: 0.8619713544845581
LOSS train 0.86197 valid 0.87806, valid PER 25.74%
EPOCH 17:
  batch 50 loss: 0.8552796423435212
  batch 100 loss: 0.7873514449596405
  batch 150 loss: 0.9155821573734283
  batch 200 loss: 0.8069015884399414
  batch 250 loss: 0.8412985181808472
  batch 300 loss: 0.850217148065567
  batch 350 loss: 0.865413134098053
  batch 400 loss: 0.8316459619998932
  batch 450 loss: 0.8351189041137695
  batch 500 loss: 0.8644026243686675
  batch 550 loss: 0.8550972282886505
  batch 600 loss: 0.8944664144515991
  batch 650 loss: 0.8415444540977478
  batch 700 loss: 0.8361795175075531
  batch 750 loss: 0.8475124716758728
  batch 800 loss: 0.8577757394313812
  batch 850 loss: 0.8214934396743775
  batch 900 loss: 0.8252812469005585
LOSS train 0.82528 valid 0.85422, valid PER 25.20%
EPOCH 18:
  batch 50 loss: 0.8096871137619018
  batch 100 loss: 0.7863400453329086
  batch 150 loss: 0.8538250434398651
  batch 200 loss: 0.837361010313034
  batch 250 loss: 0.8013804602622986
  batch 300 loss: 0.8483457219600677
  batch 350 loss: 0.8104539048671723
  batch 400 loss: 0.8025680994987487
  batch 450 loss: 0.8225451517105102
  batch 500 loss: 0.8566224312782288
  batch 550 loss: 0.9078165411949157
  batch 600 loss: 0.8304023373126984
  batch 650 loss: 0.7974690860509872
  batch 700 loss: 0.7956859135627746
  batch 750 loss: 0.837012585401535
  batch 800 loss: 0.8637599050998688
  batch 850 loss: 0.8527345168590545
  batch 900 loss: 0.8316791236400605
LOSS train 0.83168 valid 0.88281, valid PER 26.14%
EPOCH 19:
  batch 50 loss: 0.8216665208339691
  batch 100 loss: 0.8258263754844666
  batch 150 loss: 0.7732381170988083
  batch 200 loss: 0.7771792900562287
  batch 250 loss: 0.822385927438736
  batch 300 loss: 0.8345206272602081
  batch 350 loss: 0.8284479475021362
  batch 400 loss: 0.8090869200229645
  batch 450 loss: 0.7748379087448121
  batch 500 loss: 0.7894714832305908
  batch 550 loss: 0.8210321497917176
  batch 600 loss: 0.8173109710216522
  batch 650 loss: 0.8330508410930634
  batch 700 loss: 0.8626834905147552
  batch 750 loss: 0.798739914894104
  batch 800 loss: 0.7835541129112243
  batch 850 loss: 0.8318763911724091
  batch 900 loss: 0.8088157010078431
LOSS train 0.80882 valid 0.87901, valid PER 25.76%
EPOCH 20:
  batch 50 loss: 0.7635865986347199
  batch 100 loss: 0.8045884132385254
  batch 150 loss: 0.8026665127277375
  batch 200 loss: 0.7990520465373993
  batch 250 loss: 0.8183106076717377
  batch 300 loss: 0.8154230880737304
  batch 350 loss: 0.7814434897899628
  batch 400 loss: 0.7957969796657562
  batch 450 loss: 0.797826601266861
  batch 500 loss: 0.8328773164749146
  batch 550 loss: 0.7933876252174378
  batch 600 loss: 0.7903308093547821
  batch 650 loss: 0.8217982113361358
  batch 700 loss: 0.8051449489593506
  batch 750 loss: 0.7979757797718048
  batch 800 loss: 0.8125869250297546
  batch 850 loss: 0.8420507526397705
  batch 900 loss: 0.7730139243602753
LOSS train 0.77301 valid 0.84468, valid PER 24.52%
[2.273066272735596, 1.589465537071228, 1.3721299409866332, 1.2304547023773194, 1.1715435552597047, 1.1174525272846223, 1.048703145980835, 1.0494917023181916, 0.9886569046974182, 0.9534660685062408, 0.95945445895195, 0.92006831407547, 0.8791805267333984, 0.8933212482929229, 0.8123863106966018, 0.8619713544845581, 0.8252812469005585, 0.8316791236400605, 0.8088157010078431, 0.7730139243602753]
[2.164233446121216, 1.5141304731369019, 1.3004493713378906, 1.19284987449646, 1.1340937614440918, 1.0561833381652832, 1.0341297388076782, 0.9796269536018372, 0.9729548692703247, 0.9651659727096558, 0.9571245312690735, 0.9423694610595703, 0.9060795307159424, 0.8642932772636414, 0.8758271336555481, 0.8780618906021118, 0.8542172312736511, 0.8828138709068298, 0.8790116906166077, 0.8446808457374573]
Training finished in 28.0 minutes.
Model saved to checkpoints/20230125_193852/model_20
Loading model from checkpoints/20230125_193852/model_20
SUB: 17.17%, DEL: 6.76%, INS: 2.63%, COR: 76.07%, PER: 26.55%
