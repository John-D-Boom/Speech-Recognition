Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=4, fbank_dims=23, model_dims=256, concat=1, lr=0.002, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.2, beta1=0.9, beta2=0.999)
Total number of model parameters is 5326888
EPOCH 1:
  batch 50 loss: 4.4534135437011715
  batch 100 loss: 3.329725618362427
  batch 150 loss: 3.2911502647399904
  batch 200 loss: 3.282881007194519
  batch 250 loss: 3.237528529167175
  batch 300 loss: 3.117989501953125
  batch 350 loss: 3.0020495891571044
  batch 400 loss: 2.9529007625579835
  batch 450 loss: 2.865572657585144
  batch 500 loss: 2.7743763875961305
  batch 550 loss: 2.6641176557540893
  batch 600 loss: 2.6039583921432494
  batch 650 loss: 2.4874788999557493
  batch 700 loss: 2.4684071207046507
  batch 750 loss: 2.366120319366455
  batch 800 loss: 2.3120036792755125
  batch 850 loss: 2.2054579877853393
  batch 900 loss: 2.0745838046073914
LOSS train 2.07458 valid 1.95330, valid PER 68.98%
EPOCH 2:
  batch 50 loss: 1.9734017062187195
  batch 100 loss: 1.8869752430915832
  batch 150 loss: 1.7718293571472168
  batch 200 loss: 1.714856095314026
  batch 250 loss: 1.7096721267700195
  batch 300 loss: 1.6233625888824463
  batch 350 loss: 1.6161533617973327
  batch 400 loss: 1.5920548963546752
  batch 450 loss: 1.5168432664871216
  batch 500 loss: 1.5206472373008728
  batch 550 loss: 1.497331018447876
  batch 600 loss: 1.4408981418609619
  batch 650 loss: 1.3977595829963685
  batch 700 loss: 1.4033417916297912
  batch 750 loss: 1.3908972573280334
  batch 800 loss: 1.3345584082603454
  batch 850 loss: 1.3188888645172119
  batch 900 loss: 1.3054697465896608
LOSS train 1.30547 valid 1.25236, valid PER 39.27%
EPOCH 3:
  batch 50 loss: 1.2196751320362091
  batch 100 loss: 1.2955780267715453
  batch 150 loss: 1.2911315059661865
  batch 200 loss: 1.2138612246513367
  batch 250 loss: 1.1980076336860657
  batch 300 loss: 1.2453469216823578
  batch 350 loss: 1.2461019706726075
  batch 400 loss: 1.216672319173813
  batch 450 loss: 1.2007999300956727
  batch 500 loss: 1.1388796639442444
  batch 550 loss: 1.173101110458374
  batch 600 loss: 1.1292977809906006
  batch 650 loss: 1.148514177799225
  batch 700 loss: 1.1351166117191314
  batch 750 loss: 1.1359801495075226
  batch 800 loss: 1.1652739262580871
  batch 850 loss: 1.1106326067447663
  batch 900 loss: 1.1296893286705016
LOSS train 1.12969 valid 1.10110, valid PER 33.87%
EPOCH 4:
  batch 50 loss: 1.1064273416996002
  batch 100 loss: 1.0297923517227172
  batch 150 loss: 1.0698815131187438
  batch 200 loss: 1.063100870847702
  batch 250 loss: 1.089598754644394
  batch 300 loss: 1.0968766987323761
  batch 350 loss: 1.06485435962677
  batch 400 loss: 1.011974105834961
  batch 450 loss: 1.0335219776630402
  batch 500 loss: 1.1240739452838897
  batch 550 loss: 1.004991616010666
  batch 600 loss: 0.9979037523269654
  batch 650 loss: 1.04418509721756
  batch 700 loss: 1.0352946758270263
  batch 750 loss: 0.9998356211185455
  batch 800 loss: 1.0070833480358123
  batch 850 loss: 1.0154096376895905
  batch 900 loss: 1.0383413434028625
LOSS train 1.03834 valid 1.02188, valid PER 30.86%
EPOCH 5:
  batch 50 loss: 0.9780276501178742
  batch 100 loss: 0.9415727829933167
  batch 150 loss: 0.9788007020950318
  batch 200 loss: 1.0201813900470733
  batch 250 loss: 0.9352786815166474
  batch 300 loss: 1.0099678647518158
  batch 350 loss: 0.9462599515914917
  batch 400 loss: 0.9356461250782013
  batch 450 loss: 0.948918377161026
  batch 500 loss: 0.9399217820167541
  batch 550 loss: 0.972301721572876
  batch 600 loss: 0.9445359671115875
  batch 650 loss: 0.9682739281654358
  batch 700 loss: 0.927851448059082
  batch 750 loss: 0.9271924924850464
  batch 800 loss: 0.9606735551357269
  batch 850 loss: 0.9680742073059082
  batch 900 loss: 0.9409616625308991
LOSS train 0.94096 valid 0.98132, valid PER 29.80%
EPOCH 6:
  batch 50 loss: 0.9218957042694091
  batch 100 loss: 0.9372846949100494
  batch 150 loss: 0.9104033839702607
  batch 200 loss: 0.8941967713832856
  batch 250 loss: 0.909039454460144
  batch 300 loss: 0.9078810846805573
  batch 350 loss: 0.9198851382732391
  batch 400 loss: 0.9016790604591369
  batch 450 loss: 0.9239827620983124
  batch 500 loss: 0.8828101456165314
  batch 550 loss: 0.8950716090202332
  batch 600 loss: 0.9214488971233368
  batch 650 loss: 0.8798526966571808
  batch 700 loss: 0.873301875591278
  batch 750 loss: 0.8943571281433106
  batch 800 loss: 0.8823319911956787
  batch 850 loss: 0.9035658705234527
  batch 900 loss: 0.8962687492370606
LOSS train 0.89627 valid 0.90742, valid PER 28.16%
EPOCH 7:
  batch 50 loss: 0.8356770348548889
  batch 100 loss: 0.8617427241802216
  batch 150 loss: 0.8144730770587921
  batch 200 loss: 0.8218245899677277
  batch 250 loss: 0.8861656284332275
  batch 300 loss: 0.8419727921485901
  batch 350 loss: 0.8770463669300079
  batch 400 loss: 0.8108297181129456
  batch 450 loss: 0.8292897021770478
  batch 500 loss: 0.8069941985607147
  batch 550 loss: 0.8219818949699402
  batch 600 loss: 0.8345999991893769
  batch 650 loss: 0.8079687941074372
  batch 700 loss: 0.8734653878211975
  batch 750 loss: 0.8278971433639526
  batch 800 loss: 0.8229225528240204
  batch 850 loss: 0.8093028771877289
  batch 900 loss: 0.7995146000385285
LOSS train 0.79951 valid 0.89720, valid PER 26.99%
EPOCH 8:
  batch 50 loss: 0.7975570249557495
  batch 100 loss: 0.786508903503418
  batch 150 loss: 0.8197542560100556
  batch 200 loss: 0.764816609621048
  batch 250 loss: 0.7863841789960861
  batch 300 loss: 0.7713727986812592
  batch 350 loss: 0.7698856127262116
  batch 400 loss: 0.8077542209625244
  batch 450 loss: 0.875752557516098
  batch 500 loss: 0.8361756527423858
  batch 550 loss: 0.807253030538559
  batch 600 loss: 0.7813545417785644
  batch 650 loss: 0.8089231812953949
  batch 700 loss: 0.8352991366386413
  batch 750 loss: 0.7908518683910369
  batch 800 loss: 0.825758912563324
  batch 850 loss: 0.7894962084293365
  batch 900 loss: 0.8172869551181793
LOSS train 0.81729 valid 0.85537, valid PER 26.48%
EPOCH 9:
  batch 50 loss: 0.728030896782875
  batch 100 loss: 0.7199603497982026
  batch 150 loss: 0.7470224153995514
  batch 200 loss: 0.7781032443046569
  batch 250 loss: 0.7457041919231415
  batch 300 loss: 0.7728921282291412
  batch 350 loss: 0.7427912348508835
  batch 400 loss: 0.7726354944705963
  batch 450 loss: 0.7862265956401825
  batch 500 loss: 0.7420789802074432
  batch 550 loss: 0.7530838429927826
  batch 600 loss: 0.8164051496982574
  batch 650 loss: 0.7851810204982758
  batch 700 loss: 0.7738898849487305
  batch 750 loss: 0.7737247681617737
  batch 800 loss: 0.8294816744327546
  batch 850 loss: 0.8092427742481232
  batch 900 loss: 0.7578574919700622
LOSS train 0.75786 valid 0.82602, valid PER 25.52%
EPOCH 10:
  batch 50 loss: 0.7005188632011413
  batch 100 loss: 0.7182102864980697
  batch 150 loss: 0.7339051753282547
  batch 200 loss: 0.7053541243076324
  batch 250 loss: 0.7312605977058411
  batch 300 loss: 0.7962694323062897
  batch 350 loss: 0.7476481866836547
  batch 400 loss: 0.72527563393116
  batch 450 loss: 0.7484771251678467
  batch 500 loss: 0.7359283304214478
  batch 550 loss: 0.7710226893424987
  batch 600 loss: 0.7436044776439666
  batch 650 loss: 0.7625805068016053
  batch 700 loss: 0.769535038471222
  batch 750 loss: 0.7491800498962402
  batch 800 loss: 0.7338044965267181
  batch 850 loss: 0.7135511565208436
  batch 900 loss: 0.7233894324302673
LOSS train 0.72339 valid 0.83331, valid PER 25.90%
EPOCH 11:
  batch 50 loss: 0.6978661072254181
  batch 100 loss: 0.65577834546566
  batch 150 loss: 0.6947035467624665
  batch 200 loss: 0.6561708825826645
  batch 250 loss: 0.6865760380029678
  batch 300 loss: 0.7164914548397064
  batch 350 loss: 0.7166766691207885
  batch 400 loss: 0.6636306405067444
  batch 450 loss: 0.6843439453840255
  batch 500 loss: 0.692104452252388
  batch 550 loss: 0.7083898663520813
  batch 600 loss: 0.6828386950492858
  batch 650 loss: 0.7158018988370896
  batch 700 loss: 0.7693474233150482
  batch 750 loss: 0.7016624945402146
  batch 800 loss: 0.7487358665466308
  batch 850 loss: 0.7002565145492554
  batch 900 loss: 0.7167970764636994
LOSS train 0.71680 valid 0.78685, valid PER 24.18%
EPOCH 12:
  batch 50 loss: 0.6202783334255219
  batch 100 loss: 0.6394289124011994
  batch 150 loss: 0.6450263738632203
  batch 200 loss: 0.6771625459194184
  batch 250 loss: 0.7150898909568787
  batch 300 loss: 0.6768383669853211
  batch 350 loss: 0.6584542924165726
  batch 400 loss: 0.6643341374397278
  batch 450 loss: 0.6381649476289749
  batch 500 loss: 0.6686835956573486
  batch 550 loss: 0.676836267709732
  batch 600 loss: 0.7211058986186981
  batch 650 loss: 0.6963946008682251
  batch 700 loss: 0.6999905449151993
  batch 750 loss: 0.7291701287031174
  batch 800 loss: 0.6928989690542221
  batch 850 loss: 0.710522329211235
  batch 900 loss: 0.7182814633846283
LOSS train 0.71828 valid 0.80407, valid PER 24.49%
EPOCH 13:
  batch 50 loss: 0.6626736122369766
  batch 100 loss: 0.639123130440712
  batch 150 loss: 0.6451183730363845
  batch 200 loss: 0.6115114837884903
  batch 250 loss: 0.6586776745319366
  batch 300 loss: 0.6752981007099151
  batch 350 loss: 0.619547700881958
  batch 400 loss: 0.6294782680273056
  batch 450 loss: 0.653989983201027
  batch 500 loss: 0.638405287861824
  batch 550 loss: 0.6853152757883072
  batch 600 loss: 0.65959627866745
  batch 650 loss: 0.6516769295930862
  batch 700 loss: 0.6571753072738648
  batch 750 loss: 0.648905081152916
  batch 800 loss: 0.6658707141876221
  batch 850 loss: 0.6463455247879029
  batch 900 loss: 0.6629798239469529
LOSS train 0.66298 valid 0.77778, valid PER 23.64%
EPOCH 14:
  batch 50 loss: 0.5984861725568771
  batch 100 loss: 0.5779807925224304
  batch 150 loss: 0.6135518020391464
  batch 200 loss: 0.6241180735826493
  batch 250 loss: 0.6237931180000306
  batch 300 loss: 0.590085877776146
  batch 350 loss: 0.6146774220466614
  batch 400 loss: 0.6635286808013916
  batch 450 loss: 0.6247779750823974
  batch 500 loss: 0.6221123945713043
  batch 550 loss: 0.6157467663288116
  batch 600 loss: 0.6081223785877228
  batch 650 loss: 0.6485047209262848
  batch 700 loss: 0.6359073066711426
  batch 750 loss: 0.6210659676790238
  batch 800 loss: 0.6277344000339508
  batch 850 loss: 0.6674472123384476
  batch 900 loss: 0.6431343024969101
LOSS train 0.64313 valid 0.77161, valid PER 23.16%
EPOCH 15:
  batch 50 loss: 0.56682357609272
  batch 100 loss: 0.574087660908699
  batch 150 loss: 0.6061275345087052
  batch 200 loss: 0.6043335992097855
  batch 250 loss: 0.6454764837026596
  batch 300 loss: 0.6008481192588806
  batch 350 loss: 0.6194094061851502
  batch 400 loss: 0.6001830357313156
  batch 450 loss: 0.5953156799077988
  batch 500 loss: 0.5933041417598724
  batch 550 loss: 0.6410714799165725
  batch 600 loss: 0.6490417093038559
  batch 650 loss: 0.6043970942497253
  batch 700 loss: 0.5832035744190216
  batch 750 loss: 0.6231623470783234
  batch 800 loss: 0.6206612712144852
  batch 850 loss: 0.6227640569210052
  batch 900 loss: 0.5856949150562286
LOSS train 0.58569 valid 0.76070, valid PER 23.18%
EPOCH 16:
  batch 50 loss: 0.569731924533844
  batch 100 loss: 0.5652466577291488
  batch 150 loss: 0.588106842637062
  batch 200 loss: 0.6096238249540329
  batch 250 loss: 0.5835692751407623
  batch 300 loss: 0.5732303559780121
  batch 350 loss: 0.5985766911506653
  batch 400 loss: 0.5907985597848893
  batch 450 loss: 0.5881138622760773
  batch 500 loss: 0.5695114904642105
  batch 550 loss: 0.5708317327499389
  batch 600 loss: 0.6298341143131256
  batch 650 loss: 0.5954885184764862
  batch 700 loss: 0.5367446368932725
  batch 750 loss: 0.5936547553539276
  batch 800 loss: 0.5760964441299439
  batch 850 loss: 0.5884690797328949
  batch 900 loss: 0.6120894712209701
LOSS train 0.61209 valid 0.76453, valid PER 23.00%
EPOCH 17:
  batch 50 loss: 0.5646927398443222
  batch 100 loss: 0.5228779816627502
  batch 150 loss: 0.5954284846782685
  batch 200 loss: 0.5287987679243088
  batch 250 loss: 0.5545797818899154
  batch 300 loss: 0.5490505486726761
  batch 350 loss: 0.5788875287771225
  batch 400 loss: 0.5632178896665573
  batch 450 loss: 0.5506437289714813
  batch 500 loss: 0.5971965038776398
  batch 550 loss: 0.5690975701808929
  batch 600 loss: 0.5795606851577759
  batch 650 loss: 0.5509139895439148
  batch 700 loss: 0.5861666285991669
  batch 750 loss: 0.5533452910184861
  batch 800 loss: 0.5794872945547104
  batch 850 loss: 0.606012545824051
  batch 900 loss: 0.5989469450712204
LOSS train 0.59895 valid 0.75175, valid PER 22.77%
EPOCH 18:
  batch 50 loss: 0.523378284573555
  batch 100 loss: 0.5491360807418824
  batch 150 loss: 0.5784816682338715
  batch 200 loss: 0.5542765039205552
  batch 250 loss: 0.5467437583208085
  batch 300 loss: 0.5386903989315033
  batch 350 loss: 0.5274305695295334
  batch 400 loss: 0.5559340298175812
  batch 450 loss: 0.5640779328346253
  batch 500 loss: 0.5552821916341781
  batch 550 loss: 0.6080990391969681
  batch 600 loss: 0.5562244129180908
  batch 650 loss: 0.5360856223106384
  batch 700 loss: 0.5596115660667419
  batch 750 loss: 0.5573466277122497
  batch 800 loss: 0.5661036431789398
  batch 850 loss: 0.5713712865114212
  batch 900 loss: 0.5570211091637611
LOSS train 0.55702 valid 0.73978, valid PER 22.31%
EPOCH 19:
  batch 50 loss: 0.5094751000404358
  batch 100 loss: 0.5030904173851013
  batch 150 loss: 0.4817359972000122
  batch 200 loss: 0.4978857856988907
  batch 250 loss: 0.5321162539720535
  batch 300 loss: 0.5636495459079742
  batch 350 loss: 0.5799241936206818
  batch 400 loss: 0.5418083888292312
  batch 450 loss: 0.48432170689105986
  batch 500 loss: 0.4860198479890823
  batch 550 loss: 0.5544470411539077
  batch 600 loss: 0.5674323523044587
  batch 650 loss: 0.5647226876020431
  batch 700 loss: 0.5959568786621093
  batch 750 loss: 0.5712895280122757
  batch 800 loss: 0.5441298735141754
  batch 850 loss: 0.5804863542318344
  batch 900 loss: 0.5476196897029877
LOSS train 0.54762 valid 0.75341, valid PER 22.46%
EPOCH 20:
  batch 50 loss: 0.49496635973453523
  batch 100 loss: 0.5046487975120545
  batch 150 loss: 0.5092916506528854
  batch 200 loss: 0.5320348137617111
  batch 250 loss: 0.5249516588449478
  batch 300 loss: 0.5439953392744065
  batch 350 loss: 0.4964436024427414
  batch 400 loss: 0.5565523201227188
  batch 450 loss: 0.5466507822275162
  batch 500 loss: 0.5345104646682739
  batch 550 loss: 0.5247638070583344
  batch 600 loss: 0.5111356097459793
  batch 650 loss: 0.5376255768537521
  batch 700 loss: 0.514205424785614
  batch 750 loss: 0.514855260848999
  batch 800 loss: 0.5399657553434372
  batch 850 loss: 0.5251686871051788
  batch 900 loss: 0.5136241710186005
LOSS train 0.51362 valid 0.73593, valid PER 21.62%
[2.0745838046073914, 1.3054697465896608, 1.1296893286705016, 1.0383413434028625, 0.9409616625308991, 0.8962687492370606, 0.7995146000385285, 0.8172869551181793, 0.7578574919700622, 0.7233894324302673, 0.7167970764636994, 0.7182814633846283, 0.6629798239469529, 0.6431343024969101, 0.5856949150562286, 0.6120894712209701, 0.5989469450712204, 0.5570211091637611, 0.5476196897029877, 0.5136241710186005]
[1.953304409980774, 1.2523640394210815, 1.1011039018630981, 1.0218766927719116, 0.9813157320022583, 0.9074165225028992, 0.8972031474113464, 0.8553664088249207, 0.8260244727134705, 0.8333130478858948, 0.7868470549583435, 0.804074764251709, 0.777782142162323, 0.7716110944747925, 0.7606995105743408, 0.7645342350006104, 0.7517539858818054, 0.7397842407226562, 0.7534124851226807, 0.7359310984611511]
Training finished in 26.0 minutes.
Model saved to checkpoints/20230125_110309/model_20
Loading model from checkpoints/20230125_110309/model_20
SUB: 15.17%, DEL: 5.23%, INS: 3.38%, COR: 79.61%, PER: 23.77%
