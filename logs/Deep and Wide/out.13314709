Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=512, concat=1, lr=0.002, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.2, beta1=0.9, beta2=0.999)
Total number of model parameters is 8540200
EPOCH 1:
  batch 50 loss: 5.607069845199585
  batch 100 loss: 4.6364690208435055
  batch 150 loss: 4.4990192222595216
  batch 200 loss: 3.6713078355789186
  batch 250 loss: 3.6578301048278807
  batch 300 loss: 3.4918049573898315
  batch 350 loss: 3.4224131393432615
  batch 400 loss: 3.537270231246948
  batch 450 loss: 3.596439228057861
  batch 500 loss: 3.8674271869659425
  batch 550 loss: 3.4994215154647828
  batch 600 loss: 3.337270064353943
  batch 650 loss: 3.285178666114807
  batch 700 loss: 3.2604606199264525
  batch 750 loss: 3.215280838012695
  batch 800 loss: 3.1630254554748536
  batch 850 loss: 2.9549453401565553
  batch 900 loss: 2.79811044216156
LOSS train 2.79811 valid 2.72731, valid PER 78.95%
EPOCH 2:
  batch 50 loss: 2.6707873487472535
  batch 100 loss: 2.6026105880737305
  batch 150 loss: 2.4457836866378786
  batch 200 loss: 2.408641257286072
  batch 250 loss: 2.384592490196228
  batch 300 loss: 2.2751277017593385
  batch 350 loss: 2.1907612705230712
  batch 400 loss: 2.136069552898407
  batch 450 loss: 2.0913261914253236
  batch 500 loss: 2.019355442523956
  batch 550 loss: 1.9614255499839783
  batch 600 loss: 1.9355695199966432
  batch 650 loss: 1.8586488127708436
  batch 700 loss: 1.8296787810325623
  batch 750 loss: 1.813314778804779
  batch 800 loss: 1.7669207429885865
  batch 850 loss: 1.74322566986084
  batch 900 loss: 1.6753829503059388
LOSS train 1.67538 valid 1.62032, valid PER 50.58%
EPOCH 3:
  batch 50 loss: 1.5903406977653503
  batch 100 loss: 1.6595629811286927
  batch 150 loss: 1.6509346079826355
  batch 200 loss: 1.5655958938598633
  batch 250 loss: 1.5677860260009766
  batch 300 loss: 1.5529778575897217
  batch 350 loss: 1.5703587341308594
  batch 400 loss: 1.5104209995269775
  batch 450 loss: 1.4660082387924194
  batch 500 loss: 1.4436901903152466
  batch 550 loss: 1.4651265811920167
  batch 600 loss: 1.4016849040985107
  batch 650 loss: 1.4025157952308656
  batch 700 loss: 1.3937741994857789
  batch 750 loss: 1.4122091341018677
  batch 800 loss: 1.425994588136673
  batch 850 loss: 1.3702093315124513
  batch 900 loss: 1.3705501985549926
LOSS train 1.37055 valid 1.39366, valid PER 43.03%
EPOCH 4:
  batch 50 loss: 1.3632235550880432
  batch 100 loss: 1.2709344792366029
  batch 150 loss: 1.2919247961044311
  batch 200 loss: 1.3002977335453034
  batch 250 loss: 1.2987322437763213
  batch 300 loss: 1.298437626361847
  batch 350 loss: 1.246775517463684
  batch 400 loss: 1.234308830499649
  batch 450 loss: 1.260331962108612
  batch 500 loss: 1.2895277953147888
  batch 550 loss: 1.2071809220314025
  batch 600 loss: 1.195150363445282
  batch 650 loss: 1.26577486038208
  batch 700 loss: 1.2475761318206786
  batch 750 loss: 1.194567551612854
  batch 800 loss: 1.1828477227687835
  batch 850 loss: 1.1873647058010102
  batch 900 loss: 1.1901905643939972
LOSS train 1.19019 valid 1.19551, valid PER 37.85%
EPOCH 5:
  batch 50 loss: 1.1506905698776244
  batch 100 loss: 1.1240999293327332
  batch 150 loss: 1.1395276284217835
  batch 200 loss: 1.1672113990783692
  batch 250 loss: 1.1313819694519043
  batch 300 loss: 1.1462160444259644
  batch 350 loss: 1.0911334228515626
  batch 400 loss: 1.0878654170036315
  batch 450 loss: 1.092568621635437
  batch 500 loss: 1.0794354474544525
  batch 550 loss: 1.1015682101249695
  batch 600 loss: 1.1049124228954315
  batch 650 loss: 1.0986823189258574
  batch 700 loss: 1.095920114517212
  batch 750 loss: 1.0593553185462952
  batch 800 loss: 1.0934616827964783
  batch 850 loss: 1.0949568891525268
  batch 900 loss: 1.047010589838028
LOSS train 1.04701 valid 1.05641, valid PER 32.84%
EPOCH 6:
  batch 50 loss: 1.0129462885856628
  batch 100 loss: 1.0190062093734742
  batch 150 loss: 1.0278337264060975
  batch 200 loss: 0.983592449426651
  batch 250 loss: 1.0157400703430175
  batch 300 loss: 0.9948065447807312
  batch 350 loss: 1.0307811498641968
  batch 400 loss: 0.9962247228622436
  batch 450 loss: 0.9944740104675293
  batch 500 loss: 0.9629728126525879
  batch 550 loss: 0.9964020681381226
  batch 600 loss: 0.9752554082870484
  batch 650 loss: 0.9751373529434204
  batch 700 loss: 0.9835688686370849
  batch 750 loss: 0.9861060130596161
  batch 800 loss: 0.9786602926254272
  batch 850 loss: 0.9995213294029236
  batch 900 loss: 0.9735224688053131
LOSS train 0.97352 valid 1.01731, valid PER 31.55%
EPOCH 7:
  batch 50 loss: 0.8680650746822357
  batch 100 loss: 0.9349588966369629
  batch 150 loss: 0.8872989130020141
  batch 200 loss: 0.9160816860198975
  batch 250 loss: 0.9570465183258057
  batch 300 loss: 0.9087330818176269
  batch 350 loss: 0.9410746645927429
  batch 400 loss: 0.8973181402683258
  batch 450 loss: 0.9257888722419739
  batch 500 loss: 0.9075024890899658
  batch 550 loss: 0.9222769749164581
  batch 600 loss: 0.9225030553340912
  batch 650 loss: 0.8739687645435333
  batch 700 loss: 0.9219445765018464
  batch 750 loss: 0.8798944222927093
  batch 800 loss: 0.8646877455711365
  batch 850 loss: 0.8657538497447967
  batch 900 loss: 0.8952207434177398
LOSS train 0.89522 valid 0.95364, valid PER 29.30%
EPOCH 8:
  batch 50 loss: 0.8412353003025055
  batch 100 loss: 0.8252578151226043
  batch 150 loss: 0.8396287894248963
  batch 200 loss: 0.8818662071228027
  batch 250 loss: 0.8597016847133636
  batch 300 loss: 0.8034255146980286
  batch 350 loss: 0.8180404806137085
  batch 400 loss: 0.8219414317607879
  batch 450 loss: 0.8680182993412018
  batch 500 loss: 0.8392717671394349
  batch 550 loss: 0.8356182789802551
  batch 600 loss: 0.8158526945114136
  batch 650 loss: 0.8327969217300415
  batch 700 loss: 0.8277669548988342
  batch 750 loss: 0.8307198464870453
  batch 800 loss: 0.8372111415863037
  batch 850 loss: 0.8122821247577667
  batch 900 loss: 0.8236434698104859
LOSS train 0.82364 valid 0.89586, valid PER 27.38%
EPOCH 9:
  batch 50 loss: 0.7377478170394898
  batch 100 loss: 0.7560840559005737
  batch 150 loss: 0.7747452992200852
  batch 200 loss: 0.7683765196800232
  batch 250 loss: 0.7316876566410064
  batch 300 loss: 0.776103401184082
  batch 350 loss: 0.7758009785413742
  batch 400 loss: 0.8344086468219757
  batch 450 loss: 0.8449950158596039
  batch 500 loss: 0.7610318803787232
  batch 550 loss: 0.7737152266502381
  batch 600 loss: 0.8073440086841583
  batch 650 loss: 0.7733973550796509
  batch 700 loss: 0.7652384114265441
  batch 750 loss: 0.7850170445442199
  batch 800 loss: 0.788903431892395
  batch 850 loss: 0.7887898516654969
  batch 900 loss: 0.7456171995401383
LOSS train 0.74562 valid 0.87274, valid PER 26.54%
EPOCH 10:
  batch 50 loss: 0.6796860992908478
  batch 100 loss: 0.7114005655050277
  batch 150 loss: 0.7115423208475113
  batch 200 loss: 0.7037283575534821
  batch 250 loss: 0.7208013445138931
  batch 300 loss: 0.7373886132240295
  batch 350 loss: 0.7159252262115479
  batch 400 loss: 0.700757406949997
  batch 450 loss: 0.688536805510521
  batch 500 loss: 0.7103997993469239
  batch 550 loss: 0.7212986236810685
  batch 600 loss: 0.7069245374202728
  batch 650 loss: 0.739819289445877
  batch 700 loss: 0.7255159747600556
  batch 750 loss: 0.7310760319232941
  batch 800 loss: 0.7390634524822235
  batch 850 loss: 0.6990343242883682
  batch 900 loss: 0.7114744406938552
LOSS train 0.71147 valid 0.85296, valid PER 25.90%
EPOCH 11:
  batch 50 loss: 0.6559575867652893
  batch 100 loss: 0.6351078808307647
  batch 150 loss: 0.6693899846076965
  batch 200 loss: 0.6196748262643814
  batch 250 loss: 0.6595974779129028
  batch 300 loss: 0.6473123145103454
  batch 350 loss: 0.6618470376729966
  batch 400 loss: 0.6589109402894974
  batch 450 loss: 0.6592539322376251
  batch 500 loss: 0.6609154319763184
  batch 550 loss: 0.6838474518060684
  batch 600 loss: 0.6714011907577515
  batch 650 loss: 0.6899212962388992
  batch 700 loss: 0.7237109446525574
  batch 750 loss: 0.6538764721155167
  batch 800 loss: 0.7089597511291504
  batch 850 loss: 0.6929996627569198
  batch 900 loss: 0.6933009231090546
LOSS train 0.69330 valid 0.81853, valid PER 25.23%
EPOCH 12:
  batch 50 loss: 0.5994201481342316
  batch 100 loss: 0.5780134010314941
  batch 150 loss: 0.5989979100227356
  batch 200 loss: 0.6040475833415985
  batch 250 loss: 0.6073420083522797
  batch 300 loss: 0.6235399580001831
  batch 350 loss: 0.5826171857118606
  batch 400 loss: 0.6263493740558624
  batch 450 loss: 0.6058172351121902
  batch 500 loss: 0.6256007891893387
  batch 550 loss: 0.6320504450798035
  batch 600 loss: 0.6402677178382874
  batch 650 loss: 0.6549729806184769
  batch 700 loss: 0.6505508208274842
  batch 750 loss: 0.6641590887308121
  batch 800 loss: 0.6293061822652817
  batch 850 loss: 0.6184789705276489
  batch 900 loss: 0.630469286441803
LOSS train 0.63047 valid 0.86196, valid PER 25.33%
EPOCH 13:
  batch 50 loss: 0.552935186624527
  batch 100 loss: 0.5607724869251252
  batch 150 loss: 0.5481494498252869
  batch 200 loss: 0.5374485170841217
  batch 250 loss: 0.5413825380802154
  batch 300 loss: 0.5896448040008545
  batch 350 loss: 0.5599753308296204
  batch 400 loss: 0.5676983481645584
  batch 450 loss: 0.5757065951824188
  batch 500 loss: 0.5747918969392777
  batch 550 loss: 0.6339234328269958
  batch 600 loss: 0.5936228621006012
  batch 650 loss: 0.5991082113981246
  batch 700 loss: 0.5961819142103195
  batch 750 loss: 0.560744851231575
  batch 800 loss: 0.5828090929985046
  batch 850 loss: 0.5742823594808578
  batch 900 loss: 0.6117165148258209
LOSS train 0.61172 valid 0.85690, valid PER 24.92%
EPOCH 14:
  batch 50 loss: 0.5281499224901199
  batch 100 loss: 0.5164279937744141
  batch 150 loss: 0.516470056772232
  batch 200 loss: 0.5257073098421097
  batch 250 loss: 0.5307475751638413
  batch 300 loss: 0.5492288893461228
  batch 350 loss: 0.5486126399040222
  batch 400 loss: 0.5386305546760559
  batch 450 loss: 0.525903799533844
  batch 500 loss: 0.550265628695488
  batch 550 loss: 0.5551331210136413
  batch 600 loss: 0.5296830719709397
  batch 650 loss: 0.542494997382164
  batch 700 loss: 0.6344304490089416
  batch 750 loss: 0.5828830242156983
  batch 800 loss: 0.5580342996120453
  batch 850 loss: 0.5945324552059174
  batch 900 loss: 0.5753188097476959
LOSS train 0.57532 valid 0.84262, valid PER 24.58%
EPOCH 15:
  batch 50 loss: 0.45814624965190887
  batch 100 loss: 0.48835727393627165
  batch 150 loss: 0.5007519233226776
  batch 200 loss: 0.49873822391033173
  batch 250 loss: 0.49297150552272795
  batch 300 loss: 0.5162320184707642
  batch 350 loss: 0.5072710055112839
  batch 400 loss: 0.5031747567653656
  batch 450 loss: 0.5086571103334427
  batch 500 loss: 0.48254652857780456
  batch 550 loss: 0.5584680765867234
  batch 600 loss: 0.5508932626247406
  batch 650 loss: 0.50703950881958
  batch 700 loss: 0.49913509845733645
  batch 750 loss: 0.5287119716405868
  batch 800 loss: 0.5221138042211533
  batch 850 loss: 0.5293712931871414
  batch 900 loss: 0.503919124007225
LOSS train 0.50392 valid 0.84077, valid PER 24.08%
[2.79811044216156, 1.6753829503059388, 1.3705501985549926, 1.1901905643939972, 1.047010589838028, 0.9735224688053131, 0.8952207434177398, 0.8236434698104859, 0.7456171995401383, 0.7114744406938552, 0.6933009231090546, 0.630469286441803, 0.6117165148258209, 0.5753188097476959, 0.503919124007225]
[2.727313756942749, 1.6203151941299438, 1.3936576843261719, 1.1955102682113647, 1.0564097166061401, 1.017305850982666, 0.9536409378051758, 0.895862877368927, 0.8727362155914307, 0.8529579639434814, 0.8185299634933472, 0.8619613647460938, 0.856898307800293, 0.842618465423584, 0.8407732248306274]
Training finished in 18.0 minutes.
Model saved to checkpoints/20230125_184942/model_11
Loading model from checkpoints/20230125_184942/model_11
SUB: 16.88%, DEL: 8.40%, INS: 2.46%, COR: 74.73%, PER: 27.74%
