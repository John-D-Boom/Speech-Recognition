Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=4, fbank_dims=23, model_dims=256, concat=1, lr=0.002, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.0, beta1=0.9, beta2=0.999)
Total number of model parameters is 5326888
EPOCH 1:
  batch 50 loss: 4.4487562036514285
  batch 100 loss: 3.3163365268707277
  batch 150 loss: 3.2855678510665896
  batch 200 loss: 3.276558446884155
  batch 250 loss: 3.191645498275757
  batch 300 loss: 3.082170271873474
  batch 350 loss: 3.0972696590423583
  batch 400 loss: 3.010256390571594
  batch 450 loss: 2.932057571411133
  batch 500 loss: 2.8338259792327882
  batch 550 loss: 2.7498033618927002
  batch 600 loss: 2.7088757038116453
  batch 650 loss: 2.603594422340393
  batch 700 loss: 2.563930158615112
  batch 750 loss: 2.466798801422119
  batch 800 loss: 2.3967784690856933
  batch 850 loss: 2.322372598648071
  batch 900 loss: 2.3644773507118226
LOSS train 2.36448 valid 2.21112, valid PER 71.06%
EPOCH 2:
  batch 50 loss: 2.15222779750824
  batch 100 loss: 2.08205947637558
  batch 150 loss: 2.081469657421112
  batch 200 loss: 1.9934867644309997
  batch 250 loss: 1.894350836277008
  batch 300 loss: 1.8335037398338319
  batch 350 loss: 1.7513959860801698
  batch 400 loss: 1.7851664113998413
  batch 450 loss: 1.6808540105819703
  batch 500 loss: 1.6642661023139953
  batch 550 loss: 1.66307532787323
  batch 600 loss: 1.576233503818512
  batch 650 loss: 1.5598172545433044
  batch 700 loss: 1.5191927576065063
  batch 750 loss: 1.5243129920959473
  batch 800 loss: 1.4615916562080384
  batch 850 loss: 1.4566773104667663
  batch 900 loss: 2.524921178817749
LOSS train 2.52492 valid 1.44619, valid PER 45.85%
EPOCH 3:
  batch 50 loss: 1.4607759928703308
  batch 100 loss: 1.3736044406890868
  batch 150 loss: 1.3624745869636536
  batch 200 loss: 1.359175546169281
  batch 250 loss: 1.2903872907161713
  batch 300 loss: 1.2920582723617553
  batch 350 loss: 1.3474990797042847
  batch 400 loss: 1.280513378381729
  batch 450 loss: 1.2505390977859496
  batch 500 loss: 1.2505942940711976
  batch 550 loss: 1.258886570930481
  batch 600 loss: 1.2339264476299285
  batch 650 loss: 1.1795654344558715
  batch 700 loss: 1.207951387166977
  batch 750 loss: 1.260198440551758
  batch 800 loss: 1.1839650225639344
  batch 850 loss: 1.2231768369674683
  batch 900 loss: 1.1690467381477356
LOSS train 1.16905 valid 1.20265, valid PER 37.45%
EPOCH 4:
  batch 50 loss: 1.1382144486904144
  batch 100 loss: 1.1779015946388245
  batch 150 loss: 1.1065622389316558
  batch 200 loss: 1.1355580711364746
  batch 250 loss: 1.1176603138446808
  batch 300 loss: 1.1520085334777832
  batch 350 loss: 1.0794248378276825
  batch 400 loss: 1.0978904485702514
  batch 450 loss: 1.0887584114074706
  batch 500 loss: 1.0495565664768218
  batch 550 loss: 1.0812619233131409
  batch 600 loss: 1.119064062833786
  batch 650 loss: 1.0923778164386748
  batch 700 loss: 1.0609850704669952
  batch 750 loss: 1.062120987176895
  batch 800 loss: 1.0149035716056825
  batch 850 loss: 1.047437288761139
  batch 900 loss: 1.0994394934177398
LOSS train 1.09944 valid 1.05759, valid PER 33.01%
EPOCH 5:
  batch 50 loss: 1.0001649236679078
  batch 100 loss: 0.9835146141052246
  batch 150 loss: 1.054937778711319
  batch 200 loss: 0.9620336198806763
  batch 250 loss: 0.9879258334636688
  batch 300 loss: 0.9994317781925202
  batch 350 loss: 1.0145969712734222
  batch 400 loss: 1.0256891632080078
  batch 450 loss: 0.9724655389785767
  batch 500 loss: 0.9847270858287811
  batch 550 loss: 0.9225947475433349
  batch 600 loss: 1.0228864765167236
  batch 650 loss: 0.978833634853363
  batch 700 loss: 1.0233677065372466
  batch 750 loss: 0.9148065209388733
  batch 800 loss: 0.9508506166934967
  batch 850 loss: 0.9565592777729034
  batch 900 loss: 0.969875248670578
LOSS train 0.96988 valid 0.98230, valid PER 30.32%
EPOCH 6:
  batch 50 loss: 0.9574887716770172
  batch 100 loss: 0.8914597940444946
  batch 150 loss: 0.8965924847126007
  batch 200 loss: 0.9275449287891387
  batch 250 loss: 0.9333947241306305
  batch 300 loss: 0.9705133390426636
  batch 350 loss: 0.9522713267803192
  batch 400 loss: 0.9256914710998535
  batch 450 loss: 0.8975696754455567
  batch 500 loss: 0.9053773498535156
  batch 550 loss: 0.8944893944263458
  batch 600 loss: 0.8926549255847931
  batch 650 loss: 0.8813998806476593
  batch 700 loss: 0.8768180906772614
  batch 750 loss: 0.859589250087738
  batch 800 loss: 0.8881322979927063
  batch 850 loss: 0.8606635344028473
  batch 900 loss: 0.8880717468261718
LOSS train 0.88807 valid 0.90093, valid PER 28.69%
EPOCH 7:
  batch 50 loss: 0.8373232412338257
  batch 100 loss: 0.8499836593866348
  batch 150 loss: 0.8401032412052154
  batch 200 loss: 0.8540023571252823
  batch 250 loss: 0.8299046492576599
  batch 300 loss: 0.8144682824611664
  batch 350 loss: 0.8448183536529541
  batch 400 loss: 0.8828664886951446
  batch 450 loss: 0.8640828227996826
  batch 500 loss: 0.8619975066184997
  batch 550 loss: 0.8471893954277039
  batch 600 loss: 0.840332418680191
  batch 650 loss: 0.8315361440181732
  batch 700 loss: 0.8593483209609986
  batch 750 loss: 0.802787435054779
  batch 800 loss: 0.82957559466362
  batch 850 loss: 0.8297450530529022
  batch 900 loss: 0.8582349908351898
LOSS train 0.85823 valid 0.89781, valid PER 28.13%
EPOCH 8:
  batch 50 loss: 0.780486661195755
  batch 100 loss: 0.7912530970573425
  batch 150 loss: 0.7865538358688354
  batch 200 loss: 0.7636298167705536
  batch 250 loss: 0.7869830000400543
  batch 300 loss: 0.7278441536426544
  batch 350 loss: 0.8163602662086487
  batch 400 loss: 0.7707184511423111
  batch 450 loss: 0.7769125175476074
  batch 500 loss: 0.8212000489234924
  batch 550 loss: 0.7419473719596863
  batch 600 loss: 0.7969302570819855
  batch 650 loss: 0.8009992825984955
  batch 700 loss: 0.7906700325012207
  batch 750 loss: 0.7849610006809234
  batch 800 loss: 0.8098260915279388
  batch 850 loss: 0.7676638281345367
  batch 900 loss: 0.7854624629020691
LOSS train 0.78546 valid 0.83798, valid PER 26.29%
EPOCH 9:
  batch 50 loss: 0.7094979739189148
  batch 100 loss: 0.7569402134418488
  batch 150 loss: 0.7420794534683227
  batch 200 loss: 0.7161398559808732
  batch 250 loss: 0.7493924415111541
  batch 300 loss: 0.7687049281597137
  batch 350 loss: 0.8082329022884369
  batch 400 loss: 0.7531541436910629
  batch 450 loss: 0.7567637515068054
  batch 500 loss: 0.7172582399845123
  batch 550 loss: 0.7488550043106079
  batch 600 loss: 0.7979997897148132
  batch 650 loss: 0.7515355145931244
  batch 700 loss: 0.7521996635198593
  batch 750 loss: 0.7419746226072311
  batch 800 loss: 0.7599920082092285
  batch 850 loss: 0.7748253583908081
  batch 900 loss: 0.7444469058513641
LOSS train 0.74445 valid 0.87364, valid PER 27.48%
EPOCH 10:
  batch 50 loss: 0.6887550002336502
  batch 100 loss: 0.6876696038246155
  batch 150 loss: 0.699219873547554
  batch 200 loss: 0.7173676759004592
  batch 250 loss: 0.7184277880191803
  batch 300 loss: 0.6796363145112991
  batch 350 loss: 0.6927453553676606
  batch 400 loss: 0.6757446825504303
  batch 450 loss: 0.6690142148733139
  batch 500 loss: 0.7124368441104889
  batch 550 loss: 0.7674831402301788
  batch 600 loss: 0.6949449813365937
  batch 650 loss: 0.7124338096380234
  batch 700 loss: 0.7233087480068207
  batch 750 loss: 0.685329372882843
  batch 800 loss: 0.7273803347349167
  batch 850 loss: 0.7180146962404251
  batch 900 loss: 0.7298058676719665
LOSS train 0.72981 valid 0.81135, valid PER 25.68%
EPOCH 11:
  batch 50 loss: 0.6359503620862961
  batch 100 loss: 0.6253032940626144
  batch 150 loss: 0.6309452551603317
  batch 200 loss: 0.6912106639146804
  batch 250 loss: 0.6838952100276947
  batch 300 loss: 0.6435629040002823
  batch 350 loss: 0.6651591718196869
  batch 400 loss: 0.6591415309906006
  batch 450 loss: 0.7004283410310745
  batch 500 loss: 0.6528839868307114
  batch 550 loss: 0.6608348977565766
  batch 600 loss: 0.6820471388101578
  batch 650 loss: 0.7498728269338608
  batch 700 loss: 0.6529979145526886
  batch 750 loss: 0.6546969252824784
  batch 800 loss: 0.6757360851764679
  batch 850 loss: 0.6903457576036454
  batch 900 loss: 0.7348452687263489
LOSS train 0.73485 valid 0.81263, valid PER 25.04%
EPOCH 12:
  batch 50 loss: 0.6369587755203248
  batch 100 loss: 0.6086714869737625
  batch 150 loss: 0.5897053474187851
  batch 200 loss: 0.6134503084421158
  batch 250 loss: 0.6244315534830094
  batch 300 loss: 0.6301637297868729
  batch 350 loss: 0.6154456806182861
  batch 400 loss: 0.6437843596935272
  batch 450 loss: 0.6379973393678665
  batch 500 loss: 0.6392591309547424
  batch 550 loss: 0.5938274419307709
  batch 600 loss: 0.6286071228981018
  batch 650 loss: 0.6620722800493241
  batch 700 loss: 0.6673631083965301
  batch 750 loss: 0.6390525871515274
  batch 800 loss: 0.6287639838457107
  batch 850 loss: 0.6605687427520752
  batch 900 loss: 0.6478339618444443
LOSS train 0.64783 valid 0.79108, valid PER 24.80%
EPOCH 13:
  batch 50 loss: 0.6000345140695572
  batch 100 loss: 0.6011206537485123
  batch 150 loss: 0.5832142460346222
  batch 200 loss: 0.5748093611001969
  batch 250 loss: 0.5856620198488236
  batch 300 loss: 0.5567647850513459
  batch 350 loss: 0.5760686516761779
  batch 400 loss: 0.5720368456840516
  batch 450 loss: 0.583771755695343
  batch 500 loss: 0.5793067526817322
  batch 550 loss: 0.6368374782800674
  batch 600 loss: 0.5952501803636551
  batch 650 loss: 0.625774952173233
  batch 700 loss: 0.6124208754301071
  batch 750 loss: 0.5873419153690338
  batch 800 loss: 0.6031897163391113
  batch 850 loss: 0.6495380872488021
  batch 900 loss: 0.630959324836731
LOSS train 0.63096 valid 0.78970, valid PER 24.53%
EPOCH 14:
  batch 50 loss: 0.5636374175548553
  batch 100 loss: 0.555958748459816
  batch 150 loss: 0.5569114434719086
  batch 200 loss: 0.5330040830373765
  batch 250 loss: 0.5337551414966584
  batch 300 loss: 0.5664368540048599
  batch 350 loss: 0.5369810873270034
  batch 400 loss: 0.5466361027956009
  batch 450 loss: 0.5921162110567093
  batch 500 loss: 0.5831703847646713
  batch 550 loss: 0.5864169347286224
  batch 600 loss: 0.5558644261956215
  batch 650 loss: 0.5844252562522888
  batch 700 loss: 0.5875689262151718
  batch 750 loss: 0.5734127861261368
  batch 800 loss: 0.5574819225072861
  batch 850 loss: 0.5738156843185425
  batch 900 loss: 0.6007470345497131
LOSS train 0.60075 valid 0.77238, valid PER 24.12%
EPOCH 15:
  batch 50 loss: 0.5072972679138184
  batch 100 loss: 0.5164379107952118
  batch 150 loss: 0.5154907840490341
  batch 200 loss: 0.5356479752063751
  batch 250 loss: 0.5133367699384689
  batch 300 loss: 0.49339651703834536
  batch 350 loss: 0.5065864086151123
  batch 400 loss: 0.5426638853549958
  batch 450 loss: 0.5460453128814697
  batch 500 loss: 0.514779070019722
  batch 550 loss: 0.5312893012166023
  batch 600 loss: 0.5414145439863205
  batch 650 loss: 0.5534383547306061
  batch 700 loss: 0.5870261514186859
  batch 750 loss: 0.5705548024177551
  batch 800 loss: 0.5243629461526871
  batch 850 loss: 0.5420999789237976
  batch 900 loss: 0.5690611404180527
LOSS train 0.56906 valid 0.77253, valid PER 23.45%
EPOCH 16:
  batch 50 loss: 0.5009874612092972
  batch 100 loss: 0.5010936713218689
  batch 150 loss: 0.511735075712204
  batch 200 loss: 0.5084129929542541
  batch 250 loss: 0.5487059789896012
  batch 300 loss: 0.5440205752849578
  batch 350 loss: 0.5667753636837005
  batch 400 loss: 0.5236811566352845
  batch 450 loss: 0.5297313517332077
  batch 500 loss: 0.5122552394866944
  batch 550 loss: 0.5026207184791565
  batch 600 loss: 0.5103284919261932
  batch 650 loss: 0.5383962225914002
  batch 700 loss: 0.5171162289381027
  batch 750 loss: 0.52668761074543
  batch 800 loss: 0.5302711606025696
  batch 850 loss: 0.5242728382349015
  batch 900 loss: 0.5046917098760605
LOSS train 0.50469 valid 0.75391, valid PER 22.94%
EPOCH 17:
  batch 50 loss: 0.45659447252750396
  batch 100 loss: 0.4696774673461914
  batch 150 loss: 0.4667604586482048
  batch 200 loss: 0.48014088332653043
  batch 250 loss: 0.47641256392002107
  batch 300 loss: 0.4897955232858658
  batch 350 loss: 0.46840949177742003
  batch 400 loss: 0.5038188648223877
  batch 450 loss: 0.5058480662107467
  batch 500 loss: 0.4888442575931549
  batch 550 loss: 0.48676144242286684
  batch 600 loss: 0.5392415750026703
  batch 650 loss: 0.515837522149086
  batch 700 loss: 0.5332324123382568
  batch 750 loss: 0.48878861010074615
  batch 800 loss: 0.482567326426506
  batch 850 loss: 0.5134540635347367
  batch 900 loss: 0.47058839559555055
LOSS train 0.47059 valid 0.75140, valid PER 22.56%
EPOCH 18:
  batch 50 loss: 0.42188184827566144
  batch 100 loss: 0.4381011778116226
  batch 150 loss: 0.44370505154132844
  batch 200 loss: 0.4461421579122543
  batch 250 loss: 0.4451522082090378
  batch 300 loss: 0.4516554710268974
  batch 350 loss: 0.45189067631959917
  batch 400 loss: 0.46099795162677765
  batch 450 loss: 0.4760854375362396
  batch 500 loss: 0.4422095608711243
  batch 550 loss: 0.46223856091499327
  batch 600 loss: 0.46688381016254427
  batch 650 loss: 0.4675397953391075
  batch 700 loss: 0.4857145690917969
  batch 750 loss: 0.4632550537586212
  batch 800 loss: 0.46917392551898957
  batch 850 loss: 0.4563481605052948
  batch 900 loss: 0.49904791414737704
LOSS train 0.49905 valid 0.76066, valid PER 22.86%
EPOCH 19:
  batch 50 loss: 0.38880142718553545
  batch 100 loss: 0.3745951408147812
  batch 150 loss: 0.4017651075124741
  batch 200 loss: 0.4292383974790573
  batch 250 loss: 0.4346063983440399
  batch 300 loss: 0.4502148565649986
  batch 350 loss: 0.45378782987594607
  batch 400 loss: 0.44616139531135557
  batch 450 loss: 0.47164767146110537
  batch 500 loss: 0.4514343276619911
  batch 550 loss: 0.4464930832386017
  batch 600 loss: 0.43227173268795016
  batch 650 loss: 0.45728176176548
  batch 700 loss: 0.4311663728952408
  batch 750 loss: 0.412823965549469
  batch 800 loss: 0.44766630947589875
  batch 850 loss: 0.4432433193922043
  batch 900 loss: 0.45283058822155
LOSS train 0.45283 valid 0.75434, valid PER 22.36%
EPOCH 20:
  batch 50 loss: 0.37414410054683683
  batch 100 loss: 0.3660650473833084
  batch 150 loss: 0.3668065869808197
  batch 200 loss: 0.409210844039917
  batch 250 loss: 0.3751865756511688
  batch 300 loss: 0.4082591524720192
  batch 350 loss: 0.3849736595153809
  batch 400 loss: 0.42706006348133085
  batch 450 loss: 0.4084335350990295
  batch 500 loss: 0.39270850509405136
  batch 550 loss: 0.4432518437504768
  batch 600 loss: 0.4129872959852219
  batch 650 loss: 0.44318524420261385
  batch 700 loss: 0.43466873228549957
  batch 750 loss: 0.40214850783348083
  batch 800 loss: 0.43193344354629515
  batch 850 loss: 0.44149310171604156
  batch 900 loss: 0.44867286682128904
LOSS train 0.44867 valid 0.74234, valid PER 22.18%
[2.3644773507118226, 2.524921178817749, 1.1690467381477356, 1.0994394934177398, 0.969875248670578, 0.8880717468261718, 0.8582349908351898, 0.7854624629020691, 0.7444469058513641, 0.7298058676719665, 0.7348452687263489, 0.6478339618444443, 0.630959324836731, 0.6007470345497131, 0.5690611404180527, 0.5046917098760605, 0.47058839559555055, 0.49904791414737704, 0.45283058822155, 0.44867286682128904]
[2.2111153602600098, 1.4461922645568848, 1.2026458978652954, 1.0575926303863525, 0.9822955131530762, 0.9009268283843994, 0.8978126049041748, 0.8379835486412048, 0.8736411333084106, 0.8113495707511902, 0.8126307725906372, 0.7910813093185425, 0.7896980047225952, 0.7723788022994995, 0.7725257873535156, 0.7539101839065552, 0.7513965368270874, 0.7606648802757263, 0.7543436288833618, 0.7423428893089294]
Training finished in 29.0 minutes.
Model saved to checkpoints/20230125_193852/model_20
Loading model from checkpoints/20230125_193852/model_20
SUB: 14.38%, DEL: 6.62%, INS: 2.42%, COR: 79.01%, PER: 23.41%
