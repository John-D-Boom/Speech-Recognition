Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.9)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.220875897407532
  batch 100 loss: 3.2729079294204713
  batch 150 loss: 3.167327518463135
  batch 200 loss: 3.0381510400772096
  batch 250 loss: 2.9082944631576537
  batch 300 loss: 2.7703481101989746
  batch 350 loss: 2.689840521812439
  batch 400 loss: 2.6404934120178223
  batch 450 loss: 2.631601052284241
  batch 500 loss: 2.5388818359375
  batch 550 loss: 2.5388128185272216
  batch 600 loss: 2.481904525756836
  batch 650 loss: 2.442445087432861
  batch 700 loss: 2.439595928192139
  batch 750 loss: 2.4335603141784667
  batch 800 loss: 2.415804581642151
  batch 850 loss: 2.4073741245269775
  batch 900 loss: 2.368264470100403
max grad: tensor(0.9516)
LOSS train 2.36826 valid 2.65236, valid PER 87.26%
EPOCH 2:
  batch 50 loss: 2.387895317077637
  batch 100 loss: 2.3595848274230957
  batch 150 loss: 2.291675024032593
  batch 200 loss: 2.310174722671509
  batch 250 loss: 2.3207374906539915
  batch 300 loss: 2.295880136489868
  batch 350 loss: 2.283911395072937
  batch 400 loss: 2.2499586391448974
  batch 450 loss: 2.2605488157272338
  batch 500 loss: 2.2651238656044006
  batch 550 loss: 2.237169954776764
  batch 600 loss: 2.247474751472473
  batch 650 loss: 2.2129725074768065
  batch 700 loss: 2.2213017416000365
  batch 750 loss: 2.2142452573776246
  batch 800 loss: 2.165197196006775
  batch 850 loss: 2.213446433544159
  batch 900 loss: 2.1538085675239564
max grad: tensor(0.3285)
LOSS train 2.15381 valid 2.18783, valid PER 84.77%
EPOCH 3:
  batch 50 loss: 2.121896331310272
  batch 100 loss: 2.1761885023117067
  batch 150 loss: 2.186820330619812
  batch 200 loss: 2.1365944027900694
  batch 250 loss: 2.13807893037796
  batch 300 loss: 2.1759308910369874
  batch 350 loss: 2.150248532295227
  batch 400 loss: 2.1381045031547545
  batch 450 loss: 2.1192863512039186
  batch 500 loss: 2.1087793064117433
  batch 550 loss: 2.1123150849342345
  batch 600 loss: 2.0798934531211852
  batch 650 loss: 2.109248080253601
  batch 700 loss: 2.0712211656570436
  batch 750 loss: 2.114124176502228
  batch 800 loss: 2.0946960401535035
  batch 850 loss: 2.0978766560554503
  batch 900 loss: 2.0889610838890076
max grad: tensor(0.3225)
LOSS train 2.08896 valid 2.06717, valid PER 82.49%
EPOCH 4:
  batch 50 loss: 2.089826397895813
  batch 100 loss: 2.035369131565094
  batch 150 loss: 2.0560118436813353
  batch 200 loss: 2.0790338587760924
  batch 250 loss: 2.0749444365501404
  batch 300 loss: 2.0482334113121032
  batch 350 loss: 2.05094078540802
  batch 400 loss: 2.021286709308624
  batch 450 loss: 2.0422280049324035
  batch 500 loss: 2.0719429540634153
  batch 550 loss: 2.0259961676597595
  batch 600 loss: 2.0092611479759217
  batch 650 loss: 2.068991072177887
  batch 700 loss: 2.0764853954315186
  batch 750 loss: 1.9986265397071838
  batch 800 loss: 2.011392688751221
  batch 850 loss: 2.0020405435562134
  batch 900 loss: 2.022675938606262
max grad: tensor(0.2897)
LOSS train 2.02268 valid 2.10160, valid PER 80.90%
EPOCH 5:
  batch 50 loss: 2.0288737058639525
  batch 100 loss: 2.0068769788742067
  batch 150 loss: 2.0271060919761656
  batch 200 loss: 2.0403457140922545
  batch 250 loss: 1.9961260747909546
  batch 300 loss: 1.9920570969581604
  batch 350 loss: 1.9741266417503356
  batch 400 loss: 1.9751581621170045
  batch 450 loss: 1.9748309183120727
  batch 500 loss: 1.9502876615524292
  batch 550 loss: 1.9934247088432313
  batch 600 loss: 2.0236222553253174
  batch 650 loss: 1.9817825603485106
  batch 700 loss: 1.9599846768379212
  batch 750 loss: 1.9745987462997436
  batch 800 loss: 2.002621171474457
  batch 850 loss: 1.973793807029724
  batch 900 loss: 1.9581832718849181
max grad: tensor(0.2649)
LOSS train 1.95818 valid 2.00156, valid PER 78.70%
EPOCH 6:
  batch 50 loss: 1.9431063437461853
  batch 100 loss: 1.9756013822555543
  batch 150 loss: 1.936415331363678
  batch 200 loss: 1.917513518333435
  batch 250 loss: 1.956505777835846
  batch 300 loss: 1.9703118515014648
  batch 350 loss: 1.9609728622436524
  batch 400 loss: 1.9625900292396545
  batch 450 loss: 1.9631473541259765
  batch 500 loss: 1.9514590644836425
  batch 550 loss: 1.9538565373420715
  batch 600 loss: 1.9214372253417968
  batch 650 loss: 1.9185705423355102
  batch 700 loss: 1.9013951706886292
  batch 750 loss: 1.9492713403701782
  batch 800 loss: 1.9433934354782105
  batch 850 loss: 1.9497640252113342
  batch 900 loss: 1.9397200965881347
max grad: tensor(0.2826)
LOSS train 1.93972 valid 1.88672, valid PER 78.30%
EPOCH 7:
  batch 50 loss: 1.904274010658264
  batch 100 loss: 1.9678053665161133
  batch 150 loss: 1.9140999817848205
  batch 200 loss: 1.8884913849830627
  batch 250 loss: 1.927186243534088
  batch 300 loss: 1.925562629699707
  batch 350 loss: 1.92191232919693
  batch 400 loss: 1.8824315547943116
  batch 450 loss: 1.9163224244117736
  batch 500 loss: 1.9250434231758118
  batch 550 loss: 1.8665459322929383
  batch 600 loss: 1.8928437328338623
  batch 650 loss: 1.872320957183838
  batch 700 loss: 1.92862069606781
  batch 750 loss: 1.8953932309150696
  batch 800 loss: 1.9015588116645814
  batch 850 loss: 1.891454746723175
  batch 900 loss: 1.8988632011413573
max grad: tensor(0.2298)
LOSS train 1.89886 valid 1.99312, valid PER 78.28%
EPOCH 8:
  batch 50 loss: 1.904632339477539
  batch 100 loss: 1.8862645530700684
  batch 150 loss: 1.9050024366378784
  batch 200 loss: 1.8744995713233947
  batch 250 loss: 1.856212522983551
  batch 300 loss: 1.8517725944519043
  batch 350 loss: 1.87744464635849
  batch 400 loss: 1.858931267261505
  batch 450 loss: 1.9060097694396974
  batch 500 loss: 1.9027001976966857
  batch 550 loss: 1.8590687680244447
  batch 600 loss: 1.8373425531387328
  batch 650 loss: 1.8438606548309326
  batch 700 loss: 1.8534410405158996
  batch 750 loss: 1.8803019690513612
  batch 800 loss: 1.863776662349701
  batch 850 loss: 1.833944146633148
  batch 900 loss: 1.8636189079284669
max grad: tensor(0.3351)
LOSS train 1.86362 valid 1.86546, valid PER 76.23%
EPOCH 9:
  batch 50 loss: 1.8790236401557923
  batch 100 loss: 1.8388753890991212
  batch 150 loss: 1.8590505838394165
  batch 200 loss: 1.820876762866974
  batch 250 loss: 1.8276584005355836
  batch 300 loss: 1.8800827002525329
  batch 350 loss: 1.8367387008666993
  batch 400 loss: 1.8625272607803345
  batch 450 loss: 1.854454176425934
  batch 500 loss: 1.8094970870018006
  batch 550 loss: 1.8645649361610412
  batch 600 loss: 1.8622098779678344
  batch 650 loss: 1.8340961813926697
  batch 700 loss: 1.8178878831863403
  batch 750 loss: 1.8388337349891664
  batch 800 loss: 1.8579067969322205
  batch 850 loss: 1.8767549681663513
  batch 900 loss: 1.8120782160758973
max grad: tensor(0.2650)
LOSS train 1.81208 valid 1.72976, valid PER 71.26%
EPOCH 10:
  batch 50 loss: 1.8113037157058716
  batch 100 loss: 1.8370349192619324
  batch 150 loss: 1.8608235669136048
  batch 200 loss: 1.8357609033584594
  batch 250 loss: 1.8181692552566528
  batch 300 loss: 1.8008552956581116
  batch 350 loss: 1.8203217959403992
  batch 400 loss: 1.782764413356781
  batch 450 loss: 1.7912741565704347
  batch 500 loss: 1.8360138630867004
  batch 550 loss: 1.8157715201377869
  batch 600 loss: 1.7994236040115357
  batch 650 loss: 1.8073592209815978
  batch 700 loss: 1.8340537095069884
  batch 750 loss: 1.844587128162384
  batch 800 loss: 1.8326102876663208
  batch 850 loss: 1.8137377166748048
  batch 900 loss: 1.8148918962478637
max grad: tensor(0.3741)
LOSS train 1.81489 valid 1.88418, valid PER 75.80%
EPOCH 11:
  batch 50 loss: 1.8202513885498046
  batch 100 loss: 1.8063364672660827
  batch 150 loss: 1.769067633152008
  batch 200 loss: 1.7930255675315856
  batch 250 loss: 1.7555790257453918
  batch 300 loss: 1.799317717552185
  batch 350 loss: 1.816201193332672
  batch 400 loss: 1.7762229323387146
  batch 450 loss: 1.7939200139045715
  batch 500 loss: 1.7931423568725586
  batch 550 loss: 1.8149825429916382
  batch 600 loss: 1.8103473806381225
  batch 650 loss: 1.797057044506073
  batch 700 loss: 1.8519082808494567
  batch 750 loss: 1.8019249534606934
  batch 800 loss: 1.7935973596572876
  batch 850 loss: 1.7903507447242737
  batch 900 loss: 1.814671335220337
max grad: tensor(0.2406)
LOSS train 1.81467 valid 1.75519, valid PER 72.65%
EPOCH 12:
  batch 50 loss: 1.778050353527069
  batch 100 loss: 1.7504748249053954
  batch 150 loss: 1.792110562324524
  batch 200 loss: 1.8061707401275635
  batch 250 loss: 1.789531033039093
  batch 300 loss: 1.8029474663734435
  batch 350 loss: 1.7766833209991455
  batch 400 loss: 1.8002337980270386
  batch 450 loss: 1.7634726572036743
  batch 500 loss: 1.784162802696228
  batch 550 loss: 1.7939345335960388
  batch 600 loss: 1.7973266220092774
  batch 650 loss: 1.7699006724357604
  batch 700 loss: 1.7556251072883606
  batch 750 loss: 1.798296763896942
  batch 800 loss: 1.7457317233085632
  batch 850 loss: 1.780686604976654
  batch 900 loss: 1.7665272665023803
max grad: tensor(0.2366)
LOSS train 1.76653 valid 1.73151, valid PER 68.74%
EPOCH 13:
  batch 50 loss: 1.7439815068244935
  batch 100 loss: 1.7754800009727478
  batch 150 loss: 1.7717467617988587
  batch 200 loss: 1.7216274952888488
  batch 250 loss: 1.7632112836837768
  batch 300 loss: 1.7866934514045716
  batch 350 loss: 1.7460677790641785
  batch 400 loss: 1.7815607595443725
  batch 450 loss: 1.792042224407196
  batch 500 loss: 1.750206196308136
  batch 550 loss: 1.80895512342453
  batch 600 loss: 1.7446384286880494
  batch 650 loss: 1.7595553016662597
  batch 700 loss: 1.780703411102295
  batch 750 loss: 1.7133557558059693
  batch 800 loss: 1.7639310193061828
  batch 850 loss: 1.7513510179519653
  batch 900 loss: 1.7321142673492431
max grad: tensor(0.2270)
LOSS train 1.73211 valid 1.67902, valid PER 67.55%
EPOCH 14:
  batch 50 loss: 1.7364145493507386
  batch 100 loss: 1.7314998865127564
  batch 150 loss: 1.7296222138404846
  batch 200 loss: 1.7401942205429077
  batch 250 loss: 1.7399950861930846
  batch 300 loss: 1.7170426082611083
  batch 350 loss: 1.7328889131546021
  batch 400 loss: 1.7578618955612182
  batch 450 loss: 1.732252571582794
  batch 500 loss: 1.7362465000152587
  batch 550 loss: 1.7374042391777038
  batch 600 loss: 1.7095742154121398
  batch 650 loss: 1.766371829509735
  batch 700 loss: 1.7771299195289612
  batch 750 loss: 1.7436209297180176
  batch 800 loss: 1.7386757802963257
  batch 850 loss: 1.8100538897514342
  batch 900 loss: 1.7381897163391113
max grad: tensor(0.2473)
LOSS train 1.73819 valid 1.65577, valid PER 66.80%
EPOCH 15:
  batch 50 loss: 1.707658944129944
  batch 100 loss: 1.7343904304504394
  batch 150 loss: 1.7249670648574829
  batch 200 loss: 1.749061930179596
  batch 250 loss: 1.7403740406036377
  batch 300 loss: 1.9663949418067932
  batch 350 loss: 1.90353848695755
  batch 400 loss: 1.8384206700325012
  batch 450 loss: 1.8191183042526244
  batch 500 loss: 1.8302379488945006
  batch 550 loss: 1.8209330677986144
  batch 600 loss: 1.8256820297241212
  batch 650 loss: 1.7571199560165405
  batch 700 loss: 1.777089035511017
  batch 750 loss: 1.815699200630188
  batch 800 loss: 1.7613949823379516
  batch 850 loss: 1.7542044949531554
  batch 900 loss: 1.711708972454071
max grad: tensor(3.3866)
LOSS train 1.71171 valid 1.70256, valid PER 68.40%
EPOCH 16:
  batch 50 loss: 1.729318242073059
  batch 100 loss: 1.7359544682502746
  batch 150 loss: 1.741636393070221
  batch 200 loss: 1.761626443862915
  batch 250 loss: 1.7542764306068421
  batch 300 loss: 1.7698929738998412
  batch 350 loss: 1.7553240871429443
  batch 400 loss: 1.7443528914451598
  batch 450 loss: 1.7456627297401428
  batch 500 loss: 1.7469121026992798
  batch 550 loss: 1.7112515330314637
  batch 600 loss: 1.7369159197807311
  batch 650 loss: 1.738861744403839
  batch 700 loss: 1.7101401925086974
  batch 750 loss: 1.7424376511573791
  batch 800 loss: 1.7258041310310364
  batch 850 loss: 1.7393492197990417
  batch 900 loss: 1.7626842975616455
max grad: tensor(0.2547)
LOSS train 1.76268 valid 1.72626, valid PER 69.20%
EPOCH 17:
  batch 50 loss: 1.725987558364868
  batch 100 loss: 1.6848555827140808
  batch 150 loss: 1.7538067150115966
  batch 200 loss: 1.7204992842674256
  batch 250 loss: 1.736519341468811
  batch 300 loss: 1.7145059537887573
  batch 350 loss: 1.7211909151077271
  batch 400 loss: 1.7468571472167969
  batch 450 loss: 1.7384702396392822
  batch 500 loss: 1.711647846698761
  batch 550 loss: 1.7266786766052247
  batch 600 loss: 1.740936906337738
  batch 650 loss: 1.7301558446884155
  batch 700 loss: 1.7046198773384094
  batch 750 loss: 1.6726005220413207
  batch 800 loss: 1.7292877101898194
  batch 850 loss: 1.7126855587959289
  batch 900 loss: 1.7125187921524048
max grad: tensor(0.2897)
LOSS train 1.71252 valid 1.63193, valid PER 65.86%
EPOCH 18:
  batch 50 loss: 1.7058043932914735
  batch 100 loss: 1.7117826700210572
  batch 150 loss: 1.7488547539711
  batch 200 loss: 1.7074458718299865
  batch 250 loss: 1.709256467819214
  batch 300 loss: 1.6956840229034424
  batch 350 loss: 1.67882351398468
  batch 400 loss: 1.6979638314247132
  batch 450 loss: 1.7256408357620239
  batch 500 loss: 1.7026048064231873
  batch 550 loss: 1.7418008208274842
  batch 600 loss: 1.7097344923019409
  batch 650 loss: 1.6501507210731505
  batch 700 loss: 1.656577341556549
  batch 750 loss: 1.7013405132293702
  batch 800 loss: 1.7337343883514404
  batch 850 loss: 1.7037879514694214
  batch 900 loss: 1.6988687300682068
max grad: tensor(0.1942)
LOSS train 1.69887 valid 1.58787, valid PER 64.91%
EPOCH 19:
  batch 50 loss: 1.7129176688194274
  batch 100 loss: 1.712223572731018
  batch 150 loss: 1.6717301392555237
  batch 200 loss: 1.6792356514930724
  batch 250 loss: 1.7025901246070863
  batch 300 loss: 1.6831300711631776
  batch 350 loss: 1.7050264167785645
  batch 400 loss: 1.702253770828247
  batch 450 loss: 1.6445423197746276
  batch 500 loss: 1.6701336741447448
  batch 550 loss: 1.6918326759338378
  batch 600 loss: 1.717362141609192
  batch 650 loss: 1.6976192927360534
  batch 700 loss: 1.7176605868339538
  batch 750 loss: 1.6659694743156432
  batch 800 loss: 1.6392010688781737
  batch 850 loss: 1.6886360096931456
  batch 900 loss: 1.6881103324890137
max grad: tensor(0.4533)
LOSS train 1.68811 valid 1.60282, valid PER 64.41%
EPOCH 20:
  batch 50 loss: 1.6509627389907837
  batch 100 loss: 1.6663881707191468
  batch 150 loss: 1.669959614276886
  batch 200 loss: 1.684000449180603
  batch 250 loss: 1.6736708903312683
  batch 300 loss: 1.6828963470458984
  batch 350 loss: 1.6440300154685974
  batch 400 loss: 1.6969629549980163
  batch 450 loss: 1.7004525423049928
  batch 500 loss: 1.6913936924934387
  batch 550 loss: 1.6714985704421996
  batch 600 loss: 1.6314775919914246
  batch 650 loss: 1.6711902213096619
  batch 700 loss: 1.6632934165000917
  batch 750 loss: 1.6702039575576781
  batch 800 loss: 1.6698551630973817
  batch 850 loss: 1.6577275109291076
  batch 900 loss: 1.6441928243637085
max grad: tensor(0.2371)
LOSS train 1.64419 valid 1.57685, valid PER 63.83%
Training finished in 8.0 minutes.
Model saved to checkpoints/20230116_135716/model_20
Loading model from checkpoints/20230116_135716/model_20
SUB: 3.23%, DEL: 61.63%, INS: 0.03%, COR: 35.15%, PER: 64.88%
