Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.2)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.123313798904419
  batch 100 loss: 3.1228980350494386
  batch 150 loss: 3.022001180648804
  batch 200 loss: 2.902886724472046
  batch 250 loss: 2.883433017730713
  batch 300 loss: 2.6747835159301756
  batch 350 loss: 2.5358660888671873
  batch 400 loss: 2.4872144794464113
  batch 450 loss: 2.4409068536758425
  batch 500 loss: 2.240878369808197
  batch 550 loss: 2.14447233915329
  batch 600 loss: 2.0832896876335143
  batch 650 loss: 1.9974625325202942
  batch 700 loss: 2.002544012069702
  batch 750 loss: 1.9258506512641906
  batch 800 loss: 1.9041211771965028
  batch 850 loss: 1.8470005464553834
  batch 900 loss: 1.850102138519287
LOSS train 1.85010 valid 1.79726, valid PER 70.59%
EPOCH 2:
  batch 50 loss: 1.8109047937393188
  batch 100 loss: 1.781890060901642
  batch 150 loss: 1.6689138507843018
  batch 200 loss: 1.7058208227157592
  batch 250 loss: 1.6810075521469117
  batch 300 loss: 1.6739774966239929
  batch 350 loss: 1.6488808679580689
  batch 400 loss: 1.6106954669952394
  batch 450 loss: 1.6289417886734008
  batch 500 loss: 1.5947476935386657
  batch 550 loss: 1.583353383541107
  batch 600 loss: 1.5667205405235292
  batch 650 loss: 1.539897153377533
  batch 700 loss: 1.5549568939208984
  batch 750 loss: 1.5165443181991578
  batch 800 loss: 1.4710028886795044
  batch 850 loss: 1.5024536204338075
  batch 900 loss: 1.4594089436531066
LOSS train 1.45941 valid 1.45385, valid PER 51.16%
EPOCH 3:
  batch 50 loss: 1.398475968837738
  batch 100 loss: 1.4691961908340454
  batch 150 loss: 1.4717792868614197
  batch 200 loss: 1.3753366374969482
  batch 250 loss: 1.4108903670310975
  batch 300 loss: 1.4251022458076477
  batch 350 loss: 1.4308567810058594
  batch 400 loss: 1.3843319702148438
  batch 450 loss: 1.3592480444908142
  batch 500 loss: 1.3354676795005798
  batch 550 loss: 1.364099484682083
  batch 600 loss: 1.284509596824646
  batch 650 loss: 1.308041797876358
  batch 700 loss: 1.3301601326465606
  batch 750 loss: 1.359283926486969
  batch 800 loss: 1.3443578863143921
  batch 850 loss: 1.3337330412864685
  batch 900 loss: 1.3093931019306182
LOSS train 1.30939 valid 1.23319, valid PER 39.74%
EPOCH 4:
  batch 50 loss: 1.297426505088806
  batch 100 loss: 1.2520542907714844
  batch 150 loss: 1.2786772751808166
  batch 200 loss: 1.273234453201294
  batch 250 loss: 1.316559352874756
  batch 300 loss: 1.2562694585323333
  batch 350 loss: 1.262980649471283
  batch 400 loss: 1.1961113381385804
  batch 450 loss: 1.2435608279705048
  batch 500 loss: 1.2940990567207336
  batch 550 loss: 1.2081684958934784
  batch 600 loss: 1.1665455603599548
  batch 650 loss: 1.2637061548233033
  batch 700 loss: 1.2846765518188477
  batch 750 loss: 1.2156314659118652
  batch 800 loss: 1.1950630474090576
  batch 850 loss: 1.183612689971924
  batch 900 loss: 1.2039143323898316
LOSS train 1.20391 valid 1.16376, valid PER 37.50%
EPOCH 5:
  batch 50 loss: 1.1766216921806336
  batch 100 loss: 1.1813718378543854
  batch 150 loss: 1.1924730145931244
  batch 200 loss: 1.2177092778682708
  batch 250 loss: 1.165243351459503
  batch 300 loss: 1.1777959632873536
  batch 350 loss: 1.1431128883361816
  batch 400 loss: 1.1467642557621003
  batch 450 loss: 1.1296032214164733
  batch 500 loss: 1.1357532572746276
  batch 550 loss: 1.1788941860198974
  batch 600 loss: 1.1973070549964904
  batch 650 loss: 1.1556517004966735
  batch 700 loss: 1.1376988971233368
  batch 750 loss: 1.136039333343506
  batch 800 loss: 1.1884621429443358
  batch 850 loss: 1.176657327413559
  batch 900 loss: 1.147684292793274
LOSS train 1.14768 valid 1.10303, valid PER 35.62%
EPOCH 6:
  batch 50 loss: 1.113787543773651
  batch 100 loss: 1.1270701563358307
  batch 150 loss: 1.1486459827423097
  batch 200 loss: 1.1102806913852692
  batch 250 loss: 1.1262583136558533
  batch 300 loss: 1.12874258518219
  batch 350 loss: 1.1340050983428955
  batch 400 loss: 1.0947013783454895
  batch 450 loss: 1.1326420760154725
  batch 500 loss: 1.0722608828544617
  batch 550 loss: 1.1296096658706665
  batch 600 loss: 1.1096811270713807
  batch 650 loss: 1.0574278020858765
  batch 700 loss: 1.0748750245571137
  batch 750 loss: 1.114841994047165
  batch 800 loss: 1.1326554369926454
  batch 850 loss: 1.1377932596206666
  batch 900 loss: 1.1377628600597383
LOSS train 1.13776 valid 1.08098, valid PER 35.16%
EPOCH 7:
  batch 50 loss: 1.0618003499507904
  batch 100 loss: 1.1125401198863982
  batch 150 loss: 1.03139946103096
  batch 200 loss: 1.0672430682182312
  batch 250 loss: 1.0932018947601319
  batch 300 loss: 1.0546097540855408
  batch 350 loss: 1.0886311113834382
  batch 400 loss: 1.061169615983963
  batch 450 loss: 1.0477093875408172
  batch 500 loss: 1.0405526626110078
  batch 550 loss: 1.0598977947235106
  batch 600 loss: 1.0502882862091065
  batch 650 loss: 1.0364777421951294
  batch 700 loss: 1.0763442039489746
  batch 750 loss: 1.0483005464076995
  batch 800 loss: 1.043530477285385
  batch 850 loss: 1.0367343962192535
  batch 900 loss: 1.0441273522377015
LOSS train 1.04413 valid 1.05764, valid PER 34.68%
EPOCH 8:
  batch 50 loss: 1.0437260711193084
  batch 100 loss: 0.9983044445514679
  batch 150 loss: 1.0319129061698913
  batch 200 loss: 1.0176377546787263
  batch 250 loss: 1.0079086017608643
  batch 300 loss: 1.0006109285354614
  batch 350 loss: 1.0290346193313598
  batch 400 loss: 1.0308435988426208
  batch 450 loss: 1.0810989916324616
  batch 500 loss: 1.0504414737224579
  batch 550 loss: 1.0551366007328034
  batch 600 loss: 0.994605393409729
  batch 650 loss: 1.0076349294185638
  batch 700 loss: 1.0530232346057893
  batch 750 loss: 1.0395509243011474
  batch 800 loss: 1.0500207424163819
  batch 850 loss: 1.0000332009792328
  batch 900 loss: 1.033355212211609
LOSS train 1.03336 valid 1.02449, valid PER 32.64%
EPOCH 9:
  batch 50 loss: 0.9760294222831726
  batch 100 loss: 0.9666844952106476
  batch 150 loss: 0.9856789720058441
  batch 200 loss: 0.9744255542755127
  batch 250 loss: 0.9665100204944611
  batch 300 loss: 1.0096388506889342
  batch 350 loss: 0.9637511432170868
  batch 400 loss: 1.0004146003723144
  batch 450 loss: 1.0278908169269563
  batch 500 loss: 0.9869469106197357
  batch 550 loss: 0.9984219920635223
  batch 600 loss: 1.0049792671203612
  batch 650 loss: 1.0126801478862761
  batch 700 loss: 0.9824712812900543
  batch 750 loss: 0.9998132860660554
  batch 800 loss: 1.0176002609729766
  batch 850 loss: 1.0321121191978455
  batch 900 loss: 1.0108677268028259
LOSS train 1.01087 valid 1.01880, valid PER 31.74%
EPOCH 10:
  batch 50 loss: 0.9519699311256409
  batch 100 loss: 0.974891928434372
  batch 150 loss: 1.0085656130313874
  batch 200 loss: 0.9606366038322449
  batch 250 loss: 0.9665874576568604
  batch 300 loss: 0.9670997953414917
  batch 350 loss: 0.9862981736660004
  batch 400 loss: 0.9537275815010071
  batch 450 loss: 0.9527875590324402
  batch 500 loss: 0.9609406387805939
  batch 550 loss: 0.9593251371383666
  batch 600 loss: 0.9588571727275849
  batch 650 loss: 0.9949631142616272
  batch 700 loss: 0.9946450996398926
  batch 750 loss: 1.0300211465358735
  batch 800 loss: 1.04103156208992
  batch 850 loss: 1.009702364206314
  batch 900 loss: 0.989926096200943
LOSS train 0.98993 valid 1.04123, valid PER 34.64%
EPOCH 11:
  batch 50 loss: 0.9536872589588166
  batch 100 loss: 0.9408902728557587
  batch 150 loss: 0.9365847432613372
  batch 200 loss: 0.9008719968795776
  batch 250 loss: 0.9229209518432617
  batch 300 loss: 0.9399201011657715
  batch 350 loss: 0.9678575217723846
  batch 400 loss: 0.92299671292305
  batch 450 loss: 0.9549785923957824
  batch 500 loss: 0.9355088245868682
  batch 550 loss: 0.9889240777492523
  batch 600 loss: 0.9625735282897949
  batch 650 loss: 0.9595955097675324
  batch 700 loss: 1.0102665424346924
  batch 750 loss: 0.9494065856933593
  batch 800 loss: 0.9816768586635589
  batch 850 loss: 0.950760954618454
  batch 900 loss: 0.9942171192169189
LOSS train 0.99422 valid 0.97483, valid PER 31.06%
EPOCH 12:
  batch 50 loss: 0.897619161605835
  batch 100 loss: 0.8963698410987854
  batch 150 loss: 0.9103513443470002
  batch 200 loss: 0.9413486742973327
  batch 250 loss: 0.900205888748169
  batch 300 loss: 0.9569074523448944
  batch 350 loss: 0.9448274207115174
  batch 400 loss: 0.9636511516571045
  batch 450 loss: 0.9228415632247925
  batch 500 loss: 0.9501499330997467
  batch 550 loss: 0.9339197862148285
  batch 600 loss: 0.9458384549617768
  batch 650 loss: 0.940326292514801
  batch 700 loss: 0.9311514914035797
  batch 750 loss: 0.9763624000549317
  batch 800 loss: 0.89840669631958
  batch 850 loss: 0.937765531539917
  batch 900 loss: 0.9535562121868133
LOSS train 0.95356 valid 1.00037, valid PER 31.24%
EPOCH 13:
  batch 50 loss: 0.8951811504364013
  batch 100 loss: 0.9083772325515747
  batch 150 loss: 0.9393877673149109
  batch 200 loss: 0.8804476463794708
  batch 250 loss: 0.8963629305362701
  batch 300 loss: 0.9391013360023499
  batch 350 loss: 0.9033317178487777
  batch 400 loss: 0.9163867902755737
  batch 450 loss: 0.9358332228660583
  batch 500 loss: 0.9049528300762176
  batch 550 loss: 0.9709041166305542
  batch 600 loss: 0.930279883146286
  batch 650 loss: 0.9206977665424347
  batch 700 loss: 0.9554399359226227
  batch 750 loss: 0.9105429112911224
  batch 800 loss: 0.9188473474979401
  batch 850 loss: 0.9298136067390442
  batch 900 loss: 0.9479091131687164
LOSS train 0.94791 valid 0.99177, valid PER 31.11%
EPOCH 14:
  batch 50 loss: 0.9256045234203338
  batch 100 loss: 0.9035731637477875
  batch 150 loss: 0.8931568324565887
  batch 200 loss: 0.8845404243469238
  batch 250 loss: 0.916883761882782
  batch 300 loss: 0.8736593890190124
  batch 350 loss: 0.9020749521255493
  batch 400 loss: 0.9455264258384705
  batch 450 loss: 0.8928964257240295
  batch 500 loss: 0.9069284534454346
  batch 550 loss: 0.8958358478546142
  batch 600 loss: 0.8974966537952423
  batch 650 loss: 0.90780611038208
  batch 700 loss: 0.9021382606029511
  batch 750 loss: 0.8940648782253265
  batch 800 loss: 0.9007419228553772
  batch 850 loss: 0.9111234366893768
  batch 900 loss: 0.9131274342536926
LOSS train 0.91313 valid 0.97373, valid PER 30.91%
EPOCH 15:
  batch 50 loss: 0.841773669719696
  batch 100 loss: 0.8624415791034699
  batch 150 loss: 0.8415271425247193
  batch 200 loss: 0.8900577318668366
  batch 250 loss: 0.8778609836101532
  batch 300 loss: 0.8762075901031494
  batch 350 loss: 0.8548176729679108
  batch 400 loss: 0.8864599335193634
  batch 450 loss: 0.8838729572296142
  batch 500 loss: 0.880649995803833
  batch 550 loss: 0.9123474216461182
  batch 600 loss: 0.918799444437027
  batch 650 loss: 0.881483393907547
  batch 700 loss: 0.8770567774772644
  batch 750 loss: 0.9048915719985962
  batch 800 loss: 0.8765862441062927
  batch 850 loss: 0.8891695201396942
  batch 900 loss: 0.8556485843658447
LOSS train 0.85565 valid 0.95735, valid PER 31.15%
EPOCH 16:
  batch 50 loss: 0.8659324884414673
  batch 100 loss: 0.8306838345527648
  batch 150 loss: 0.8730953073501587
  batch 200 loss: 0.9451621115207672
  batch 250 loss: 0.8892765581607819
  batch 300 loss: 0.946720097064972
  batch 350 loss: 0.9221273529529571
  batch 400 loss: 0.8753208112716675
  batch 450 loss: 0.9107255697250366
  batch 500 loss: 0.9026914870738983
  batch 550 loss: 0.8788960218429566
  batch 600 loss: 0.9250919139385223
  batch 650 loss: 0.942919191122055
  batch 700 loss: 0.8879066479206085
  batch 750 loss: 0.9107973301410675
  batch 800 loss: 0.8926313471794128
  batch 850 loss: 0.8429751038551331
  batch 900 loss: 0.8872977674007416
LOSS train 0.88730 valid 0.95961, valid PER 30.88%
EPOCH 17:
  batch 50 loss: 0.8667541313171386
  batch 100 loss: 0.8044709575176239
  batch 150 loss: 0.8781663119792938
  batch 200 loss: 0.8402022528648376
  batch 250 loss: 0.8790290451049805
  batch 300 loss: 0.8529562020301819
  batch 350 loss: 0.8752108108997345
  batch 400 loss: 0.8954682159423828
  batch 450 loss: 0.8775233161449433
  batch 500 loss: 0.8616042423248291
  batch 550 loss: 0.8582773172855377
  batch 600 loss: 0.8915178966522217
  batch 650 loss: 0.8528303611278534
  batch 700 loss: 0.8650589597225189
  batch 750 loss: 0.8311233019828796
  batch 800 loss: 0.8920154547691346
  batch 850 loss: 0.8431302750110626
  batch 900 loss: 0.8651189482212067
LOSS train 0.86512 valid 0.95269, valid PER 29.92%
EPOCH 18:
  batch 50 loss: 0.8215405762195587
  batch 100 loss: 0.8091764640808106
  batch 150 loss: 0.8810200250148773
  batch 200 loss: 0.8268095099925995
  batch 250 loss: 0.8141993844509124
  batch 300 loss: 0.8518676960468292
  batch 350 loss: 0.8286763644218444
  batch 400 loss: 0.8504621922969818
  batch 450 loss: 0.8942366671562195
  batch 500 loss: 0.9096265017986298
  batch 550 loss: 0.9287982404232025
  batch 600 loss: 0.8952912163734436
  batch 650 loss: 0.8670174694061279
  batch 700 loss: 0.8430531740188598
  batch 750 loss: 0.8637810683250428
  batch 800 loss: 0.877825665473938
  batch 850 loss: 0.8614228963851929
  batch 900 loss: 0.864703756570816
LOSS train 0.86470 valid 0.94573, valid PER 30.18%
EPOCH 19:
  batch 50 loss: 0.858550192117691
  batch 100 loss: 0.8580341529846192
  batch 150 loss: 0.8348531544208526
  batch 200 loss: 0.8242476892471313
  batch 250 loss: 0.8616277778148651
  batch 300 loss: 0.8678681015968323
  batch 350 loss: 0.8996482539176941
  batch 400 loss: 0.8533847284317017
  batch 450 loss: 0.7998510015010833
  batch 500 loss: 0.8465129506587982
  batch 550 loss: 0.885847235918045
  batch 600 loss: 0.8939501821994782
  batch 650 loss: 0.8888687121868134
  batch 700 loss: 0.8833802127838135
  batch 750 loss: 0.8325235438346863
  batch 800 loss: 0.8179249560832977
  batch 850 loss: 0.8723820841312409
  batch 900 loss: 0.8481851446628571
LOSS train 0.84819 valid 0.94118, valid PER 29.90%
EPOCH 20:
  batch 50 loss: 0.7898690354824066
  batch 100 loss: 0.8178558588027954
  batch 150 loss: 0.8329659974575043
  batch 200 loss: 0.8526062381267547
  batch 250 loss: 0.8560393321514129
  batch 300 loss: 0.8374750924110412
  batch 350 loss: 0.8192497289180756
  batch 400 loss: 0.8383843636512757
  batch 450 loss: 0.8239268267154694
  batch 500 loss: 0.8383379030227661
  batch 550 loss: 0.8276206290721894
  batch 600 loss: 0.803243253827095
  batch 650 loss: 0.8568066453933716
  batch 700 loss: 0.8301267313957215
  batch 750 loss: 0.8051983213424683
  batch 800 loss: 0.8434360527992248
  batch 850 loss: 0.8265917801856995
  batch 900 loss: 0.8245928370952607
LOSS train 0.82459 valid 0.96824, valid PER 30.03%
Training finished in 10.0 minutes.
Model saved to checkpoints/20230118_153522/model_19
Loading model from checkpoints/20230118_153522/model_19
SUB: 15.92%, DEL: 13.96%, INS: 1.97%, COR: 70.12%, PER: 31.85%
