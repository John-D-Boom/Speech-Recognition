Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.2)
Total number of model parameters is 166952
EPOCH 1:
LEARNING RATE: 0.5
  batch 50 loss: 3.9641328620910645
  batch 100 loss: 3.139042468070984
  batch 150 loss: 3.037078790664673
  batch 200 loss: 2.90415397644043
  batch 250 loss: 2.8386203336715696
  batch 300 loss: 2.678347935676575
  batch 350 loss: 2.5570994663238524
  batch 400 loss: 2.502242016792297
  batch 450 loss: 2.4515716552734377
  batch 500 loss: 2.429089903831482
  batch 550 loss: 2.360381031036377
  batch 600 loss: 2.2587992644309995
  batch 650 loss: 2.032890725135803
  batch 700 loss: 2.020016849040985
  batch 750 loss: 1.9523259544372558
  batch 800 loss: 1.9168035435676574
  batch 850 loss: 1.8627261590957642
  batch 900 loss: 1.835181851387024
LOSS train 1.83518 valid 1.82984, valid PER 70.76%
loss updated
EPOCH 2:
LEARNING RATE: 0.25
  batch 50 loss: 1.8148057985305786
  batch 100 loss: 1.778996732234955
  batch 150 loss: 1.6651083612442017
  batch 200 loss: 1.7017571115493775
  batch 250 loss: 1.696084690093994
  batch 300 loss: 1.6567426204681397
  batch 350 loss: 1.6544570159912109
  batch 400 loss: 1.609289631843567
  batch 450 loss: 1.6186194133758545
  batch 500 loss: 1.5936405324935914
  batch 550 loss: 1.5807556653022765
  batch 600 loss: 1.5747192597389221
  batch 650 loss: 1.5326178693771362
  batch 700 loss: 1.5496517777442933
  batch 750 loss: 1.518257701396942
  batch 800 loss: 1.482669677734375
  batch 850 loss: 1.498761157989502
  batch 900 loss: 1.4536402130126953
LOSS train 1.45364 valid 1.42771, valid PER 48.69%
loss updated
EPOCH 3:
LEARNING RATE: 0.125
  batch 50 loss: 1.3970487785339356
  batch 100 loss: 1.4506530284881591
  batch 150 loss: 1.460940318107605
  batch 200 loss: 1.3713898158073425
  batch 250 loss: 1.3877496099472046
  batch 300 loss: 1.4076709842681885
  batch 350 loss: 1.4323568892478944
  batch 400 loss: 1.3662362730503081
  batch 450 loss: 1.352579538822174
  batch 500 loss: 1.3168382620811463
  batch 550 loss: 1.3646086263656616
  batch 600 loss: 1.292834415435791
  batch 650 loss: 1.2929445135593414
  batch 700 loss: 1.3011980330944062
  batch 750 loss: 1.3532917308807373
  batch 800 loss: 1.3333707213401795
  batch 850 loss: 1.3094862031936645
  batch 900 loss: 1.2955975246429443
LOSS train 1.29560 valid 1.21124, valid PER 39.14%
loss updated
EPOCH 4:
LEARNING RATE: 0.0625
  batch 50 loss: 1.2768570721149444
  batch 100 loss: 1.2213889610767366
  batch 150 loss: 1.2538406109809876
  batch 200 loss: 1.2334207439422606
  batch 250 loss: 1.2632802045345306
  batch 300 loss: 1.2235128700733184
  batch 350 loss: 1.2472211933135986
  batch 400 loss: 1.188143949508667
  batch 450 loss: 1.2235698449611663
  batch 500 loss: 1.281773841381073
  batch 550 loss: 1.2259637558460235
  batch 600 loss: 1.186127097606659
  batch 650 loss: 1.285293354988098
  batch 700 loss: 1.2940140306949615
  batch 750 loss: 1.208167804479599
  batch 800 loss: 1.229637804031372
  batch 850 loss: 1.2075926625728608
  batch 900 loss: 1.2326962995529174
LOSS train 1.23270 valid 1.17873, valid PER 36.89%
loss updated
EPOCH 5:
LEARNING RATE: 0.03125
  batch 50 loss: 1.1911225497722626
  batch 100 loss: 1.1761072957515717
  batch 150 loss: 1.178244981765747
  batch 200 loss: 1.2061029958724976
  batch 250 loss: 1.183876371383667
  batch 300 loss: 1.1906791996955872
  batch 350 loss: 1.1415808594226837
  batch 400 loss: 1.1395147919654847
  batch 450 loss: 1.1334792900085449
  batch 500 loss: 1.1259440863132477
  batch 550 loss: 1.1773284423351287
  batch 600 loss: 1.2017199969291688
  batch 650 loss: 1.1702631664276124
  batch 700 loss: 1.1654202437400818
  batch 750 loss: 1.1851801443099976
  batch 800 loss: 1.2019601166248322
  batch 850 loss: 1.2065089893341066
  batch 900 loss: 1.1445532858371734
LOSS train 1.14455 valid 1.11431, valid PER 35.61%
loss updated
EPOCH 6:
LEARNING RATE: 0.015625
  batch 50 loss: 1.13097904920578
  batch 100 loss: 1.1762841820716858
  batch 150 loss: 1.163858848810196
  batch 200 loss: 1.0848082637786864
  batch 250 loss: 1.1594465637207032
  batch 300 loss: 1.1563676548004151
  batch 350 loss: 1.1412628149986268
  batch 400 loss: 1.1301980602741242
  batch 450 loss: 1.127421190738678
  batch 500 loss: 1.1012436592578887
  batch 550 loss: 1.1312314188480377
  batch 600 loss: 1.120767560005188
  batch 650 loss: 1.0887596583366395
  batch 700 loss: 1.076381092071533
  batch 750 loss: 1.1064430475234985
  batch 800 loss: 1.1450675451755523
  batch 850 loss: 1.1501091396808625
  batch 900 loss: 1.1394525635242463
LOSS train 1.13945 valid 1.09641, valid PER 35.38%
loss updated
EPOCH 7:
LEARNING RATE: 0.0078125
  batch 50 loss: 1.0968993747234344
  batch 100 loss: 1.12620858669281
  batch 150 loss: 1.0452855706214905
  batch 200 loss: 1.0756224548816682
  batch 250 loss: 1.1217106819152831
  batch 300 loss: 1.0584117698669433
  batch 350 loss: 1.120713983774185
  batch 400 loss: 1.0926677978038788
  batch 450 loss: 1.0959840881824494
  batch 500 loss: 1.0920404529571532
  batch 550 loss: 1.1182871091365814
  batch 600 loss: 1.084889553785324
  batch 650 loss: 1.0588852918148042
  batch 700 loss: 1.1253270840644836
  batch 750 loss: 1.0906569290161132
  batch 800 loss: 1.085531346797943
  batch 850 loss: 1.0752962124347687
  batch 900 loss: 1.0566577911376953
LOSS train 1.05666 valid 1.10130, valid PER 35.84%
EPOCH 8:
LEARNING RATE: 0.0078125
  batch 50 loss: 1.0740832269191742
  batch 100 loss: 1.0524197912216187
  batch 150 loss: 1.0878829991817474
  batch 200 loss: 1.0596885764598847
  batch 250 loss: 1.0375253856182098
  batch 300 loss: 1.0456448674201966
  batch 350 loss: 1.0427193975448608
  batch 400 loss: 1.0448140072822572
  batch 450 loss: 1.0834133529663086
  batch 500 loss: 1.0757411849498748
  batch 550 loss: 1.0888143908977508
  batch 600 loss: 1.006636073589325
  batch 650 loss: 1.0103061950206758
  batch 700 loss: 1.0648789882659913
  batch 750 loss: 1.0635629546642305
  batch 800 loss: 1.0551165556907653
  batch 850 loss: 1.0098621201515199
  batch 900 loss: 1.0330341315269471
LOSS train 1.03303 valid 1.04357, valid PER 33.14%
loss updated
EPOCH 9:
LEARNING RATE: 0.00390625
  batch 50 loss: 1.0603798151016235
  batch 100 loss: 1.0092774164676666
  batch 150 loss: 1.0218810760974884
  batch 200 loss: 0.992006688117981
  batch 250 loss: 0.9946296381950378
  batch 300 loss: 1.0101667904853822
  batch 350 loss: 1.0262213456630707
  batch 400 loss: 1.0537742710113525
  batch 450 loss: 1.0739707279205322
  batch 500 loss: 1.0232867729663848
  batch 550 loss: 1.1016952419281005
  batch 600 loss: 1.1479322814941406
  batch 650 loss: 1.070866870880127
  batch 700 loss: 1.0365962421894073
  batch 750 loss: 1.0533571815490723
  batch 800 loss: 1.0994924914836883
  batch 850 loss: 1.1013035154342652
  batch 900 loss: 1.0320567178726197
LOSS train 1.03206 valid 1.04370, valid PER 33.59%
EPOCH 10:
LEARNING RATE: 0.00390625
  batch 50 loss: 0.9919958245754242
  batch 100 loss: 1.040621167421341
  batch 150 loss: 1.049174644947052
  batch 200 loss: 1.0057416355609894
  batch 250 loss: 0.9915589213371276
  batch 300 loss: 1.0161870408058167
  batch 350 loss: 1.0029161393642425
  batch 400 loss: 0.9738271927833557
  batch 450 loss: 1.0287998354434966
  batch 500 loss: 1.044957892894745
  batch 550 loss: 1.016464991569519
  batch 600 loss: 0.9973608005046845
  batch 650 loss: 1.0320807707309723
  batch 700 loss: 1.0567337906360625
  batch 750 loss: 1.023444036245346
  batch 800 loss: 1.0247304964065551
  batch 850 loss: 1.0720849740505218
  batch 900 loss: 1.0572016358375549
LOSS train 1.05720 valid 1.09144, valid PER 35.38%
EPOCH 11:
LEARNING RATE: 0.00390625
  batch 50 loss: 1.002554590702057
  batch 100 loss: 0.9846451377868652
  batch 150 loss: 0.9884651374816894
  batch 200 loss: 1.000160619020462
  batch 250 loss: 1.0029801070690154
  batch 300 loss: 0.9888992011547089
  batch 350 loss: 1.0473710763454438
  batch 400 loss: 0.9660660135746002
  batch 450 loss: 1.0792191624641418
  batch 500 loss: 1.105034967660904
  batch 550 loss: 1.1022492158412933
  batch 600 loss: 1.0881618416309358
  batch 650 loss: 1.0803666460514068
  batch 700 loss: 1.1561249339580535
  batch 750 loss: 1.04008274435997
  batch 800 loss: 1.043391259908676
  batch 850 loss: 1.0530688226222993
  batch 900 loss: 1.0550799107551574
LOSS train 1.05508 valid 1.04442, valid PER 33.46%
EPOCH 12:
LEARNING RATE: 0.00390625
  batch 50 loss: 1.0771730983257293
  batch 100 loss: 1.0147962880134582
  batch 150 loss: 1.0286510455608369
  batch 200 loss: 1.0156960833072661
  batch 250 loss: 1.0659193539619445
  batch 300 loss: 1.0884794223308563
  batch 350 loss: 1.023144347667694
  batch 400 loss: 1.0390461599826812
  batch 450 loss: 1.002295149564743
  batch 500 loss: 1.0238203692436219
  batch 550 loss: 1.0533691477775573
  batch 600 loss: 1.0650505065917968
  batch 650 loss: 1.0222445631027222
  batch 700 loss: 1.0129504585266114
  batch 750 loss: 1.0501531624794007
  batch 800 loss: 0.9876511895656586
  batch 850 loss: 1.0061613249778747
  batch 900 loss: 1.017609544992447
LOSS train 1.01761 valid 1.03363, valid PER 32.34%
loss updated
EPOCH 13:
LEARNING RATE: 0.001953125
  batch 50 loss: 0.9663192641735077
  batch 100 loss: 0.9769530320167541
  batch 150 loss: 0.9965520310401916
  batch 200 loss: 0.9565752673149109
  batch 250 loss: 0.969159939289093
  batch 300 loss: 1.0030615651607513
  batch 350 loss: 0.9727758920192718
  batch 400 loss: 1.0169570314884187
  batch 450 loss: 1.06426043510437
  batch 500 loss: 1.0100024247169495
  batch 550 loss: 1.0902521312236786
  batch 600 loss: 1.0706682467460633
  batch 650 loss: 1.0479749071598052
  batch 700 loss: 1.0577510797977447
  batch 750 loss: 0.9848407459259033
  batch 800 loss: 1.017475550174713
  batch 850 loss: 0.9786691474914551
  batch 900 loss: 0.9936509966850281
LOSS train 0.99365 valid 1.01912, valid PER 32.39%
loss updated
EPOCH 14:
LEARNING RATE: 0.0009765625
  batch 50 loss: 0.9789787173271179
  batch 100 loss: 0.9552784550189972
  batch 150 loss: 0.9768652093410491
  batch 200 loss: 1.026608579158783
  batch 250 loss: 0.9846553862094879
  batch 300 loss: 0.9975103640556335
  batch 350 loss: 0.9945117032527924
  batch 400 loss: 1.0557699728012084
  batch 450 loss: 0.99636305809021
  batch 500 loss: 1.037373880147934
  batch 550 loss: 1.0074503898620606
  batch 600 loss: 0.9687366330623627
  batch 650 loss: 0.9891984152793885
  batch 700 loss: 0.9980481207370758
  batch 750 loss: 0.9906117606163025
  batch 800 loss: 1.0088528490066528
  batch 850 loss: 1.0324183022975921
  batch 900 loss: 0.9769002735614777
LOSS train 0.97690 valid 1.10451, valid PER 34.64%
EPOCH 15:
LEARNING RATE: 0.0009765625
  batch 50 loss: 0.9643965995311737
  batch 100 loss: 0.9717779302597046
  batch 150 loss: 0.9311980175971984
  batch 200 loss: 0.962482385635376
  batch 250 loss: 0.9633759713172912
  batch 300 loss: 0.9571991157531738
  batch 350 loss: 0.9296919047832489
  batch 400 loss: 0.9651766896247864
  batch 450 loss: 0.9504291141033172
  batch 500 loss: 0.9480070209503174
  batch 550 loss: 0.9749224877357483
  batch 600 loss: 0.9788931047916413
  batch 650 loss: 0.9236528027057648
  batch 700 loss: 0.9259173262119293
  batch 750 loss: 0.953114504814148
  batch 800 loss: 0.9225545823574066
  batch 850 loss: 0.9151176166534424
  batch 900 loss: 0.9090269362926483
LOSS train 0.90903 valid 1.06086, valid PER 33.84%
EPOCH 16:
LEARNING RATE: 0.0009765625
  batch 50 loss: 0.9561465668678284
  batch 100 loss: 0.9035611605644226
  batch 150 loss: 0.9394535434246063
  batch 200 loss: 0.9467153525352479
  batch 250 loss: 0.9216053295135498
  batch 300 loss: 0.9570437681674957
  batch 350 loss: 0.9289853703975678
  batch 400 loss: 0.9360648798942566
  batch 450 loss: 0.9636620151996612
  batch 500 loss: 0.97042560338974
  batch 550 loss: 0.944023540019989
  batch 600 loss: 0.9544302880764007
  batch 650 loss: 0.9561948299407959
  batch 700 loss: 1.2304641675949097
  batch 750 loss: 1.22710653424263
  batch 800 loss: 1.05937628865242
  batch 850 loss: 1.0237529182434082
  batch 900 loss: 0.986940107345581
LOSS train 0.98694 valid 1.04852, valid PER 33.61%
EPOCH 17:
LEARNING RATE: 0.0009765625
  batch 50 loss: 0.9307034039497375
  batch 100 loss: 0.887057284116745
  batch 150 loss: 0.9602521181106567
  batch 200 loss: 0.9227423965930939
  batch 250 loss: 0.9671164417266845
  batch 300 loss: 0.9391427326202393
  batch 350 loss: 0.9370345318317413
  batch 400 loss: 0.980573046207428
  batch 450 loss: 0.9459955024719239
  batch 500 loss: 0.9332658469676971
  batch 550 loss: 0.9668479204177857
  batch 600 loss: 1.0179606020450591
  batch 650 loss: 1.0136089718341827
  batch 700 loss: 0.9937502384185791
  batch 750 loss: 0.949131338596344
  batch 800 loss: 0.9975214183330536
  batch 850 loss: 0.9557181167602539
  batch 900 loss: 0.9472596967220306
LOSS train 0.94726 valid 1.02415, valid PER 32.27%
EPOCH 18:
LEARNING RATE: 0.0009765625
  batch 50 loss: 0.9713173401355744
  batch 100 loss: 0.9409257960319519
  batch 150 loss: 0.9708957374095917
  batch 200 loss: 0.9537352418899536
  batch 250 loss: 1.024286712408066
  batch 300 loss: 1.0169246876239777
  batch 350 loss: 0.9239374196529389
  batch 400 loss: 0.9442657554149627
  batch 450 loss: 0.9403576123714447
  batch 500 loss: 0.9360918700695038
  batch 550 loss: 0.9607152450084686
  batch 600 loss: 0.9486139726638794
  batch 650 loss: 0.9112261641025543
  batch 700 loss: 0.913415653705597
  batch 750 loss: 0.8994920551776886
  batch 800 loss: 0.9529292750358581
  batch 850 loss: 0.9405340468883514
  batch 900 loss: 0.9125733363628388
LOSS train 0.91257 valid 0.97914, valid PER 31.03%
loss updated
EPOCH 19:
LEARNING RATE: 0.00048828125
  batch 50 loss: 0.881104187965393
  batch 100 loss: 0.9065170562267304
  batch 150 loss: 0.9099608945846558
  batch 200 loss: 0.8845987772941589
  batch 250 loss: 0.953338817358017
  batch 300 loss: 0.9323733401298523
  batch 350 loss: 0.9192342126369476
  batch 400 loss: 0.9578561294078827
  batch 450 loss: 0.8863272094726562
  batch 500 loss: 0.9192017877101898
  batch 550 loss: 0.9438857054710388
  batch 600 loss: 0.9688694882392883
  batch 650 loss: 0.9549063849449158
  batch 700 loss: 0.9770675897598267
  batch 750 loss: 0.9119769096374511
  batch 800 loss: 0.876761976480484
  batch 850 loss: 0.9214648473262786
  batch 900 loss: 0.9064814722537995
LOSS train 0.90648 valid 0.99010, valid PER 31.50%
EPOCH 20:
LEARNING RATE: 0.00048828125
  batch 50 loss: 0.8302485227584839
  batch 100 loss: 0.8527917790412903
  batch 150 loss: 0.867837507724762
  batch 200 loss: 0.868832870721817
  batch 250 loss: 0.90264204621315
  batch 300 loss: 0.8745054650306702
  batch 350 loss: 0.8604186475276947
  batch 400 loss: 0.8435643291473389
  batch 450 loss: 0.869888424873352
  batch 500 loss: 0.8928534412384033
  batch 550 loss: 0.8652236235141754
  batch 600 loss: 0.8421559309959412
  batch 650 loss: 0.9092569518089294
  batch 700 loss: 0.8855013275146484
  batch 750 loss: 0.8680513978004456
  batch 800 loss: 0.9569307398796082
  batch 850 loss: 0.9424897384643555
  batch 900 loss: 0.9271771681308746
LOSS train 0.92718 valid 0.98831, valid PER 31.86%
Training finished in 12.0 minutes.
Model saved to checkpoints/20230118_192119/model_18
Loading model from checkpoints/20230118_192119/model_18
SUB: 16.71%, DEL: 14.56%, INS: 1.68%, COR: 68.73%, PER: 32.95%
