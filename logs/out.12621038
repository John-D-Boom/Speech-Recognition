Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.2)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.123313798904419
  batch 100 loss: 3.1228980350494386
  batch 150 loss: 3.022001180648804
  batch 200 loss: 2.902886724472046
  batch 250 loss: 2.883433017730713
  batch 300 loss: 2.6747835159301756
  batch 350 loss: 2.5358660888671873
  batch 400 loss: 2.4872144794464113
  batch 450 loss: 2.4409068536758425
  batch 500 loss: 2.240878369808197
  batch 550 loss: 2.14447233915329
  batch 600 loss: 2.0832896876335143
  batch 650 loss: 1.9974625325202942
  batch 700 loss: 2.002544012069702
  batch 750 loss: 1.9258506512641906
  batch 800 loss: 1.9041211771965028
  batch 850 loss: 1.8470005464553834
  batch 900 loss: 1.850102138519287
LOSS train 1.85010 valid 1.79726, valid PER 70.59%
EPOCH 2:
  batch 50 loss: 1.8109047937393188
  batch 100 loss: 1.781890060901642
  batch 150 loss: 1.6689138507843018
  batch 200 loss: 1.7058208227157592
  batch 250 loss: 1.6810075521469117
  batch 300 loss: 1.6739774966239929
  batch 350 loss: 1.6488808679580689
  batch 400 loss: 1.6106954669952394
  batch 450 loss: 1.6289417886734008
  batch 500 loss: 1.5947476935386657
  batch 550 loss: 1.583353383541107
  batch 600 loss: 1.5667205405235292
  batch 650 loss: 1.539897153377533
  batch 700 loss: 1.5549568939208984
  batch 750 loss: 1.5165443181991578
  batch 800 loss: 1.4710028886795044
  batch 850 loss: 1.5024536204338075
  batch 900 loss: 1.4594089436531066
LOSS train 1.45941 valid 1.45385, valid PER 51.16%
EPOCH 3:
  batch 50 loss: 1.398475968837738
  batch 100 loss: 1.4691961908340454
  batch 150 loss: 1.4717792868614197
  batch 200 loss: 1.3753366374969482
  batch 250 loss: 1.4108903670310975
  batch 300 loss: 1.4251022458076477
  batch 350 loss: 1.4308567810058594
  batch 400 loss: 1.3843319702148438
  batch 450 loss: 1.3592480444908142
  batch 500 loss: 1.3354676795005798
  batch 550 loss: 1.364099484682083
  batch 600 loss: 1.284509596824646
