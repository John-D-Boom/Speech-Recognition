Namespace(seed=123, train_json='train_fbank_speeds.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=4, fbank_dims=23, model_dims=256, concat=1, lr=0.0005, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.4, beta1=0.9, beta2=0.999, exp=0.5)
Total number of model parameters is 5326888
EPOCH 1:
  batch 50 loss: 6.182499294281006
  batch 100 loss: 3.309565334320068
  batch 150 loss: 3.2891686582565307
  batch 200 loss: 3.271372356414795
  batch 250 loss: 3.2456861972808837
  batch 300 loss: 3.150920352935791
  batch 350 loss: 2.9465777254104615
  batch 400 loss: 2.808546195030212
  batch 450 loss: 2.713250675201416
  batch 500 loss: 2.5616760635375977
  batch 550 loss: 2.45688289642334
  batch 600 loss: 2.3856742191314697
  batch 650 loss: 2.23937516450882
  batch 700 loss: 2.1927542591094973
  batch 750 loss: 2.085560884475708
  batch 800 loss: 2.032571566104889
  batch 850 loss: 1.9358129906654358
  batch 900 loss: 1.8461440372467042
LOSS train 1.84614 valid 1.72281, valid PER 61.72%
loss: 0.00025
EPOCH 2:
  batch 50 loss: 1.7660707831382751
  batch 100 loss: 1.6925465273857117
  batch 150 loss: 1.6313011932373047
  batch 200 loss: 1.6038709211349487
  batch 250 loss: 1.5700410842895507
  batch 300 loss: 1.5017805671691895
  batch 350 loss: 1.5041435074806213
  batch 400 loss: 1.4624668455123901
  batch 450 loss: 1.421358118057251
  batch 500 loss: 1.4162190103530883
  batch 550 loss: 1.3737015676498414
  batch 600 loss: 1.3141660737991332
  batch 650 loss: 1.2873974275588989
  batch 700 loss: 1.30381920337677
  batch 750 loss: 1.285588641166687
  batch 800 loss: 1.2355392670631409
  batch 850 loss: 1.2245436525344848
  batch 900 loss: 1.208672412633896
LOSS train 1.20867 valid 1.11032, valid PER 35.66%
loss: 0.000125
EPOCH 3:
  batch 50 loss: 1.1440402090549469
  batch 100 loss: 1.1775536966323852
  batch 150 loss: 1.1790130150318145
  batch 200 loss: 1.1191032707691193
  batch 250 loss: 1.1260080444812774
  batch 300 loss: 1.12735582113266
  batch 350 loss: 1.1574523186683654
  batch 400 loss: 1.104619300365448
  batch 450 loss: 1.0900387930870057
  batch 500 loss: 1.0583350026607514
  batch 550 loss: 1.0692865085601806
  batch 600 loss: 1.0262875509262086
  batch 650 loss: 1.0462175679206849
  batch 700 loss: 1.0400494492053987
  batch 750 loss: 1.0414469730854035
  batch 800 loss: 1.0914540243148805
  batch 850 loss: 1.040565459728241
  batch 900 loss: 1.0442590284347535
LOSS train 1.04426 valid 0.97318, valid PER 30.70%
loss: 6.25e-05
EPOCH 4:
  batch 50 loss: 1.017691216468811
  batch 100 loss: 0.9491796553134918
  batch 150 loss: 0.9767672228813171
  batch 200 loss: 0.9581088387966156
  batch 250 loss: 0.9742481589317322
  batch 300 loss: 0.9904857420921326
  batch 350 loss: 0.9630285406112671
  batch 400 loss: 0.9312221503257752
  batch 450 loss: 0.9394670498371124
  batch 500 loss: 1.0139377117156982
  batch 550 loss: 0.9340749084949493
  batch 600 loss: 0.9125678634643555
  batch 650 loss: 0.9566669809818268
  batch 700 loss: 0.9386633217334748
  batch 750 loss: 0.9286723232269287
  batch 800 loss: 0.9135354351997376
  batch 850 loss: 0.897294955253601
  batch 900 loss: 0.9580663239955902
LOSS train 0.95807 valid 0.88199, valid PER 27.34%
loss: 3.125e-05
EPOCH 5:
  batch 50 loss: 0.8530768275260925
  batch 100 loss: 0.8620028376579285
  batch 150 loss: 0.8638696491718292
  batch 200 loss: 0.9150677502155304
  batch 250 loss: 0.8622100734710694
  batch 300 loss: 0.886816987991333
  batch 350 loss: 0.8358384501934052
  batch 400 loss: 0.799355206489563
  batch 450 loss: 0.8301435899734497
  batch 500 loss: 0.8161558043956757
  batch 550 loss: 0.8511671483516693
  batch 600 loss: 0.8719432997703552
  batch 650 loss: 0.841113097667694
  batch 700 loss: 0.8375837361812591
  batch 750 loss: 0.8133607745170593
  batch 800 loss: 0.8625943863391876
  batch 850 loss: 0.8617338490486145
  batch 900 loss: 0.8474670445919037
LOSS train 0.84747 valid 0.83398, valid PER 25.80%
loss: 1.5625e-05
EPOCH 6:
  batch 50 loss: 0.8193243718147278
  batch 100 loss: 0.8230260372161865
  batch 150 loss: 0.7744160175323487
  batch 200 loss: 0.7842172592878341
  batch 250 loss: 0.7861787915229798
  batch 300 loss: 0.8017104816436768
  batch 350 loss: 0.7975197911262513
  batch 400 loss: 0.7809707129001617
  batch 450 loss: 0.8052695524692536
  batch 500 loss: 0.7354385298490524
  batch 550 loss: 0.7772341203689576
  batch 600 loss: 0.7583752465248108
  batch 650 loss: 0.7345428025722504
  batch 700 loss: 0.7640135341882706
  batch 750 loss: 0.8035753810405731
  batch 800 loss: 0.7764068675041199
  batch 850 loss: 0.8027023148536682
  batch 900 loss: 0.7926921999454498
LOSS train 0.79269 valid 0.76904, valid PER 23.85%
loss: 7.8125e-06
EPOCH 7:
  batch 50 loss: 0.6991051071882248
  batch 100 loss: 0.7504472196102142
  batch 150 loss: 0.7315140068531036
  batch 200 loss: 0.7218289744853973
  batch 250 loss: 0.7784465837478638
  batch 300 loss: 0.7254718172550202
  batch 350 loss: 0.7561075079441071
  batch 400 loss: 0.7073886185884476
  batch 450 loss: 0.7310535341501236
  batch 500 loss: 0.7146151602268219
  batch 550 loss: 0.7141155499219894
  batch 600 loss: 0.7156984406709671
  batch 650 loss: 0.7168835693597794
  batch 700 loss: 0.7245341873168946
  batch 750 loss: 0.7549475276470184
  batch 800 loss: 0.7066682338714599
  batch 850 loss: 0.7037011021375656
  batch 900 loss: 0.7276778727769851
LOSS train 0.72768 valid 0.73996, valid PER 23.48%
loss: 3.90625e-06
EPOCH 8:
  batch 50 loss: 0.6832022881507873
  batch 100 loss: 0.6417472666501999
  batch 150 loss: 0.6695534759759902
  batch 200 loss: 0.6667156153917313
  batch 250 loss: 0.6773280739784241
  batch 300 loss: 0.6640499413013459
  batch 350 loss: 0.6563722318410874
  batch 400 loss: 0.6699842357635498
  batch 450 loss: 0.7007804089784622
  batch 500 loss: 0.679081027507782
  batch 550 loss: 0.6752191835641861
  batch 600 loss: 0.6679777610301971
  batch 650 loss: 0.6858857917785645
  batch 700 loss: 0.7066974502801895
  batch 750 loss: 0.694533703327179
  batch 800 loss: 0.7094269824028016
  batch 850 loss: 0.6918819183111191
  batch 900 loss: 0.672091463804245
LOSS train 0.67209 valid 0.72434, valid PER 22.26%
loss: 1.953125e-06
EPOCH 9:
  batch 50 loss: 0.6215149360895157
  batch 100 loss: 0.5959122067689896
  batch 150 loss: 0.6192088705301285
  batch 200 loss: 0.6164571160078048
  batch 250 loss: 0.5818873739242554
  batch 300 loss: 0.6502530485391617
  batch 350 loss: 0.6240797275304795
  batch 400 loss: 0.6625581330060959
  batch 450 loss: 0.6604594546556473
  batch 500 loss: 0.6348303073644638
  batch 550 loss: 0.6488804060220719
  batch 600 loss: 0.6710298997163773
  batch 650 loss: 0.6640391814708709
  batch 700 loss: 0.633290814757347
  batch 750 loss: 0.6749340081214905
  batch 800 loss: 0.6664635783433914
  batch 850 loss: 0.6442067182064056
  batch 900 loss: 0.6044371765851975
LOSS train 0.60444 valid 0.68624, valid PER 21.65%
loss: 9.765625e-07
EPOCH 10:
  batch 50 loss: 0.5676771885156632
  batch 100 loss: 0.5766203480958939
  batch 150 loss: 0.5986036568880081
  batch 200 loss: 0.5628432691097259
  batch 250 loss: 0.5887693375349045
  batch 300 loss: 0.597774550318718
  batch 350 loss: 0.5860606366395951
  batch 400 loss: 0.588955819606781
  batch 450 loss: 0.5914591854810715
  batch 500 loss: 0.5699248492717743
  batch 550 loss: 0.5768677711486816
  batch 600 loss: 0.5772711533308029
  batch 650 loss: 0.616047922372818
  batch 700 loss: 0.6056732779741287
  batch 750 loss: 0.6231717050075531
  batch 800 loss: 0.608786780834198
  batch 850 loss: 0.6070167332887649
  batch 900 loss: 0.59544677734375
LOSS train 0.59545 valid 0.69245, valid PER 21.19%
EPOCH 11:
  batch 50 loss: 0.5547780376672745
  batch 100 loss: 0.5365108370780944
  batch 150 loss: 0.5444959276914596
  batch 200 loss: 0.5136346936225891
  batch 250 loss: 0.5451028949022293
  batch 300 loss: 0.5314607346057891
  batch 350 loss: 0.5804239910840988
  batch 400 loss: 0.5499281638860702
  batch 450 loss: 0.5363677334785462
  batch 500 loss: 0.5636809605360031
  batch 550 loss: 0.5756455457210541
  batch 600 loss: 0.5520273655653
  batch 650 loss: 0.5764399176836014
  batch 700 loss: 0.5920990824699401
  batch 750 loss: 0.5803315794467926
  batch 800 loss: 0.5886250913143158
  batch 850 loss: 0.5832123446464539
  batch 900 loss: 0.5870951998233795
LOSS train 0.58710 valid 0.69795, valid PER 21.98%
EPOCH 12:
  batch 50 loss: 0.5249383699893951
  batch 100 loss: 0.5167688632011413
  batch 150 loss: 0.5157885652780533
  batch 200 loss: 0.5268001168966293
  batch 250 loss: 0.528432377576828
  batch 300 loss: 0.5266645878553391
  batch 350 loss: 0.5411868065595626
  batch 400 loss: 0.5476292824745178
  batch 450 loss: 0.5141552573442459
  batch 500 loss: 0.5339893704652786
  batch 550 loss: 0.5514700263738632
  batch 600 loss: 0.5439146912097931
  batch 650 loss: 0.5131787949800491
  batch 700 loss: 0.5397444874048233
  batch 750 loss: 0.557498871088028
  batch 800 loss: 0.5090008077025413
  batch 850 loss: 0.5608132815361023
  batch 900 loss: 0.5626377195119858
LOSS train 0.56264 valid 0.69310, valid PER 21.29%
EPOCH 13:
  batch 50 loss: 0.4738704949617386
  batch 100 loss: 0.4934320840239525
  batch 150 loss: 0.5080940043926239
  batch 200 loss: 0.4714167606830597
  batch 250 loss: 0.4953623414039612
  batch 300 loss: 0.5175079095363617
  batch 350 loss: 0.48767158210277556
  batch 400 loss: 0.5019341218471527
  batch 450 loss: 0.5509988510608673
  batch 500 loss: 0.5417346727848052
  batch 550 loss: 0.5541668266057969
  batch 600 loss: 0.5455089312791824
  batch 650 loss: 0.5198184013366699
  batch 700 loss: 0.5262093460559845
  batch 750 loss: 0.504047862291336
  batch 800 loss: 0.5527224266529083
  batch 850 loss: 0.5164499092102051
  batch 900 loss: 0.5478124076128006
LOSS train 0.54781 valid 0.65981, valid PER 20.28%
loss: 4.8828125e-07
EPOCH 14:
  batch 50 loss: 0.4764862963557243
  batch 100 loss: 0.4695757657289505
  batch 150 loss: 0.4802510267496109
  batch 200 loss: 0.4646495079994202
  batch 250 loss: 0.46356316477060316
  batch 300 loss: 0.4735367256402969
  batch 350 loss: 0.4784149616956711
  batch 400 loss: 0.5101982933282853
  batch 450 loss: 0.47613238126039503
  batch 500 loss: 0.4758758094906807
  batch 550 loss: 0.5016678714752197
  batch 600 loss: 0.48790834605693817
  batch 650 loss: 0.49943427681922914
  batch 700 loss: 0.5232270586490632
  batch 750 loss: 0.5093265753984452
  batch 800 loss: 0.49325003325939176
  batch 850 loss: 0.5106445640325546
  batch 900 loss: 0.5099750432372093
LOSS train 0.50998 valid 0.64678, valid PER 19.74%
loss: 2.44140625e-07
EPOCH 15:
  batch 50 loss: 0.4278534346818924
  batch 100 loss: 0.435891432762146
  batch 150 loss: 0.4546576392650604
  batch 200 loss: 0.4473810958862305
  batch 250 loss: 0.4643448168039322
  batch 300 loss: 0.4696282023191452
  batch 350 loss: 0.4659536242485046
  batch 400 loss: 0.44521901905536654
  batch 450 loss: 0.4465954053401947
  batch 500 loss: 0.44370622634887696
  batch 550 loss: 0.4943579810857773
  batch 600 loss: 0.4974446833133698
  batch 650 loss: 0.46911205410957335
  batch 700 loss: 0.45737696915864945
  batch 750 loss: 0.4856174486875534
  batch 800 loss: 0.45280398726463317
  batch 850 loss: 0.4657116723060608
  batch 900 loss: 0.4487916874885559
LOSS train 0.44879 valid 0.65744, valid PER 20.11%
EPOCH 16:
  batch 50 loss: 0.4440469378232956
  batch 100 loss: 0.40519009590148924
  batch 150 loss: 0.41025440752506254
  batch 200 loss: 0.446775444149971
  batch 250 loss: 0.43324629426002503
  batch 300 loss: 0.4259815910458565
  batch 350 loss: 0.4503167963027954
  batch 400 loss: 0.43066063821315764
  batch 450 loss: 0.45777626216411593
  batch 500 loss: 0.43994922637939454
  batch 550 loss: 0.423760929107666
  batch 600 loss: 0.4870151615142822
  batch 650 loss: 0.46174120128154755
  batch 700 loss: 0.4178739497065544
  batch 750 loss: 0.43581324130296706
  batch 800 loss: 0.4465645033121109
  batch 850 loss: 0.44072348177433013
  batch 900 loss: 0.47381048679351806
LOSS train 0.47381 valid 0.65026, valid PER 19.94%
EPOCH 17:
  batch 50 loss: 0.42471290946006773
  batch 100 loss: 0.3827742996811867
  batch 150 loss: 0.4124382585287094
  batch 200 loss: 0.3921124303340912
  batch 250 loss: 0.4027719897031784
  batch 300 loss: 0.3874812537431717
  batch 350 loss: 0.4234854692220688
  batch 400 loss: 0.4144912922382355
  batch 450 loss: 0.4034921994805336
  batch 500 loss: 0.4224481827020645
  batch 550 loss: 0.39235034614801406
  batch 600 loss: 0.44024041831493377
  batch 650 loss: 0.4171027326583862
  batch 700 loss: 0.44378174006938936
  batch 750 loss: 0.4128177279233933
  batch 800 loss: 0.4476504784822464
  batch 850 loss: 0.4193581521511078
  batch 900 loss: 0.42091203451156617
LOSS train 0.42091 valid 0.65191, valid PER 19.76%
EPOCH 18:
  batch 50 loss: 0.35645709246397017
  batch 100 loss: 0.3481926056742668
  batch 150 loss: 0.39780571788549424
  batch 200 loss: 0.38786276668310166
  batch 250 loss: 0.377263730764389
  batch 300 loss: 0.3815853503346443
  batch 350 loss: 0.39584855616092685
  batch 400 loss: 0.40374465197324755
  batch 450 loss: 0.43300995290279387
  batch 500 loss: 0.4078939568996429
  batch 550 loss: 0.4520099693536758
  batch 600 loss: 0.4139397296309471
  batch 650 loss: 0.40280464619398115
  batch 700 loss: 0.40581677079200745
  batch 750 loss: 0.4093283215165138
  batch 800 loss: 0.44554726988077165
  batch 850 loss: 0.39006112843751906
  batch 900 loss: 0.414551662504673
LOSS train 0.41455 valid 0.66145, valid PER 19.14%
[1.8461440372467042, 1.208672412633896, 1.0442590284347535, 0.9580663239955902, 0.8474670445919037, 0.7926921999454498, 0.7276778727769851, 0.672091463804245, 0.6044371765851975, 0.59544677734375, 0.5870951998233795, 0.5626377195119858, 0.5478124076128006, 0.5099750432372093, 0.4487916874885559, 0.47381048679351806, 0.42091203451156617, 0.414551662504673]
[1.7228057384490967, 1.1103198528289795, 0.9731757640838623, 0.8819860816001892, 0.8339793682098389, 0.769040048122406, 0.7399641871452332, 0.7243353128433228, 0.6862378716468811, 0.6924471855163574, 0.6979455351829529, 0.6931038498878479, 0.6598106026649475, 0.6467773914337158, 0.6574394702911377, 0.6502593755722046, 0.6519128084182739, 0.6614488959312439]
Training finished in 42.0 minutes.
Model saved to checkpoints/20230126_103538/model_14
Loading model from checkpoints/20230126_103538/model_14
SUB: 13.76%, DEL: 5.83%, INS: 2.34%, COR: 80.41%, PER: 21.92%
