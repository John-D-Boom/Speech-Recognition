Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=128, concat=1, lr=0.001, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.2, beta1=0.9, beta2=0.999)
Total number of model parameters is 562216
EPOCH 1:
  batch 50 loss: 7.14891475200653
  batch 100 loss: 3.1852953243255615
  batch 150 loss: 3.126206865310669
  batch 200 loss: 3.0373469018936157
  batch 250 loss: 2.968363347053528
  batch 300 loss: 2.8653173780441286
  batch 350 loss: 2.7348925924301146
  batch 400 loss: 2.6502358675003053
  batch 450 loss: 2.555229902267456
  batch 500 loss: 2.4077080273628235
  batch 550 loss: 2.3098432064056396
  batch 600 loss: 2.2396414566040037
  batch 650 loss: 2.132393455505371
  batch 700 loss: 2.079059064388275
  batch 750 loss: 1.9683155298233033
  batch 800 loss: 1.9304913091659546
  batch 850 loss: 1.8614372396469117
  batch 900 loss: 1.803758180141449
LOSS train 1.80376 valid 1.75168, valid PER 66.52%
EPOCH 2:
  batch 50 loss: 1.7423291850090026
  batch 100 loss: 1.7033713388442993
  batch 150 loss: 1.6209044122695924
  batch 200 loss: 1.6071104526519775
  batch 250 loss: 1.590248818397522
  batch 300 loss: 1.5489169096946716
  batch 350 loss: 1.5488935351371764
  batch 400 loss: 1.4895003032684326
  batch 450 loss: 1.4672776103019713
  batch 500 loss: 1.4576534724235535
  batch 550 loss: 1.4126004481315613
  batch 600 loss: 1.3891073155403137
  batch 650 loss: 1.3468196177482605
  batch 700 loss: 1.3647807788848878
  batch 750 loss: 1.3341304326057435
  batch 800 loss: 1.3072902989387511
  batch 850 loss: 1.2913111233711243
  batch 900 loss: 1.2631115436553955
LOSS train 1.26311 valid 1.24016, valid PER 41.81%
EPOCH 3:
  batch 50 loss: 1.1848666262626648
  batch 100 loss: 1.2612806510925294
  batch 150 loss: 1.2657812595367433
  batch 200 loss: 1.2019874370098114
  batch 250 loss: 1.193644564151764
  batch 300 loss: 1.2026523053646088
  batch 350 loss: 1.178910142183304
  batch 400 loss: 1.1700165677070617
  batch 450 loss: 1.1447140610218047
  batch 500 loss: 1.137931799888611
  batch 550 loss: 1.1676956593990326
  batch 600 loss: 1.100347797870636
  batch 650 loss: 1.1304017102718353
  batch 700 loss: 1.1006427550315856
  batch 750 loss: 1.1276579332351684
  batch 800 loss: 1.1256380438804627
  batch 850 loss: 1.0850969922542573
  batch 900 loss: 1.1251184260845184
LOSS train 1.12512 valid 1.08830, valid PER 33.72%
EPOCH 4:
  batch 50 loss: 1.0703905820846558
  batch 100 loss: 1.0193943965435028
  batch 150 loss: 1.036990134716034
  batch 200 loss: 1.0252887034416198
  batch 250 loss: 1.0489987635612488
  batch 300 loss: 1.036155811548233
  batch 350 loss: 1.0013360249996186
  batch 400 loss: 0.9749045860767365
  batch 450 loss: 0.9969766092300415
  batch 500 loss: 1.0625908708572387
  batch 550 loss: 1.0085831880569458
  batch 600 loss: 0.9830654466152191
  batch 650 loss: 1.0414040219783782
  batch 700 loss: 1.0218749237060547
  batch 750 loss: 1.0049285125732421
  batch 800 loss: 0.9876953256130219
  batch 850 loss: 0.9830844914913177
  batch 900 loss: 1.0012314474582673
LOSS train 1.00123 valid 1.03444, valid PER 31.70%
EPOCH 5:
  batch 50 loss: 0.9468390703201294
  batch 100 loss: 0.9175167274475098
  batch 150 loss: 0.9502387642860413
  batch 200 loss: 0.9841218292713165
  batch 250 loss: 0.9098048794269562
  batch 300 loss: 0.9784599757194519
  batch 350 loss: 0.936131899356842
  batch 400 loss: 0.9092374074459076
  batch 450 loss: 0.9140577924251556
  batch 500 loss: 0.8966263067722321
  batch 550 loss: 0.9492522704601288
  batch 600 loss: 0.9268624103069305
  batch 650 loss: 0.9130046987533569
  batch 700 loss: 0.9314947056770325
  batch 750 loss: 0.9337306416034699
  batch 800 loss: 0.9533732831478119
  batch 850 loss: 0.9338746976852417
  batch 900 loss: 0.907878897190094
LOSS train 0.90788 valid 0.97478, valid PER 29.96%
EPOCH 6:
  batch 50 loss: 0.8764855277538299
  batch 100 loss: 0.911217303276062
  batch 150 loss: 0.8957234299182892
  batch 200 loss: 0.8678496742248535
  batch 250 loss: 0.8620646524429322
  batch 300 loss: 0.8975942015647889
  batch 350 loss: 0.8799897634983063
  batch 400 loss: 0.8564332926273346
  batch 450 loss: 0.8834801709651947
  batch 500 loss: 0.8354534167051315
  batch 550 loss: 0.8618798875808715
  batch 600 loss: 0.8439917039871215
  batch 650 loss: 0.8216403985023498
  batch 700 loss: 0.83306875705719
  batch 750 loss: 0.8705269622802735
  batch 800 loss: 0.8711997652053833
  batch 850 loss: 0.8920904123783111
  batch 900 loss: 0.8946779042482376
LOSS train 0.89468 valid 0.91873, valid PER 28.92%
EPOCH 7:
  batch 50 loss: 0.8106697118282318
  batch 100 loss: 0.838712158203125
  batch 150 loss: 0.7975246357917786
  batch 200 loss: 0.8222486305236817
  batch 250 loss: 0.8739048409461975
  batch 300 loss: 0.822614290714264
  batch 350 loss: 0.8350616264343261
  batch 400 loss: 0.7950183415412903
  batch 450 loss: 0.814220005273819
  batch 500 loss: 0.8292604625225067
  batch 550 loss: 0.8042451751232147
  batch 600 loss: 0.8142594885826111
  batch 650 loss: 0.8060927522182465
  batch 700 loss: 0.8228449845314025
  batch 750 loss: 0.8204062986373901
  batch 800 loss: 0.8203244602680206
  batch 850 loss: 0.794149159193039
  batch 900 loss: 0.8218019139766694
LOSS train 0.82180 valid 0.88918, valid PER 27.68%
EPOCH 8:
  batch 50 loss: 0.7770162558555603
  batch 100 loss: 0.7583542859554291
  batch 150 loss: 0.7825689089298248
  batch 200 loss: 0.767836697101593
  batch 250 loss: 0.7900308358669281
  batch 300 loss: 0.7622168374061584
  batch 350 loss: 0.7722975897789002
  batch 400 loss: 0.7533723568916321
  batch 450 loss: 0.8198846554756165
  batch 500 loss: 0.7711437690258026
  batch 550 loss: 0.7682892096042633
  batch 600 loss: 0.737849999666214
  batch 650 loss: 0.7590022051334381
  batch 700 loss: 0.7843075585365296
  batch 750 loss: 0.7816420555114746
  batch 800 loss: 0.7727070283889771
  batch 850 loss: 0.7583405709266663
  batch 900 loss: 0.768314802646637
LOSS train 0.76831 valid 0.86262, valid PER 26.54%
EPOCH 9:
  batch 50 loss: 0.706553755402565
  batch 100 loss: 0.7111602282524109
  batch 150 loss: 0.715681380033493
  batch 200 loss: 0.7090610504150391
  batch 250 loss: 0.6978739112615585
  batch 300 loss: 0.7294363379478455
  batch 350 loss: 0.7285193705558777
  batch 400 loss: 0.7492443394660949
  batch 450 loss: 0.7636509376764298
  batch 500 loss: 0.7325490856170654
  batch 550 loss: 0.7216439235210419
  batch 600 loss: 0.750373512506485
  batch 650 loss: 0.7452112710475922
  batch 700 loss: 0.7217051231861115
  batch 750 loss: 0.7436633783578873
  batch 800 loss: 0.7615498864650726
  batch 850 loss: 0.7470883256196976
  batch 900 loss: 0.7077110928297042
LOSS train 0.70771 valid 0.86182, valid PER 26.43%
EPOCH 10:
  batch 50 loss: 0.6924671506881714
  batch 100 loss: 0.7244753813743592
  batch 150 loss: 0.7020147478580475
  batch 200 loss: 0.6829436641931533
  batch 250 loss: 0.6900701916217804
  batch 300 loss: 0.6885657560825348
  batch 350 loss: 0.6783805745840072
  batch 400 loss: 0.6696916300058365
  batch 450 loss: 0.6883432358503342
  batch 500 loss: 0.6694465905427933
  batch 550 loss: 0.6938971060514451
  batch 600 loss: 0.7064145529270172
  batch 650 loss: 0.7186954092979431
  batch 700 loss: 0.7238756108283997
  batch 750 loss: 0.7247032952308655
  batch 800 loss: 0.7228837651014328
  batch 850 loss: 0.6982268023490906
  batch 900 loss: 0.6987615358829499
LOSS train 0.69876 valid 0.85340, valid PER 25.70%
EPOCH 11:
  batch 50 loss: 0.6677535796165466
  batch 100 loss: 0.628694731593132
  batch 150 loss: 0.6570674914121628
  batch 200 loss: 0.6187152403593064
  batch 250 loss: 0.6577779376506805
  batch 300 loss: 0.6524809277057648
  batch 350 loss: 0.6953944051265717
  batch 400 loss: 0.6868915337324143
  batch 450 loss: 0.6562561899423599
  batch 500 loss: 0.671228021979332
  batch 550 loss: 0.6909286010265351
  batch 600 loss: 0.6756510245800018
  batch 650 loss: 0.696951402425766
  batch 700 loss: 0.7028667306900025
  batch 750 loss: 0.6811617994308472
  batch 800 loss: 0.6806111592054367
  batch 850 loss: 0.6617085814476014
  batch 900 loss: 0.6875538361072541
LOSS train 0.68755 valid 0.81601, valid PER 25.34%
EPOCH 12:
  batch 50 loss: 0.6009100651741028
  batch 100 loss: 0.5914000821113586
  batch 150 loss: 0.6147953355312348
  batch 200 loss: 0.6213450753688812
  batch 250 loss: 0.6349939608573913
  batch 300 loss: 0.6739574992656707
  batch 350 loss: 0.6236268752813339
  batch 400 loss: 0.6662039083242416
  batch 450 loss: 0.636568985581398
  batch 500 loss: 0.6535788720846176
  batch 550 loss: 0.6475789058208465
  batch 600 loss: 0.6279079675674438
  batch 650 loss: 0.625711298584938
  batch 700 loss: 0.6145316904783249
  batch 750 loss: 0.6341054832935333
  batch 800 loss: 0.6051002222299576
  batch 850 loss: 0.6358414220809937
  batch 900 loss: 0.64742506980896
LOSS train 0.64743 valid 0.82981, valid PER 24.72%
EPOCH 13:
  batch 50 loss: 0.5662610697746276
  batch 100 loss: 0.5743136358261108
  batch 150 loss: 0.6053056865930557
  batch 200 loss: 0.5658576452732086
  batch 250 loss: 0.592976320385933
  batch 300 loss: 0.6340753483772278
  batch 350 loss: 0.5932033741474152
  batch 400 loss: 0.5987829566001892
  batch 450 loss: 0.6374225431680679
  batch 500 loss: 0.6118747597932815
  batch 550 loss: 0.6389283525943756
  batch 600 loss: 0.6128782111406327
  batch 650 loss: 0.6037562322616578
  batch 700 loss: 0.6257421433925628
  batch 750 loss: 0.5842318338155746
  batch 800 loss: 0.6163317000865937
  batch 850 loss: 0.5884476584196091
  batch 900 loss: 0.6136185026168823
LOSS train 0.61362 valid 0.81556, valid PER 24.91%
EPOCH 14:
  batch 50 loss: 0.5727230554819107
  batch 100 loss: 0.5513573831319809
  batch 150 loss: 0.5600604635477066
  batch 200 loss: 0.5585684710741043
  batch 250 loss: 0.5613534706830978
  batch 300 loss: 0.5518200373649598
  batch 350 loss: 0.5847111403942108
  batch 400 loss: 0.5975279361009598
  batch 450 loss: 0.5538122338056565
  batch 500 loss: 0.5690371817350388
  batch 550 loss: 0.5927396684885025
  batch 600 loss: 0.5886757904291153
  batch 650 loss: 0.6043433851003647
  batch 700 loss: 0.6238271933794022
  batch 750 loss: 0.5927676111459732
  batch 800 loss: 0.5748098117113113
  batch 850 loss: 0.6085510677099228
  batch 900 loss: 0.5996988886594772
LOSS train 0.59970 valid 0.81506, valid PER 24.66%
EPOCH 15:
  batch 50 loss: 0.5202918672561645
  batch 100 loss: 0.5356532043218613
  batch 150 loss: 0.5625503069162369
  batch 200 loss: 0.550607277750969
  batch 250 loss: 0.593545823097229
  batch 300 loss: 0.5848262530565261
  batch 350 loss: 0.5691216206550598
  batch 400 loss: 0.575049078464508
  batch 450 loss: 0.5572864216566086
  batch 500 loss: 0.5310141098499298
  batch 550 loss: 0.5994318985939026
  batch 600 loss: 0.603846333026886
  batch 650 loss: 0.5831553226709366
  batch 700 loss: 0.5579817032814026
  batch 750 loss: 0.574837406873703
  batch 800 loss: 0.5525928699970245
  batch 850 loss: 0.5656844210624695
  batch 900 loss: 0.5241653591394424
LOSS train 0.52417 valid 0.79591, valid PER 23.70%
EPOCH 16:
  batch 50 loss: 0.505011870265007
  batch 100 loss: 0.49216806530952456
  batch 150 loss: 0.5011772352457047
  batch 200 loss: 0.5499066215753555
  batch 250 loss: 0.5302997344732284
  batch 300 loss: 0.5318391191959381
  batch 350 loss: 0.5222467267513276
  batch 400 loss: 0.5271320337057114
  batch 450 loss: 0.5492250275611877
  batch 500 loss: 0.5362829637527465
  batch 550 loss: 0.5237218856811523
  batch 600 loss: 0.5908041447401047
  batch 650 loss: 0.5665285634994507
  batch 700 loss: 0.5324676430225372
  batch 750 loss: 0.5477263635396957
  batch 800 loss: 0.5270617413520813
  batch 850 loss: 0.5404845601320267
  batch 900 loss: 0.5654090863466262
LOSS train 0.56541 valid 0.81580, valid PER 23.88%
EPOCH 17:
  batch 50 loss: 0.5096190375089645
  batch 100 loss: 0.47520006835460665
  batch 150 loss: 0.5242812651395797
  batch 200 loss: 0.47027035474777223
  batch 250 loss: 0.4977258276939392
  batch 300 loss: 0.4858721828460693
  batch 350 loss: 0.521801407635212
  batch 400 loss: 0.525460838675499
  batch 450 loss: 0.494391473531723
  batch 500 loss: 0.5288545364141464
  batch 550 loss: 0.5181756174564361
  batch 600 loss: 0.5370799744129181
  batch 650 loss: 0.5172897398471832
  batch 700 loss: 0.5307893890142441
  batch 750 loss: 0.5034070754051209
  batch 800 loss: 0.5341453582048417
  batch 850 loss: 0.5277492922544479
  batch 900 loss: 0.5132203793525696
LOSS train 0.51322 valid 0.80515, valid PER 23.90%
EPOCH 18:
  batch 50 loss: 0.46229803502559663
  batch 100 loss: 0.4449178421497345
  batch 150 loss: 0.5030513495206833
  batch 200 loss: 0.48561270713806154
  batch 250 loss: 0.4725960999727249
  batch 300 loss: 0.4863044553995132
  batch 350 loss: 0.4758741080760956
  batch 400 loss: 0.46274442672729493
  batch 450 loss: 0.4849365884065628
  batch 500 loss: 0.47702533364295957
  batch 550 loss: 0.5116988790035247
  batch 600 loss: 0.4958830922842026
  batch 650 loss: 0.4852105078101158
  batch 700 loss: 0.49214987993240356
  batch 750 loss: 0.495415580868721
  batch 800 loss: 0.5132206898927688
  batch 850 loss: 0.5027621507644653
  batch 900 loss: 0.5022522994875908
LOSS train 0.50225 valid 0.81179, valid PER 23.76%
EPOCH 19:
  batch 50 loss: 0.43374026119709014
  batch 100 loss: 0.44886270225048064
  batch 150 loss: 0.4349842929840088
  batch 200 loss: 0.4439517629146576
  batch 250 loss: 0.47000863432884216
  batch 300 loss: 0.46788626044988635
  batch 350 loss: 0.48599160730838775
  batch 400 loss: 0.4631750416755676
  batch 450 loss: 0.4282174813747406
  batch 500 loss: 0.4438964408636093
  batch 550 loss: 0.4799932289123535
  batch 600 loss: 0.4880917602777481
  batch 650 loss: 0.49522154867649076
  batch 700 loss: 0.48288762211799624
  batch 750 loss: 0.4722146052122116
  batch 800 loss: 0.4776431596279144
  batch 850 loss: 0.5202659219503403
  batch 900 loss: 0.525896605849266
LOSS train 0.52590 valid 0.85510, valid PER 24.46%
EPOCH 20:
  batch 50 loss: 0.43115248024463654
  batch 100 loss: 0.4374876397848129
  batch 150 loss: 0.4315515765547752
  batch 200 loss: 0.43231563448905946
  batch 250 loss: 0.4566050755977631
  batch 300 loss: 0.45152794659137724
  batch 350 loss: 0.42481080055236814
  batch 400 loss: 0.447281893491745
  batch 450 loss: 0.43880192160606385
  batch 500 loss: 0.47030484676361084
  batch 550 loss: 0.4965683114528656
  batch 600 loss: 0.46777325838804246
  batch 650 loss: 0.4782441061735153
  batch 700 loss: 0.45713899195194246
  batch 750 loss: 0.4453336387872696
  batch 800 loss: 0.48316581785678864
  batch 850 loss: 0.4678220388293266
  batch 900 loss: 0.46267709881067276
LOSS train 0.46268 valid 0.83523, valid PER 23.61%
[1.803758180141449, 1.2631115436553955, 1.1251184260845184, 1.0012314474582673, 0.907878897190094, 0.8946779042482376, 0.8218019139766694, 0.768314802646637, 0.7077110928297042, 0.6987615358829499, 0.6875538361072541, 0.64742506980896, 0.6136185026168823, 0.5996988886594772, 0.5241653591394424, 0.5654090863466262, 0.5132203793525696, 0.5022522994875908, 0.525896605849266, 0.46267709881067276]
[tensor(1.7517, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.2402, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.0883, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.0344, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9748, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9187, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8892, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8626, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8618, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8534, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8160, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8298, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8156, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8151, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.7959, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8158, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8052, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8118, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8551, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8352, device='cuda:0', grad_fn=<DivBackward0>)]
Training finished in 5.0 minutes.
Model saved to checkpoints/20230122_135411/model_15
Loading model from checkpoints/20230122_135411/model_15
SUB: 15.99%, DEL: 7.92%, INS: 2.39%, COR: 76.09%, PER: 26.30%
