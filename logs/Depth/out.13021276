Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=4, fbank_dims=23, model_dims=128, concat=1, lr=0.003, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.2, beta1=0.9, beta2=0.999)
Total number of model parameters is 1352744
EPOCH 1:
  batch 50 loss: 4.652362594604492
  batch 100 loss: 3.336441254615784
  batch 150 loss: 3.318823299407959
  batch 200 loss: 3.297153730392456
  batch 250 loss: 3.2600022554397583
  batch 300 loss: 3.21418333530426
  batch 350 loss: 3.1517870903015135
  batch 400 loss: 3.076194086074829
  batch 450 loss: 2.977870388031006
  batch 500 loss: 2.9108511304855345
  batch 550 loss: 2.86663565158844
  batch 600 loss: 2.8421060752868654
  batch 650 loss: 2.786070876121521
  batch 700 loss: 2.7405507564544678
  batch 750 loss: 2.664930830001831
  batch 800 loss: 2.60935188293457
  batch 850 loss: 2.520079298019409
  batch 900 loss: 2.4318402194976807
LOSS train 2.43184 valid 2.33028, valid PER 73.53%
EPOCH 2:
  batch 50 loss: 2.318665475845337
  batch 100 loss: 2.258878538608551
  batch 150 loss: 2.1114222192764283
  batch 200 loss: 2.0831863617897035
  batch 250 loss: 2.034435384273529
  batch 300 loss: 1.9415766501426697
  batch 350 loss: 1.8985150933265686
  batch 400 loss: 1.8275348234176636
  batch 450 loss: 1.785140678882599
  batch 500 loss: 1.7713256621360778
  batch 550 loss: 1.7100313258171083
  batch 600 loss: 1.6412998580932616
  batch 650 loss: 1.56858713388443
  batch 700 loss: 1.5688989853858948
  batch 750 loss: 1.549857268333435
  batch 800 loss: 1.4915871715545654
  batch 850 loss: 1.4481739759445191
  batch 900 loss: 1.4266236686706544
LOSS train 1.42662 valid 1.39987, valid PER 43.19%
EPOCH 3:
  batch 50 loss: 1.4009343099594116
  batch 100 loss: 1.439955201148987
  batch 150 loss: 1.458388500213623
  batch 200 loss: 1.367402355670929
  batch 250 loss: 1.3511489868164062
  batch 300 loss: 1.3459775853157043
  batch 350 loss: 1.3502511048316956
  batch 400 loss: 1.3063280153274537
  batch 450 loss: 1.3278506994247437
  batch 500 loss: 1.2958526372909547
  batch 550 loss: 1.318558659553528
  batch 600 loss: 1.2115805888175963
  batch 650 loss: 1.246498512029648
  batch 700 loss: 1.2607306945323944
  batch 750 loss: 1.2660094356536866
  batch 800 loss: 1.2885756397247314
  batch 850 loss: 1.245101305246353
  batch 900 loss: 1.2461809706687927
LOSS train 1.24618 valid 1.21249, valid PER 37.25%
EPOCH 4:
  batch 50 loss: 1.2376742160320282
  batch 100 loss: 1.1726723504066467
  batch 150 loss: 1.1851180124282836
  batch 200 loss: 1.1850995671749116
  batch 250 loss: 1.18918679356575
  batch 300 loss: 1.1914428567886353
  batch 350 loss: 1.151486451625824
  batch 400 loss: 1.1317331409454345
  batch 450 loss: 1.1203706347942353
  batch 500 loss: 1.209456617832184
  batch 550 loss: 1.1081305706501008
  batch 600 loss: 1.107346614599228
  batch 650 loss: 1.1564838314056396
  batch 700 loss: 1.1717655658721924
  batch 750 loss: 1.1118326532840728
  batch 800 loss: 1.115214354991913
  batch 850 loss: 1.1205089962482453
  batch 900 loss: 1.11144562125206
LOSS train 1.11145 valid 1.11485, valid PER 34.47%
EPOCH 5:
  batch 50 loss: 1.0924057459831238
  batch 100 loss: 1.0456206679344178
  batch 150 loss: 1.0925812900066376
  batch 200 loss: 1.128104178905487
  batch 250 loss: 1.0726560819149018
  batch 300 loss: 1.1062438917160033
  batch 350 loss: 1.0483648300170898
  batch 400 loss: 1.0502014553546906
  batch 450 loss: 1.0502110195159913
  batch 500 loss: 1.029299556016922
  batch 550 loss: 1.0739263582229615
  batch 600 loss: 1.0724526035785675
  batch 650 loss: 1.0560669147968291
  batch 700 loss: 1.0465745973587035
  batch 750 loss: 1.0325437545776368
  batch 800 loss: 1.0925790071487427
  batch 850 loss: 1.0826108169555664
  batch 900 loss: 1.061658673286438
LOSS train 1.06166 valid 1.13906, valid PER 34.98%
EPOCH 6:
  batch 50 loss: 1.041780560016632
  batch 100 loss: 1.0453290796279908
  batch 150 loss: 1.0359728288650514
  batch 200 loss: 0.9775659096240997
  batch 250 loss: 0.9984572207927704
  batch 300 loss: 1.0294033312797546
  batch 350 loss: 1.053337116241455
  batch 400 loss: 1.0238063025474549
  batch 450 loss: 1.0420277523994446
  batch 500 loss: 1.0065205085277558
  batch 550 loss: 0.9962623083591461
  batch 600 loss: 0.9775594687461853
  batch 650 loss: 0.9545293152332306
  batch 700 loss: 0.9560120463371277
  batch 750 loss: 1.0347748970985413
  batch 800 loss: 1.008200991153717
  batch 850 loss: 1.0277953910827637
  batch 900 loss: 1.018156281709671
LOSS train 1.01816 valid 0.97202, valid PER 30.01%
EPOCH 7:
  batch 50 loss: 0.9533712863922119
  batch 100 loss: 1.0182968163490296
  batch 150 loss: 0.9339071655273438
  batch 200 loss: 0.9295320725440979
  batch 250 loss: 1.0055757248401642
  batch 300 loss: 0.9500840592384339
  batch 350 loss: 0.9907015502452851
  batch 400 loss: 0.9344653356075286
  batch 450 loss: 0.9435495352745056
  batch 500 loss: 0.9255223953723908
  batch 550 loss: 0.9302491068840026
  batch 600 loss: 0.9291523563861847
  batch 650 loss: 0.9130280697345734
  batch 700 loss: 0.9610540127754211
  batch 750 loss: 0.9432152724266052
  batch 800 loss: 0.9389143967628479
  batch 850 loss: 0.9366356694698333
  batch 900 loss: 0.9401562285423278
LOSS train 0.94016 valid 0.97226, valid PER 30.36%
EPOCH 8:
  batch 50 loss: 0.9187141740322113
  batch 100 loss: 0.8823801159858704
  batch 150 loss: 0.9305264985561371
  batch 200 loss: 0.8992873501777648
  batch 250 loss: 0.9038813889026642
  batch 300 loss: 0.8982678425312042
  batch 350 loss: 0.9012667739391327
  batch 400 loss: 0.8761312413215637
  batch 450 loss: 0.9785196089744568
  batch 500 loss: 0.9272497642040253
  batch 550 loss: 0.9049527513980865
  batch 600 loss: 0.8940586841106415
  batch 650 loss: 0.931614272594452
  batch 700 loss: 0.926147803068161
  batch 750 loss: 0.9171422410011292
  batch 800 loss: 0.9427527439594269
  batch 850 loss: 0.9032653951644898
  batch 900 loss: 0.9110933899879455
LOSS train 0.91109 valid 0.91002, valid PER 28.49%
EPOCH 9:
  batch 50 loss: 0.8636321544647216
  batch 100 loss: 0.844067713022232
  batch 150 loss: 0.869225583076477
  batch 200 loss: 0.870117861032486
  batch 250 loss: 0.8250230407714844
  batch 300 loss: 0.8827861273288726
  batch 350 loss: 0.8815081262588501
  batch 400 loss: 0.9062001419067383
  batch 450 loss: 0.9093399274349213
  batch 500 loss: 0.8570476734638214
  batch 550 loss: 0.8664893233776092
  batch 600 loss: 0.9137203419208526
  batch 650 loss: 0.8936652255058288
  batch 700 loss: 0.8548071432113648
  batch 750 loss: 0.8606349599361419
  batch 800 loss: 0.9143459701538086
  batch 850 loss: 0.8912317180633544
  batch 900 loss: 0.8586655712127685
LOSS train 0.85867 valid 0.88452, valid PER 28.12%
EPOCH 10:
  batch 50 loss: 0.836025493144989
  batch 100 loss: 0.8567436861991883
  batch 150 loss: 0.8772941505908967
  batch 200 loss: 0.8444912898540496
  batch 250 loss: 0.8208682370185852
  batch 300 loss: 0.8076387333869934
  batch 350 loss: 0.8173629105091095
  batch 400 loss: 0.803097870349884
  batch 450 loss: 0.8506961572170257
  batch 500 loss: 0.8337098693847657
  batch 550 loss: 0.8514319932460785
  batch 600 loss: 0.8430773377418518
  batch 650 loss: 0.8568693137168885
  batch 700 loss: 0.8556525433063507
  batch 750 loss: 0.8852917659282684
  batch 800 loss: 0.8746213006973267
  batch 850 loss: 0.8554399609565735
  batch 900 loss: 0.8298494935035705
LOSS train 0.82985 valid 0.92093, valid PER 28.22%
EPOCH 11:
  batch 50 loss: 0.8347572410106658
  batch 100 loss: 0.7932843011617661
  batch 150 loss: 0.8457894992828369
  batch 200 loss: 0.8073820233345032
  batch 250 loss: 0.8142716896533966
  batch 300 loss: 0.824065283536911
  batch 350 loss: 0.8984089076519013
  batch 400 loss: 0.8105245494842529
  batch 450 loss: 0.8092722570896149
  batch 500 loss: 0.8117028033733368
  batch 550 loss: 0.83304922580719
  batch 600 loss: 0.8079857420921326
  batch 650 loss: 0.8109463357925415
  batch 700 loss: 0.8818908739089966
  batch 750 loss: 0.7934425783157348
  batch 800 loss: 0.8229184854030609
  batch 850 loss: 0.8056708121299744
  batch 900 loss: 0.8438389921188354
LOSS train 0.84384 valid 0.85850, valid PER 26.98%
EPOCH 12:
  batch 50 loss: 0.7483087432384491
  batch 100 loss: 0.7670262551307678
  batch 150 loss: 0.7962582290172577
  batch 200 loss: 0.7849906134605408
  batch 250 loss: 0.8013469779491424
  batch 300 loss: 0.8074935412406922
  batch 350 loss: 0.780521125793457
  batch 400 loss: 0.8300385975837707
  batch 450 loss: 0.7939665651321411
  batch 500 loss: 0.810635142326355
  batch 550 loss: 0.815440029501915
  batch 600 loss: 0.7857378363609314
  batch 650 loss: 0.767377700805664
  batch 700 loss: 0.8024207627773285
  batch 750 loss: 0.8225264465808868
  batch 800 loss: 0.7709965562820434
  batch 850 loss: 0.7873210275173187
  batch 900 loss: 0.8047505855560303
LOSS train 0.80475 valid 0.87583, valid PER 27.05%
EPOCH 13:
  batch 50 loss: 0.7632379734516144
  batch 100 loss: 0.7655291503667832
  batch 150 loss: 0.7622159123420715
  batch 200 loss: 0.7327724021673202
  batch 250 loss: 0.7521216285228729
  batch 300 loss: 0.7864634323120118
  batch 350 loss: 0.7711030602455139
  batch 400 loss: 0.7650304555892944
  batch 450 loss: 0.7870415389537812
  batch 500 loss: 0.7778420424461365
  batch 550 loss: 0.8270240384340286
  batch 600 loss: 0.7940840268135071
  batch 650 loss: 0.7874156832695007
  batch 700 loss: 0.788958306312561
  batch 750 loss: 0.7482171845436096
  batch 800 loss: 0.7537328749895096
  batch 850 loss: 0.748727662563324
  batch 900 loss: 0.7691499960422515
LOSS train 0.76915 valid 0.83146, valid PER 25.77%
EPOCH 14:
  batch 50 loss: 0.7399687743186951
  batch 100 loss: 0.7256373119354248
  batch 150 loss: 0.7629779934883117
  batch 200 loss: 0.7847294318675995
  batch 250 loss: 0.7970884370803833
  batch 300 loss: 0.768023933172226
  batch 350 loss: 0.7552475345134735
  batch 400 loss: 0.7981777572631836
  batch 450 loss: 0.7387210506200791
  batch 500 loss: 0.761486228108406
  batch 550 loss: 0.7738555890321731
  batch 600 loss: 0.7510938370227813
  batch 650 loss: 0.7631782746315002
  batch 700 loss: 0.7781735038757325
  batch 750 loss: 0.7752182877063751
  batch 800 loss: 0.7510408365726471
  batch 850 loss: 0.799537787437439
  batch 900 loss: 0.7735809075832367
LOSS train 0.77358 valid 0.84245, valid PER 25.52%
EPOCH 15:
  batch 50 loss: 0.7155361330509186
  batch 100 loss: 0.7602710390090942
  batch 150 loss: 0.7205690449476242
  batch 200 loss: 0.7536697381734848
  batch 250 loss: 0.779763309955597
  batch 300 loss: 0.7394642186164856
  batch 350 loss: 0.7339368999004364
  batch 400 loss: 0.7415977162122727
  batch 450 loss: 0.7271514290571213
  batch 500 loss: 0.7433565735816956
  batch 550 loss: 0.7599639046192169
  batch 600 loss: 0.7701971673965454
  batch 650 loss: 0.7394965350627899
  batch 700 loss: 0.704930745959282
  batch 750 loss: 0.7656845152378082
  batch 800 loss: 0.750441986322403
  batch 850 loss: 0.738049418926239
  batch 900 loss: 0.7174585562944412
LOSS train 0.71746 valid 0.85527, valid PER 26.95%
EPOCH 16:
  batch 50 loss: 0.7251078486442566
  batch 100 loss: 0.7094027960300445
  batch 150 loss: 0.7249228775501251
  batch 200 loss: 0.7529978823661804
  batch 250 loss: 0.7192278289794922
  batch 300 loss: 0.7255631977319718
  batch 350 loss: 0.7307384330034256
  batch 400 loss: 0.7243951362371445
  batch 450 loss: 0.7528114062547684
  batch 500 loss: 0.7124689650535584
  batch 550 loss: 0.7018336647748947
  batch 600 loss: 0.7397659718990326
  batch 650 loss: 0.7471346634626389
  batch 700 loss: 0.7133359515666962
  batch 750 loss: 0.7226581466197968
  batch 800 loss: 0.7039020580053329
  batch 850 loss: 0.7187724030017852
  batch 900 loss: 0.7248692536354064
LOSS train 0.72487 valid 0.82078, valid PER 24.46%
EPOCH 17:
  batch 50 loss: 0.7164031851291657
  batch 100 loss: 0.65876120865345
  batch 150 loss: 0.7252037346363067
  batch 200 loss: 0.673993690609932
  batch 250 loss: 0.6932784056663513
  batch 300 loss: 0.6775717568397522
  batch 350 loss: 0.70696622133255
  batch 400 loss: 0.7402413928508759
  batch 450 loss: 0.7195429641008377
  batch 500 loss: 0.7191394340991973
  batch 550 loss: 0.7148835968971252
  batch 600 loss: 0.7552631634473801
  batch 650 loss: 0.6955803364515305
  batch 700 loss: 0.7330998170375824
  batch 750 loss: 0.686976724267006
  batch 800 loss: 0.7229544526338577
  batch 850 loss: 0.7139768761396408
  batch 900 loss: 0.71643914103508
LOSS train 0.71644 valid 0.78788, valid PER 24.36%
EPOCH 18:
  batch 50 loss: 0.66579285800457
  batch 100 loss: 0.6631798964738845
  batch 150 loss: 0.73883061170578
  batch 200 loss: 0.6935563385486603
  batch 250 loss: 0.6560460340976715
  batch 300 loss: 0.669334083199501
  batch 350 loss: 0.6620608013868332
  batch 400 loss: 0.6668000292778015
  batch 450 loss: 0.7279514867067337
  batch 500 loss: 0.7002836638689041
  batch 550 loss: 0.7377759873867035
  batch 600 loss: 0.7196785032749176
  batch 650 loss: 0.7010391831398011
  batch 700 loss: 0.7207444155216217
  batch 750 loss: 0.7002305555343628
  batch 800 loss: 0.7564279520511628
  batch 850 loss: 0.7288690054416657
  batch 900 loss: 0.723040167093277
LOSS train 0.72304 valid 0.80880, valid PER 25.34%
EPOCH 19:
  batch 50 loss: 0.6958105856180191
  batch 100 loss: 0.7000556349754333
  batch 150 loss: 0.6559148609638215
  batch 200 loss: 0.6418887728452682
  batch 250 loss: 0.6865506190061569
  batch 300 loss: 0.6880118834972382
  batch 350 loss: 0.685445374250412
  batch 400 loss: 0.679976692199707
  batch 450 loss: 0.6764372879266739
  batch 500 loss: 0.6717859429121017
  batch 550 loss: 0.7392699384689331
  batch 600 loss: 0.7533990496397018
  batch 650 loss: 0.7507381045818329
  batch 700 loss: 0.7976644265651703
  batch 750 loss: 0.743454424738884
  batch 800 loss: 0.6977184778451919
  batch 850 loss: 0.7064556473493576
  batch 900 loss: 0.7031250369548797
LOSS train 0.70313 valid 0.81260, valid PER 24.62%
EPOCH 20:
  batch 50 loss: 0.6279555207490921
  batch 100 loss: 0.6558125650882721
  batch 150 loss: 0.658684800863266
  batch 200 loss: 0.6692728745937347
  batch 250 loss: 0.6795976412296295
  batch 300 loss: 0.6955220752954483
  batch 350 loss: 0.6506159996986389
  batch 400 loss: 0.6750861841440201
  batch 450 loss: 0.6504414278268814
  batch 500 loss: 0.7037439215183258
  batch 550 loss: 0.6972200393676757
  batch 600 loss: 0.6627933454513549
  batch 650 loss: 0.6765097188949585
  batch 700 loss: 0.6637716281414032
  batch 750 loss: 0.6514239871501922
  batch 800 loss: 0.6828121674060822
  batch 850 loss: 0.6971583187580108
  batch 900 loss: 0.6582268404960633
LOSS train 0.65823 valid 0.79736, valid PER 24.62%
[2.4318402194976807, 1.4266236686706544, 1.2461809706687927, 1.11144562125206, 1.061658673286438, 1.018156281709671, 0.9401562285423278, 0.9110933899879455, 0.8586655712127685, 0.8298494935035705, 0.8438389921188354, 0.8047505855560303, 0.7691499960422515, 0.7735809075832367, 0.7174585562944412, 0.7248692536354064, 0.71643914103508, 0.723040167093277, 0.7031250369548797, 0.6582268404960633]
[tensor(2.3303, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.3999, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.2125, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.1148, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.1391, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9720, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9723, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9100, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8845, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9209, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8585, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8758, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8315, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8425, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8553, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8208, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.7879, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8088, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8126, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.7974, device='cuda:0', grad_fn=<DivBackward0>)]
Training finished in 9.0 minutes.
Model saved to checkpoints/20230122_144422/model_17
Loading model from checkpoints/20230122_144422/model_17
SUB: 15.96%, DEL: 6.74%, INS: 2.34%, COR: 77.30%, PER: 25.04%
