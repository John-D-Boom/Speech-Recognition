Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=128, concat=1, lr=0.002, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.2, beta1=0.9, beta2=0.999)
Total number of model parameters is 562216
EPOCH 1:
  batch 50 loss: 5.65109094619751
  batch 100 loss: 3.222267599105835
  batch 150 loss: 3.1028766345977785
  batch 200 loss: 2.908786096572876
  batch 250 loss: 2.670080637931824
  batch 300 loss: 2.45729971408844
  batch 350 loss: 2.277995252609253
  batch 400 loss: 2.1748434615135195
  batch 450 loss: 2.040362527370453
  batch 500 loss: 1.9157986092567443
  batch 550 loss: 1.8115906739234924
  batch 600 loss: 1.7492639374732972
  batch 650 loss: 1.6494854736328124
  batch 700 loss: 1.6555583333969117
  batch 750 loss: 1.5796244883537292
  batch 800 loss: 1.5544532823562622
  batch 850 loss: 1.4911670112609863
  batch 900 loss: 1.4703930616378784
LOSS train 1.47039 valid 1.46002, valid PER 50.89%
EPOCH 2:
  batch 50 loss: 1.4295213150978088
  batch 100 loss: 1.3860755777359008
  batch 150 loss: 1.316526415348053
  batch 200 loss: 1.2993498158454895
  batch 250 loss: 1.3281337666511535
  batch 300 loss: 1.263606517314911
  batch 350 loss: 1.2772172486782074
  batch 400 loss: 1.2445824587345122
  batch 450 loss: 1.2150858318805695
  batch 500 loss: 1.2266555559635162
  batch 550 loss: 1.1819425058364867
  batch 600 loss: 1.189116690158844
  batch 650 loss: 1.1435595285892486
  batch 700 loss: 1.1784961676597596
  batch 750 loss: 1.1638012158870696
  batch 800 loss: 1.130588666200638
  batch 850 loss: 1.1389039504528045
  batch 900 loss: 1.1117666256427765
LOSS train 1.11177 valid 1.08376, valid PER 34.73%
EPOCH 3:
  batch 50 loss: 1.0267019605636596
  batch 100 loss: 1.0712372279167175
  batch 150 loss: 1.0983870553970336
  batch 200 loss: 1.0445835673809052
  batch 250 loss: 1.0256655848026275
  batch 300 loss: 1.0569182527065277
  batch 350 loss: 1.04619393825531
  batch 400 loss: 1.0307554924488067
  batch 450 loss: 1.0269432747364045
  batch 500 loss: 1.0064630377292634
  batch 550 loss: 1.0212120175361634
  batch 600 loss: 0.9702859055995942
  batch 650 loss: 0.9885876524448395
  batch 700 loss: 0.9976647555828094
  batch 750 loss: 0.9916528272628784
  batch 800 loss: 1.03123198390007
  batch 850 loss: 0.9950222539901733
  batch 900 loss: 0.997502783536911
LOSS train 0.99750 valid 1.03637, valid PER 32.44%
EPOCH 4:
  batch 50 loss: 0.9878219258785248
  batch 100 loss: 0.9177998292446137
  batch 150 loss: 0.9416439473628998
  batch 200 loss: 0.9314011383056641
  batch 250 loss: 0.9688072073459625
  batch 300 loss: 0.9490810453891754
  batch 350 loss: 0.9292505812644959
  batch 400 loss: 0.8911095952987671
  batch 450 loss: 0.9189016544818878
  batch 500 loss: 0.9931226277351379
  batch 550 loss: 0.913215103149414
  batch 600 loss: 0.8948623299598694
  batch 650 loss: 0.9241571044921875
  batch 700 loss: 0.9446360826492309
  batch 750 loss: 0.9629108953475952
  batch 800 loss: 0.9362878000736237
  batch 850 loss: 0.9168658220767975
  batch 900 loss: 0.9019613242149354
LOSS train 0.90196 valid 0.94412, valid PER 28.89%
EPOCH 5:
  batch 50 loss: 0.8652439343929291
  batch 100 loss: 0.8436114108562469
  batch 150 loss: 0.8483968508243561
  batch 200 loss: 0.8802408993244171
  batch 250 loss: 0.8309547626972198
  batch 300 loss: 0.8797472763061523
  batch 350 loss: 0.8440500998497009
  batch 400 loss: 0.8538848030567169
  batch 450 loss: 0.9037771999835968
  batch 500 loss: 0.8137093245983124
  batch 550 loss: 0.884474526643753
  batch 600 loss: 0.8570142722129822
  batch 650 loss: 0.8752892065048218
  batch 700 loss: 0.8517255765199662
  batch 750 loss: 0.845414354801178
  batch 800 loss: 0.8914651048183441
  batch 850 loss: 0.9004667818546295
  batch 900 loss: 0.849603853225708
LOSS train 0.84960 valid 0.88322, valid PER 27.72%
EPOCH 6:
  batch 50 loss: 0.7870623886585235
  batch 100 loss: 0.8094886744022369
  batch 150 loss: 0.7993956542015076
  batch 200 loss: 0.7853123903274536
  batch 250 loss: 0.7851445198059082
  batch 300 loss: 0.8431315207481385
  batch 350 loss: 0.8162919902801513
  batch 400 loss: 0.7955541825294494
  batch 450 loss: 0.8356980049610138
  batch 500 loss: 0.7667043030261993
  batch 550 loss: 0.7974152660369873
  batch 600 loss: 0.7833673071861267
  batch 650 loss: 0.7936595177650452
  batch 700 loss: 0.7834770888090133
  batch 750 loss: 0.8164123213291168
  batch 800 loss: 0.8077877765893936
  batch 850 loss: 0.8334274429082871
  batch 900 loss: 0.83561572432518
LOSS train 0.83562 valid 0.87818, valid PER 27.24%
EPOCH 7:
  batch 50 loss: 0.7713879311084747
  batch 100 loss: 0.8089495289325714
  batch 150 loss: 0.7320745325088501
  batch 200 loss: 0.7475898122787475
  batch 250 loss: 0.7888224399089814
  batch 300 loss: 0.7557643735408783
  batch 350 loss: 0.7691697680950165
  batch 400 loss: 0.7390451276302338
  batch 450 loss: 0.7434143328666687
  batch 500 loss: 0.7526352846622467
  batch 550 loss: 0.772687919139862
  batch 600 loss: 0.7469728326797486
  batch 650 loss: 0.753431544303894
  batch 700 loss: 0.7807612574100494
  batch 750 loss: 0.7692441868782044
  batch 800 loss: 0.7581793588399887
  batch 850 loss: 0.7476546734571456
  batch 900 loss: 0.7481689214706421
LOSS train 0.74817 valid 0.85795, valid PER 26.75%
EPOCH 8:
  batch 50 loss: 0.7191131299734116
  batch 100 loss: 0.6793031734228134
  batch 150 loss: 0.7043892753124237
  batch 200 loss: 0.7119226759672165
  batch 250 loss: 0.7091449165344238
  batch 300 loss: 0.7215478926897049
  batch 350 loss: 0.7209475743770599
  batch 400 loss: 0.7113687074184418
  batch 450 loss: 0.7830097186565399
  batch 500 loss: 0.7657093679904938
  batch 550 loss: 0.7628838628530502
  batch 600 loss: 0.7280280184745789
  batch 650 loss: 0.741944328546524
  batch 700 loss: 0.761848783493042
  batch 750 loss: 0.731097571849823
  batch 800 loss: 0.7554826140403748
  batch 850 loss: 0.738824474811554
  batch 900 loss: 0.7340617233514786
LOSS train 0.73406 valid 0.85259, valid PER 26.45%
EPOCH 9:
  batch 50 loss: 0.6872019124031067
  batch 100 loss: 0.6616795831918716
  batch 150 loss: 0.6944477236270905
  batch 200 loss: 0.6885443234443664
  batch 250 loss: 0.6524318867921829
  batch 300 loss: 0.6958744525909424
  batch 350 loss: 0.6819516515731812
  batch 400 loss: 0.713451823592186
  batch 450 loss: 0.7024564290046692
  batch 500 loss: 0.678453598022461
  batch 550 loss: 0.6644849491119384
  batch 600 loss: 0.694799228310585
  batch 650 loss: 0.7026805192232132
  batch 700 loss: 0.6900493890047074
  batch 750 loss: 0.685683628320694
  batch 800 loss: 0.6993439626693726
  batch 850 loss: 0.7129655170440674
  batch 900 loss: 0.6704359924793244
LOSS train 0.67044 valid 0.82271, valid PER 24.83%
EPOCH 10:
  batch 50 loss: 0.6335114592313766
  batch 100 loss: 0.6418161904811859
  batch 150 loss: 0.6617223745584488
  batch 200 loss: 0.6468547481298447
  batch 250 loss: 0.6477312582731247
  batch 300 loss: 0.6516796523332595
  batch 350 loss: 0.6486550158262253
  batch 400 loss: 0.633706858754158
  batch 450 loss: 0.6376294589042664
  batch 500 loss: 0.66063794195652
  batch 550 loss: 0.6779704594612121
  batch 600 loss: 0.6634528195858002
  batch 650 loss: 0.696995131969452
  batch 700 loss: 0.6831191730499268
  batch 750 loss: 0.7341637814044952
  batch 800 loss: 0.7194813323020935
  batch 850 loss: 0.6742417925596237
  batch 900 loss: 0.6669968414306641
LOSS train 0.66700 valid 0.83077, valid PER 24.72%
EPOCH 11:
  batch 50 loss: 0.636124330163002
  batch 100 loss: 0.6027466410398483
  batch 150 loss: 0.6324961405992507
  batch 200 loss: 0.6018993359804153
  batch 250 loss: 0.6260615938901901
  batch 300 loss: 0.5990129005908966
  batch 350 loss: 0.6458732426166535
  batch 400 loss: 0.6303804767131805
  batch 450 loss: 0.6161627304553986
  batch 500 loss: 0.6275880926847458
  batch 550 loss: 0.6588616335391998
  batch 600 loss: 0.630427542924881
  batch 650 loss: 0.6481419706344604
  batch 700 loss: 0.6956600177288056
  batch 750 loss: 0.653574424982071
  batch 800 loss: 0.66920376598835
  batch 850 loss: 0.6511587691307068
  batch 900 loss: 0.6717359954118729
LOSS train 0.67174 valid 0.82130, valid PER 24.94%
EPOCH 12:
  batch 50 loss: 0.5763633453845978
  batch 100 loss: 0.5734877693653107
  batch 150 loss: 0.5940054178237915
  batch 200 loss: 0.614198021888733
  batch 250 loss: 0.6271508479118347
  batch 300 loss: 0.6255896538496017
  batch 350 loss: 0.6060512208938599
  batch 400 loss: 0.6732259976863861
  batch 450 loss: 0.6109037238359452
  batch 500 loss: 0.6384232145547867
  batch 550 loss: 0.6343757140636445
  batch 600 loss: 0.6542687857151032
  batch 650 loss: 0.650530007481575
  batch 700 loss: 0.6445832830667496
  batch 750 loss: 0.6569210261106491
  batch 800 loss: 0.6059802502393723
  batch 850 loss: 0.6441697371006012
  batch 900 loss: 0.6687304013967514
LOSS train 0.66873 valid 0.81854, valid PER 24.41%
EPOCH 13:
  batch 50 loss: 0.5807198989391327
  batch 100 loss: 0.5875483626127243
  batch 150 loss: 0.59353187084198
  batch 200 loss: 0.576538832783699
  batch 250 loss: 0.5858985674381256
  batch 300 loss: 0.6125618922710419
  batch 350 loss: 0.5788670617341996
  batch 400 loss: 0.5620295453071594
  batch 450 loss: 0.5856523567438126
  batch 500 loss: 0.5868856859207153
  batch 550 loss: 0.633884083032608
  batch 600 loss: 0.5985716104507446
  batch 650 loss: 0.5990869212150574
  batch 700 loss: 0.6155781745910645
  batch 750 loss: 0.5863883066177368
  batch 800 loss: 0.6190588682889938
  batch 850 loss: 0.5842555904388428
  batch 900 loss: 0.5966555947065353
LOSS train 0.59666 valid 0.82080, valid PER 24.57%
EPOCH 14:
  batch 50 loss: 0.5576224309206009
  batch 100 loss: 0.5383713608980178
  batch 150 loss: 0.556663236618042
  batch 200 loss: 0.5591387683153153
  batch 250 loss: 0.5511781024932861
  batch 300 loss: 0.5365894657373428
  batch 350 loss: 0.5537540847063065
  batch 400 loss: 0.6018716818094254
  batch 450 loss: 0.5646566194295883
  batch 500 loss: 0.5568069839477539
  batch 550 loss: 0.5661699956655503
  batch 600 loss: 0.5596506178379059
  batch 650 loss: 0.5715510469675064
  batch 700 loss: 0.5930362552404403
  batch 750 loss: 0.5813309699296951
  batch 800 loss: 0.558507667183876
  batch 850 loss: 0.6081653541326523
  batch 900 loss: 0.5843043768405914
LOSS train 0.58430 valid 0.81146, valid PER 24.31%
EPOCH 15:
  batch 50 loss: 0.49543882846832277
  batch 100 loss: 0.5122989511489868
  batch 150 loss: 0.519988431930542
  batch 200 loss: 0.5469434064626694
  batch 250 loss: 0.5564873218536377
  batch 300 loss: 0.5381912964582444
  batch 350 loss: 0.5265979623794556
  batch 400 loss: 0.5290827858448028
  batch 450 loss: 0.5349236464500428
  batch 500 loss: 0.5170854568481446
  batch 550 loss: 0.5683214068412781
  batch 600 loss: 0.5812073618173599
  batch 650 loss: 0.5339119350910186
  batch 700 loss: 0.5350728458166123
  batch 750 loss: 0.566412193775177
  batch 800 loss: 0.5480214303731918
  batch 850 loss: 0.5368190485239029
  batch 900 loss: 0.5336844128370285
LOSS train 0.53368 valid 0.79249, valid PER 23.33%
EPOCH 16:
  batch 50 loss: 0.5000553619861603
  batch 100 loss: 0.4801550304889679
  batch 150 loss: 0.4852178817987442
  batch 200 loss: 0.5167935013771057
  batch 250 loss: 0.514582302570343
  batch 300 loss: 0.531845018863678
  batch 350 loss: 0.531679419875145
  batch 400 loss: 0.5257600057125091
  batch 450 loss: 0.5362394630908967
  batch 500 loss: 0.5139369142055511
  batch 550 loss: 0.4970416170358658
  batch 600 loss: 0.5512248438596725
  batch 650 loss: 0.5543187648057938
  batch 700 loss: 0.510216036438942
  batch 750 loss: 0.5344467920064926
  batch 800 loss: 0.519708257317543
  batch 850 loss: 0.537257406115532
  batch 900 loss: 0.5642182946205139
LOSS train 0.56422 valid 0.80177, valid PER 23.76%
EPOCH 17:
  batch 50 loss: 0.5461984014511109
  batch 100 loss: 0.4840166801214218
  batch 150 loss: 0.5193212509155274
  batch 200 loss: 0.4776127219200134
  batch 250 loss: 0.5046090990304947
  batch 300 loss: 0.4962860590219498
  batch 350 loss: 0.5080163139104843
  batch 400 loss: 0.5182415843009949
  batch 450 loss: 0.5033331316709518
  batch 500 loss: 0.5067805951833725
  batch 550 loss: 0.5236853748559952
  batch 600 loss: 0.5378316575288773
  batch 650 loss: 0.5091226458549499
  batch 700 loss: 0.5742801392078399
  batch 750 loss: 0.5190587246417999
  batch 800 loss: 0.5483992046117783
  batch 850 loss: 0.5336014246940612
  batch 900 loss: 0.5024449342489242
LOSS train 0.50244 valid 0.80744, valid PER 23.58%
EPOCH 18:
  batch 50 loss: 0.43646991848945615
  batch 100 loss: 0.43120059609413147
  batch 150 loss: 0.48622592389583585
  batch 200 loss: 0.4530538576841354
  batch 250 loss: 0.45288612842559817
  batch 300 loss: 0.46246678829193116
  batch 350 loss: 0.45940826773643495
  batch 400 loss: 0.48883386254310607
  batch 450 loss: 0.49653484761714933
  batch 500 loss: 0.50060083091259
  batch 550 loss: 0.5349712294340133
  batch 600 loss: 0.5082414865493774
  batch 650 loss: 0.5061220723390579
  batch 700 loss: 0.4926907134056091
  batch 750 loss: 0.49095662057399747
  batch 800 loss: 0.5266319400072098
  batch 850 loss: 0.5124572110176087
  batch 900 loss: 0.5171195542812348
LOSS train 0.51712 valid 0.80052, valid PER 23.83%
EPOCH 19:
  batch 50 loss: 0.4312182828783989
  batch 100 loss: 0.42715224385261535
  batch 150 loss: 0.4178300613164902
  batch 200 loss: 0.4219516611099243
  batch 250 loss: 0.44495242595672607
  batch 300 loss: 0.47907407343387604
  batch 350 loss: 0.48279629468917845
  batch 400 loss: 0.4565617924928665
  batch 450 loss: 0.4452513122558594
  batch 500 loss: 0.44297992944717407
  batch 550 loss: 0.48501745641231536
  batch 600 loss: 0.4948347505927086
  batch 650 loss: 0.5022654592990875
  batch 700 loss: 0.4993817335367203
  batch 750 loss: 0.47248350858688354
  batch 800 loss: 0.47526046574115755
  batch 850 loss: 0.5023041462898254
  batch 900 loss: 0.49849814653396607
LOSS train 0.49850 valid 0.83776, valid PER 24.30%
EPOCH 20:
  batch 50 loss: 0.42637718439102174
  batch 100 loss: 0.4415220904350281
  batch 150 loss: 0.43219056248664856
  batch 200 loss: 0.45471752524375914
  batch 250 loss: 0.4692363739013672
  batch 300 loss: 0.4707183885574341
  batch 350 loss: 0.43282535880804063
  batch 400 loss: 0.46020634651184084
  batch 450 loss: 0.44273573338985445
  batch 500 loss: 0.44602652013301847
  batch 550 loss: 0.4747418165206909
  batch 600 loss: 0.4616894656419754
  batch 650 loss: 0.47019681572914124
  batch 700 loss: 0.47678791761398315
  batch 750 loss: 0.4781890434026718
  batch 800 loss: 0.4924276286363602
  batch 850 loss: 0.4667377269268036
  batch 900 loss: 0.4528299206495285
LOSS train 0.45283 valid 0.82499, valid PER 23.31%
[1.4703930616378784, 1.1117666256427765, 0.997502783536911, 0.9019613242149354, 0.849603853225708, 0.83561572432518, 0.7481689214706421, 0.7340617233514786, 0.6704359924793244, 0.6669968414306641, 0.6717359954118729, 0.6687304013967514, 0.5966555947065353, 0.5843043768405914, 0.5336844128370285, 0.5642182946205139, 0.5024449342489242, 0.5171195542812348, 0.49849814653396607, 0.4528299206495285]
[tensor(1.4600, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.0838, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.0364, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9441, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8832, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8782, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8579, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8526, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8227, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8308, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8213, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8185, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8208, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8115, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.7925, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8018, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8074, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8005, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8378, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8250, device='cuda:0', grad_fn=<DivBackward0>)]
Training finished in 6.0 minutes.
Model saved to checkpoints/20230122_142756/model_15
Loading model from checkpoints/20230122_142756/model_15
SUB: 16.10%, DEL: 7.20%, INS: 2.45%, COR: 76.70%, PER: 25.75%
