Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=4, fbank_dims=23, model_dims=128, concat=1, lr=0.002, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.2, beta1=0.9, beta2=0.999)
Total number of model parameters is 1352744
EPOCH 1:
  batch 50 loss: 4.9976029920578
  batch 100 loss: 3.3183343982696534
  batch 150 loss: 3.299527087211609
  batch 200 loss: 3.2617482280731203
  batch 250 loss: 3.207654972076416
  batch 300 loss: 3.1151410818099974
  batch 350 loss: 2.987703113555908
  batch 400 loss: 2.9149050903320313
  batch 450 loss: 2.8420301151275633
  batch 500 loss: 2.7467350578308105
  batch 550 loss: 2.6678789758682253
  batch 600 loss: 2.6160693979263305
  batch 650 loss: 2.522995343208313
  batch 700 loss: 2.4329916048049927
  batch 750 loss: 2.3116285514831545
  batch 800 loss: 2.2576889395713806
  batch 850 loss: 2.1597190999984743
  batch 900 loss: 2.0789501810073854
LOSS train 2.07895 valid 1.96834, valid PER 58.53%
EPOCH 2:
  batch 50 loss: 2.0203744626045226
  batch 100 loss: 1.9648833966255188
  batch 150 loss: 1.874936065673828
  batch 200 loss: 1.8315511107444764
  batch 250 loss: 1.8053945732116699
  batch 300 loss: 1.7276734352111816
  batch 350 loss: 1.717893602848053
  batch 400 loss: 1.683187325000763
  batch 450 loss: 1.6365067410469054
  batch 500 loss: 1.634960880279541
  batch 550 loss: 1.5831397438049317
  batch 600 loss: 1.5585736417770386
  batch 650 loss: 1.5021531009674072
  batch 700 loss: 1.4986695098876952
  batch 750 loss: 1.4750820970535279
  batch 800 loss: 1.4301826310157777
  batch 850 loss: 1.403772783279419
  batch 900 loss: 1.3779009437561036
LOSS train 1.37790 valid 1.37195, valid PER 41.80%
EPOCH 3:
  batch 50 loss: 1.3320602250099183
  batch 100 loss: 1.3928030133247375
  batch 150 loss: 1.3590087866783143
  batch 200 loss: 1.2995099329948425
  batch 250 loss: 1.3109734511375428
  batch 300 loss: 1.2780751919746398
  batch 350 loss: 1.3121658635139466
  batch 400 loss: 1.2632847905158997
  batch 450 loss: 1.2767308974266052
  batch 500 loss: 1.2371372413635253
  batch 550 loss: 1.217479614019394
  batch 600 loss: 1.178711131811142
  batch 650 loss: 1.2041558384895326
  batch 700 loss: 1.1968326485157013
  batch 750 loss: 1.1986412596702576
  batch 800 loss: 1.2037817168235778
  batch 850 loss: 1.1751397883892059
  batch 900 loss: 1.1740772998332978
LOSS train 1.17408 valid 1.14148, valid PER 36.75%
EPOCH 4:
  batch 50 loss: 1.1493472337722779
  batch 100 loss: 1.108976995944977
  batch 150 loss: 1.126813873052597
  batch 200 loss: 1.1397791409492493
  batch 250 loss: 1.1257126104831696
  batch 300 loss: 1.1165477955341339
  batch 350 loss: 1.1109450268745422
  batch 400 loss: 1.0601503479480743
  batch 450 loss: 1.052426370382309
  batch 500 loss: 1.1120998096466064
  batch 550 loss: 1.0590074050426483
  batch 600 loss: 1.0671991729736328
  batch 650 loss: 1.096075986623764
  batch 700 loss: 1.1041663241386415
  batch 750 loss: 1.0760690152645112
  batch 800 loss: 1.0558671605587007
  batch 850 loss: 1.0280757200717927
  batch 900 loss: 1.064910225868225
LOSS train 1.06491 valid 1.03443, valid PER 32.24%
EPOCH 5:
  batch 50 loss: 0.9920596563816071
  batch 100 loss: 0.9888046300411224
  batch 150 loss: 1.0315948796272278
  batch 200 loss: 1.05350870013237
  batch 250 loss: 1.0002007937431336
  batch 300 loss: 1.0073173415660859
  batch 350 loss: 0.9721786522865296
  batch 400 loss: 0.9591082298755645
  batch 450 loss: 0.9715531420707703
  batch 500 loss: 0.9482102179527283
  batch 550 loss: 1.0125603437423707
  batch 600 loss: 0.9878244829177857
  batch 650 loss: 0.9921429169178009
  batch 700 loss: 0.9925513136386871
  batch 750 loss: 0.9913991606235504
  batch 800 loss: 1.0142178189754487
  batch 850 loss: 1.017245852947235
  batch 900 loss: 0.9804363906383514
LOSS train 0.98044 valid 0.97211, valid PER 30.40%
EPOCH 6:
  batch 50 loss: 0.9258762884140015
  batch 100 loss: 0.9362727189064026
  batch 150 loss: 0.9333875739574432
  batch 200 loss: 0.8995252013206482
  batch 250 loss: 0.9100146973133088
  batch 300 loss: 0.9342473554611206
  batch 350 loss: 0.92469597697258
  batch 400 loss: 0.902094476222992
  batch 450 loss: 0.957262544631958
  batch 500 loss: 0.8794124400615693
  batch 550 loss: 0.9352530825138092
  batch 600 loss: 0.9216696465015412
  batch 650 loss: 0.8860063922405242
  batch 700 loss: 0.9088017666339874
  batch 750 loss: 0.909582599401474
  batch 800 loss: 0.90955064535141
  batch 850 loss: 0.9181025159358979
  batch 900 loss: 0.932199993133545
LOSS train 0.93220 valid 0.90062, valid PER 29.08%
EPOCH 7:
  batch 50 loss: 0.8596157217025757
  batch 100 loss: 0.904454517364502
  batch 150 loss: 0.8446077394485474
  batch 200 loss: 0.8639438617229461
  batch 250 loss: 0.9182103562355042
  batch 300 loss: 0.8766724467277527
  batch 350 loss: 0.9150653326511383
  batch 400 loss: 0.878681868314743
  batch 450 loss: 0.899306571483612
  batch 500 loss: 0.8650341594219207
  batch 550 loss: 0.853749520778656
  batch 600 loss: 0.8779101753234864
  batch 650 loss: 0.8496394777297973
  batch 700 loss: 0.8944080662727356
  batch 750 loss: 0.8670792174339295
  batch 800 loss: 0.8490772581100464
  batch 850 loss: 0.842653204202652
  batch 900 loss: 0.859917221069336
LOSS train 0.85992 valid 0.88445, valid PER 27.71%
EPOCH 8:
  batch 50 loss: 0.8248934268951416
  batch 100 loss: 0.7973258471488953
  batch 150 loss: 0.8255011975765228
  batch 200 loss: 0.8677902865409851
  batch 250 loss: 0.8137804400920868
  batch 300 loss: 0.7989707636833191
  batch 350 loss: 0.8365162873268127
  batch 400 loss: 0.8291316163539887
  batch 450 loss: 0.8510454273223877
  batch 500 loss: 0.8240801990032196
  batch 550 loss: 0.8297950148582458
  batch 600 loss: 0.7860822069644928
  batch 650 loss: 0.8094840908050537
  batch 700 loss: 0.8286244773864746
  batch 750 loss: 0.8279423582553863
  batch 800 loss: 0.834812091588974
  batch 850 loss: 0.8115998864173889
  batch 900 loss: 0.8235653972625733
LOSS train 0.82357 valid 0.83977, valid PER 26.53%
EPOCH 9:
  batch 50 loss: 0.7574362683296204
  batch 100 loss: 0.7635818505287171
  batch 150 loss: 0.7714484131336212
  batch 200 loss: 0.7732407438755036
  batch 250 loss: 0.7608215951919556
  batch 300 loss: 0.7866960108280182
  batch 350 loss: 0.7925733137130737
  batch 400 loss: 0.8160840171575546
  batch 450 loss: 0.820468897819519
  batch 500 loss: 0.7907810425758361
  batch 550 loss: 0.7642048561573028
  batch 600 loss: 0.8017355883121491
  batch 650 loss: 0.7932598638534546
  batch 700 loss: 0.7811234772205353
  batch 750 loss: 0.7831300294399262
  batch 800 loss: 0.8309239494800568
  batch 850 loss: 0.7949539744853973
  batch 900 loss: 0.7654294061660767
LOSS train 0.76543 valid 0.83621, valid PER 26.74%
EPOCH 10:
  batch 50 loss: 0.7619455051422119
  batch 100 loss: 0.8053267163038254
  batch 150 loss: 0.7782504713535309
  batch 200 loss: 0.757889518737793
  batch 250 loss: 0.7604434996843338
  batch 300 loss: 0.7586704158782959
  batch 350 loss: 0.7583676201105117
  batch 400 loss: 0.745455596446991
  batch 450 loss: 0.7475209259986877
  batch 500 loss: 0.7519009554386139
  batch 550 loss: 0.7347751772403717
  batch 600 loss: 0.7456076180934906
  batch 650 loss: 0.7589011549949646
  batch 700 loss: 0.7579114520549775
  batch 750 loss: 0.757756398320198
  batch 800 loss: 0.7633551388978959
  batch 850 loss: 0.7105804735422134
  batch 900 loss: 0.7504368960857392
LOSS train 0.75044 valid 0.83390, valid PER 26.39%
EPOCH 11:
  batch 50 loss: 0.726936690211296
  batch 100 loss: 0.6957389777898788
  batch 150 loss: 0.7217067587375641
  batch 200 loss: 0.6893615472316742
  batch 250 loss: 0.7047063398361206
  batch 300 loss: 0.715083549618721
  batch 350 loss: 0.7377184277772904
  batch 400 loss: 0.7296852868795395
  batch 450 loss: 0.7171461927890778
  batch 500 loss: 0.7152402675151825
  batch 550 loss: 0.7345044952630997
  batch 600 loss: 0.7255604434013366
  batch 650 loss: 0.7392688655853271
  batch 700 loss: 0.8119693285226822
  batch 750 loss: 0.7529065084457397
  batch 800 loss: 0.7248426419496536
  batch 850 loss: 0.7053844845294952
  batch 900 loss: 0.7593667334318162
LOSS train 0.75937 valid 0.82634, valid PER 25.57%
EPOCH 12:
  batch 50 loss: 0.6696964997053146
  batch 100 loss: 0.6730243927240371
  batch 150 loss: 0.6924411571025848
  batch 200 loss: 0.691451969742775
  batch 250 loss: 0.6840595424175262
  batch 300 loss: 0.7149208772182465
  batch 350 loss: 0.6933321279287338
  batch 400 loss: 0.7139527732133866
  batch 450 loss: 0.6946803951263427
  batch 500 loss: 0.7063930654525756
  batch 550 loss: 0.6961497443914414
  batch 600 loss: 0.7025827604532242
  batch 650 loss: 0.703431915640831
  batch 700 loss: 0.6867285662889481
  batch 750 loss: 0.7203444588184357
  batch 800 loss: 0.683568902015686
  batch 850 loss: 0.7055856198072433
  batch 900 loss: 0.701522336602211
LOSS train 0.70152 valid 0.78172, valid PER 24.16%
EPOCH 13:
  batch 50 loss: 0.651469064950943
  batch 100 loss: 0.6542341995239258
  batch 150 loss: 0.6793575292825699
  batch 200 loss: 0.6454754799604416
  batch 250 loss: 0.6655738371610641
  batch 300 loss: 0.7061328279972077
  batch 350 loss: 0.6583654791116714
  batch 400 loss: 0.6495674407482147
  batch 450 loss: 0.683129472732544
  batch 500 loss: 0.6666927403211593
  batch 550 loss: 0.7059437745809555
  batch 600 loss: 0.6891677325963974
  batch 650 loss: 0.6792513966560364
  batch 700 loss: 0.6930672144889831
  batch 750 loss: 0.6612449610233306
  batch 800 loss: 0.6665634882450103
  batch 850 loss: 0.6701062482595443
  batch 900 loss: 0.6623413753509522
LOSS train 0.66234 valid 0.77983, valid PER 24.18%
EPOCH 14:
  batch 50 loss: 0.6120349842309952
  batch 100 loss: 0.5924896037578583
  batch 150 loss: 0.6290188014507294
  batch 200 loss: 0.6536637842655182
  batch 250 loss: 0.6464877957105637
  batch 300 loss: 0.6237621712684631
  batch 350 loss: 0.6223726373910904
  batch 400 loss: 0.6542651903629303
  batch 450 loss: 0.6269006621837616
  batch 500 loss: 0.6353368645906449
  batch 550 loss: 0.6516226780414581
  batch 600 loss: 0.6437904387712479
  batch 650 loss: 0.6713258844614028
  batch 700 loss: 0.6626214396953582
  batch 750 loss: 0.6600921815633773
  batch 800 loss: 0.6466381973028184
  batch 850 loss: 0.6908746260404587
  batch 900 loss: 0.6808514875173569
LOSS train 0.68085 valid 0.77701, valid PER 24.23%
EPOCH 15:
  batch 50 loss: 0.5920476531982422
  batch 100 loss: 0.6029817426204681
  batch 150 loss: 0.619078243970871
  batch 200 loss: 0.6550760281085968
  batch 250 loss: 0.6646113485097885
  batch 300 loss: 0.6378998196125031
  batch 350 loss: 0.6316791903972626
  batch 400 loss: 0.6338420671224594
  batch 450 loss: 0.6431904464960099
  batch 500 loss: 0.6255511164665222
  batch 550 loss: 0.652915101647377
  batch 600 loss: 0.663730781674385
  batch 650 loss: 0.6211267286539077
  batch 700 loss: 0.6029122912883759
  batch 750 loss: 0.6600659137964249
  batch 800 loss: 0.6162495219707489
  batch 850 loss: 0.6195870023965836
  batch 900 loss: 0.5883042532205581
LOSS train 0.58830 valid 0.75381, valid PER 23.64%
EPOCH 16:
  batch 50 loss: 0.588920669555664
  batch 100 loss: 0.5602611303329468
  batch 150 loss: 0.586753545999527
  batch 200 loss: 0.6084343498945236
  batch 250 loss: 0.5890227609872818
  batch 300 loss: 0.6002129799127579
  batch 350 loss: 0.6057476955652237
  batch 400 loss: 0.6353087651729584
  batch 450 loss: 0.6464812129735946
  batch 500 loss: 0.6321783578395843
  batch 550 loss: 0.5775355529785157
  batch 600 loss: 0.6333876836299897
  batch 650 loss: 0.626339555978775
  batch 700 loss: 0.6180371195077896
  batch 750 loss: 0.606073972582817
  batch 800 loss: 0.6027921313047409
  batch 850 loss: 0.6212289834022522
  batch 900 loss: 0.6414699882268906
LOSS train 0.64147 valid 0.78221, valid PER 24.26%
EPOCH 17:
  batch 50 loss: 0.5957859987020493
  batch 100 loss: 0.5742344534397126
  batch 150 loss: 0.6291278213262558
  batch 200 loss: 0.5970548659563064
  batch 250 loss: 0.5988778883218765
  batch 300 loss: 0.5900728696584702
  batch 350 loss: 0.6278450107574463
  batch 400 loss: 0.6215433531999588
  batch 450 loss: 0.5745188540220261
  batch 500 loss: 0.6067775112390518
  batch 550 loss: 0.5684941905736923
  batch 600 loss: 0.6344368559122086
  batch 650 loss: 0.5807336634397506
  batch 700 loss: 0.5953017222881317
  batch 750 loss: 0.5725597351789474
  batch 800 loss: 0.6042101567983628
  batch 850 loss: 0.6186492937803268
  batch 900 loss: 0.5879699748754501
LOSS train 0.58797 valid 0.73831, valid PER 23.03%
EPOCH 18:
  batch 50 loss: 0.5341949075460434
  batch 100 loss: 0.531435723900795
  batch 150 loss: 0.5929028129577637
  batch 200 loss: 0.5697030735015869
  batch 250 loss: 0.5406732261180878
  batch 300 loss: 0.5461027479171753
  batch 350 loss: 0.5404293394088745
  batch 400 loss: 0.5807808899879455
  batch 450 loss: 0.6012260949611664
  batch 500 loss: 0.593021679520607
  batch 550 loss: 0.6197469645738601
  batch 600 loss: 0.5942860925197602
  batch 650 loss: 0.5779197397828102
  batch 700 loss: 0.562899763584137
  batch 750 loss: 0.5916705751419067
  batch 800 loss: 0.5851238018274307
  batch 850 loss: 0.5830697417259216
  batch 900 loss: 0.5875242120027542
LOSS train 0.58752 valid 0.74872, valid PER 23.14%
EPOCH 19:
  batch 50 loss: 0.5209856617450714
  batch 100 loss: 0.5320325553417206
  batch 150 loss: 0.5070284354686737
  batch 200 loss: 0.5515725606679917
  batch 250 loss: 0.5604244834184646
  batch 300 loss: 0.5654873377084733
  batch 350 loss: 0.5622160983085632
  batch 400 loss: 0.5413509839773178
  batch 450 loss: 0.5420622819662094
  batch 500 loss: 0.5571615487337113
  batch 550 loss: 0.5804823553562164
  batch 600 loss: 0.5580878847837448
  batch 650 loss: 0.5944670176506043
  batch 700 loss: 0.5972689235210419
  batch 750 loss: 0.5641220700740814
  batch 800 loss: 0.5394108110666275
  batch 850 loss: 0.561926645040512
  batch 900 loss: 0.5560885214805603
LOSS train 0.55609 valid 0.77534, valid PER 23.40%
EPOCH 20:
  batch 50 loss: 0.5226332777738572
  batch 100 loss: 0.5454438990354538
  batch 150 loss: 0.5426524317264557
  batch 200 loss: 0.5271920663118362
  batch 250 loss: 0.547293449640274
  batch 300 loss: 0.5277225470542908
  batch 350 loss: 0.4985679340362549
  batch 400 loss: 0.5455298143625259
  batch 450 loss: 0.5191868305206299
  batch 500 loss: 0.554140778183937
  batch 550 loss: 0.5431034457683563
  batch 600 loss: 0.5312290966510773
  batch 650 loss: 0.5475712978839874
  batch 700 loss: 0.5537334978580475
  batch 750 loss: 0.5424761807918549
  batch 800 loss: 0.5684533852338791
  batch 850 loss: 0.5704154634475708
  batch 900 loss: 0.5686644840240479
LOSS train 0.56866 valid 0.76738, valid PER 23.34%
[2.0789501810073854, 1.3779009437561036, 1.1740772998332978, 1.064910225868225, 0.9804363906383514, 0.932199993133545, 0.859917221069336, 0.8235653972625733, 0.7654294061660767, 0.7504368960857392, 0.7593667334318162, 0.701522336602211, 0.6623413753509522, 0.6808514875173569, 0.5883042532205581, 0.6414699882268906, 0.5879699748754501, 0.5875242120027542, 0.5560885214805603, 0.5686644840240479]
[tensor(1.9683, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.3720, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.1415, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.0344, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9721, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9006, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8844, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8398, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8362, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8339, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8263, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.7817, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.7798, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.7770, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.7538, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.7822, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.7383, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.7487, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.7753, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.7674, device='cuda:0', grad_fn=<DivBackward0>)]
Training finished in 9.0 minutes.
Model saved to checkpoints/20230122_145744/model_17
Loading model from checkpoints/20230122_145744/model_17
SUB: 15.28%, DEL: 6.77%, INS: 2.22%, COR: 77.95%, PER: 24.27%
