Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=3, fbank_dims=23, model_dims=128, concat=1, lr=0.002, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.2, beta1=0.9, beta2=0.999)
Total number of model parameters is 957480
EPOCH 1:
  batch 50 loss: 5.2778342866897585
  batch 100 loss: 3.2852116298675536
  batch 150 loss: 3.2715072441101074
  batch 200 loss: 3.240929207801819
  batch 250 loss: 3.136433205604553
  batch 300 loss: 2.895677571296692
  batch 350 loss: 2.7404601192474365
  batch 400 loss: 2.680982961654663
  batch 450 loss: 2.585131196975708
  batch 500 loss: 2.433854331970215
  batch 550 loss: 2.2816681098937988
  batch 600 loss: 2.1971800756454467
  batch 650 loss: 2.0685020327568053
  batch 700 loss: 2.019862298965454
  batch 750 loss: 1.8971819329261779
  batch 800 loss: 1.8602744364738464
  batch 850 loss: 1.788857274055481
  batch 900 loss: 1.696953947544098
LOSS train 1.69695 valid 1.63912, valid PER 50.76%
EPOCH 2:
  batch 50 loss: 1.6343336367607117
  batch 100 loss: 1.625067286491394
  batch 150 loss: 1.5270397067070007
  batch 200 loss: 1.487318844795227
  batch 250 loss: 1.4708228826522827
  batch 300 loss: 1.4057299423217773
  batch 350 loss: 1.4211272192001343
  batch 400 loss: 1.4002296495437623
  batch 450 loss: 1.3675738644599915
  batch 500 loss: 1.3491790461540223
  batch 550 loss: 1.3016808116436005
  batch 600 loss: 1.2872578811645508
  batch 650 loss: 1.245764763355255
  batch 700 loss: 1.2894255375862123
  batch 750 loss: 1.2470952582359314
  batch 800 loss: 1.2273083233833313
  batch 850 loss: 1.1944024348258973
  batch 900 loss: 1.1832485568523408
LOSS train 1.18325 valid 1.16450, valid PER 36.78%
EPOCH 3:
  batch 50 loss: 1.116420418024063
  batch 100 loss: 1.1847935950756072
  batch 150 loss: 1.2056909680366517
  batch 200 loss: 1.1113283848762512
  batch 250 loss: 1.1070239377021789
  batch 300 loss: 1.125358887910843
  batch 350 loss: 1.126065320968628
  batch 400 loss: 1.1071764206886292
  batch 450 loss: 1.0843674373626708
  batch 500 loss: 1.0662132358551026
  batch 550 loss: 1.0848367726802826
  batch 600 loss: 1.0417865538597106
  batch 650 loss: 1.05156529545784
  batch 700 loss: 1.0627511262893676
  batch 750 loss: 1.0633154892921448
  batch 800 loss: 1.0791915333271027
  batch 850 loss: 1.0214468038082123
  batch 900 loss: 1.0566562914848328
LOSS train 1.05666 valid 1.05060, valid PER 32.40%
EPOCH 4:
  batch 50 loss: 1.0354579424858092
  batch 100 loss: 0.947048327922821
  batch 150 loss: 0.9692193973064422
  batch 200 loss: 0.9778507769107818
  batch 250 loss: 0.9785252487659455
  batch 300 loss: 0.9737328433990479
  batch 350 loss: 0.964298609495163
  batch 400 loss: 0.9289264404773712
  batch 450 loss: 0.9478024613857269
  batch 500 loss: 1.0010181331634522
  batch 550 loss: 0.9404755175113678
  batch 600 loss: 0.9426861357688904
  batch 650 loss: 0.9760233223438263
  batch 700 loss: 0.9874333691596985
  batch 750 loss: 0.9513319826126099
  batch 800 loss: 0.9405252945423126
  batch 850 loss: 0.9212358725070954
  batch 900 loss: 0.9440485334396362
LOSS train 0.94405 valid 0.99968, valid PER 30.08%
EPOCH 5:
  batch 50 loss: 0.9194193160533906
  batch 100 loss: 0.8882714760303497
  batch 150 loss: 0.9225198066234589
  batch 200 loss: 0.932640187740326
  batch 250 loss: 0.8800765979290008
  batch 300 loss: 0.9228758823871612
  batch 350 loss: 0.8681216776371002
  batch 400 loss: 0.8437048161029815
  batch 450 loss: 0.900457044839859
  batch 500 loss: 0.8703719127178192
  batch 550 loss: 0.9351002776622772
  batch 600 loss: 0.881157443523407
  batch 650 loss: 0.8892765009403228
  batch 700 loss: 0.8716707873344421
  batch 750 loss: 0.8863047206401825
  batch 800 loss: 0.9229973590373993
  batch 850 loss: 0.9170661163330078
  batch 900 loss: 0.8544405114650726
LOSS train 0.85444 valid 0.90366, valid PER 28.20%
EPOCH 6:
  batch 50 loss: 0.8082006216049195
  batch 100 loss: 0.8435007619857788
  batch 150 loss: 0.798542788028717
  batch 200 loss: 0.7942202317714692
  batch 250 loss: 0.7907741928100586
  batch 300 loss: 0.829908595085144
  batch 350 loss: 0.8347669422626496
  batch 400 loss: 0.8327035713195801
  batch 450 loss: 0.8645039570331573
  batch 500 loss: 0.7981572216749191
  batch 550 loss: 0.8186902391910553
  batch 600 loss: 0.80468759059906
  batch 650 loss: 0.8003154623508454
  batch 700 loss: 0.7897641062736511
  batch 750 loss: 0.8134135222434997
  batch 800 loss: 0.8299205756187439
  batch 850 loss: 0.8544685852527618
  batch 900 loss: 0.857613251209259
LOSS train 0.85761 valid 0.87503, valid PER 27.21%
EPOCH 7:
  batch 50 loss: 0.7566814196109771
  batch 100 loss: 0.798619129061699
  batch 150 loss: 0.7586011278629303
  batch 200 loss: 0.7611794191598892
  batch 250 loss: 0.7989688527584076
  batch 300 loss: 0.754386090040207
  batch 350 loss: 0.7880309987068176
  batch 400 loss: 0.7523666059970856
  batch 450 loss: 0.7973112165927887
  batch 500 loss: 0.7939529323577881
  batch 550 loss: 0.7753628432750702
  batch 600 loss: 0.7761012029647827
  batch 650 loss: 0.7674457097053528
  batch 700 loss: 0.7980153739452363
  batch 750 loss: 0.7856106543540955
  batch 800 loss: 0.7591464340686798
  batch 850 loss: 0.7641281342506409
  batch 900 loss: 0.7804545199871064
LOSS train 0.78045 valid 0.89456, valid PER 27.82%
EPOCH 8:
  batch 50 loss: 0.778511547446251
  batch 100 loss: 0.7071767199039459
  batch 150 loss: 0.7440891617536545
  batch 200 loss: 0.7514690637588501
  batch 250 loss: 0.7164108657836914
  batch 300 loss: 0.7248257052898407
  batch 350 loss: 0.7395577597618103
  batch 400 loss: 0.7136237061023712
  batch 450 loss: 0.7771780228614807
  batch 500 loss: 0.7407945781946182
  batch 550 loss: 0.732468523979187
  batch 600 loss: 0.7028102469444275
  batch 650 loss: 0.7357572734355926
  batch 700 loss: 0.7596368932723999
  batch 750 loss: 0.752499577999115
  batch 800 loss: 0.7781737172603607
  batch 850 loss: 0.7321230971813202
  batch 900 loss: 0.7246660256385803
LOSS train 0.72467 valid 0.82333, valid PER 25.39%
EPOCH 9:
  batch 50 loss: 0.6474299424886704
  batch 100 loss: 0.6485364192724228
  batch 150 loss: 0.6921998995542527
  batch 200 loss: 0.6897919511795044
  batch 250 loss: 0.6821076363325119
  batch 300 loss: 0.7110662770271301
  batch 350 loss: 0.6806665879487991
  batch 400 loss: 0.7140304148197174
  batch 450 loss: 0.7380293571949005
  batch 500 loss: 0.7103594863414764
  batch 550 loss: 0.714395831823349
  batch 600 loss: 0.7162743711471558
  batch 650 loss: 0.7254700154066086
  batch 700 loss: 0.7302898490428924
  batch 750 loss: 0.7400947666168213
  batch 800 loss: 0.7650844240188599
  batch 850 loss: 0.7268930393457412
  batch 900 loss: 0.6756846469640732
LOSS train 0.67568 valid 0.81413, valid PER 24.58%
EPOCH 10:
  batch 50 loss: 0.6380786907672882
  batch 100 loss: 0.6476443356275559
  batch 150 loss: 0.674105036854744
  batch 200 loss: 0.6410447227954864
  batch 250 loss: 0.6836954033374787
  batch 300 loss: 0.6789480394124985
  batch 350 loss: 0.6876818031072617
  batch 400 loss: 0.6468956381082535
  batch 450 loss: 0.6489226317405701
  batch 500 loss: 0.6870180183649063
  batch 550 loss: 0.6867839789390564
  batch 600 loss: 0.6852388250827789
  batch 650 loss: 0.7147422909736634
  batch 700 loss: 0.6797055637836457
  batch 750 loss: 0.7023063838481903
  batch 800 loss: 0.7017001503705979
  batch 850 loss: 0.6671328645944595
  batch 900 loss: 0.6571930092573166
LOSS train 0.65719 valid 0.78487, valid PER 24.26%
EPOCH 11:
  batch 50 loss: 0.5977898496389389
  batch 100 loss: 0.5973440927267074
  batch 150 loss: 0.6244607323408127
  batch 200 loss: 0.6004503542184829
  batch 250 loss: 0.633042408823967
  batch 300 loss: 0.6109295934438705
  batch 350 loss: 0.6635232383012771
  batch 400 loss: 0.6310209256410598
  batch 450 loss: 0.6396096181869507
  batch 500 loss: 0.6422076809406281
  batch 550 loss: 0.6460703718662262
  batch 600 loss: 0.6428685402870178
  batch 650 loss: 0.6690564739704132
  batch 700 loss: 0.7116232788562775
  batch 750 loss: 0.649817955493927
  batch 800 loss: 0.6671368187665939
  batch 850 loss: 0.6463543325662613
  batch 900 loss: 0.6458754801750183
LOSS train 0.64588 valid 0.79828, valid PER 24.22%
EPOCH 12:
  batch 50 loss: 0.5822854632139206
  batch 100 loss: 0.5650933974981308
  batch 150 loss: 0.6015101337432861
  batch 200 loss: 0.5964902615547181
  batch 250 loss: 0.5917492681741714
  batch 300 loss: 0.6250150567293167
  batch 350 loss: 0.6602111607789993
  batch 400 loss: 0.7339451313018799
  batch 450 loss: 0.69236736536026
  batch 500 loss: 0.6528912431001663
  batch 550 loss: 0.6235414624214173
  batch 600 loss: 0.642459767460823
  batch 650 loss: 0.6371265625953675
  batch 700 loss: 0.6333336758613587
  batch 750 loss: 0.6563832545280457
  batch 800 loss: 0.6087027645111084
  batch 850 loss: 0.6324955642223358
  batch 900 loss: 0.6565684515237808
LOSS train 0.65657 valid 0.80219, valid PER 23.99%
EPOCH 13:
  batch 50 loss: 0.5620482790470124
  batch 100 loss: 0.5777609801292419
  batch 150 loss: 0.6132371258735657
  batch 200 loss: 0.5677639526128769
  batch 250 loss: 0.586296791434288
  batch 300 loss: 0.6082308864593506
  batch 350 loss: 0.5636102741956711
  batch 400 loss: 0.5915024113655091
  batch 450 loss: 0.6095931589603424
  batch 500 loss: 0.5746830958127975
  batch 550 loss: 0.6120100659132004
  batch 600 loss: 0.5963503623008728
  batch 650 loss: 0.5931626731157302
  batch 700 loss: 0.6113263618946075
  batch 750 loss: 0.5790907788276672
  batch 800 loss: 0.5955398833751678
  batch 850 loss: 0.6149511450529098
  batch 900 loss: 0.6200595718622207
LOSS train 0.62006 valid 0.79242, valid PER 24.04%
EPOCH 14:
  batch 50 loss: 0.5578092283010483
  batch 100 loss: 0.5319264525175095
  batch 150 loss: 0.5715091699361801
  batch 200 loss: 0.5558118665218353
  batch 250 loss: 0.5593831288814545
  batch 300 loss: 0.5417101001739502
  batch 350 loss: 0.5762522846460343
  batch 400 loss: 0.5763776749372482
  batch 450 loss: 0.5480217772722245
  batch 500 loss: 0.5654898649454116
  batch 550 loss: 0.5813036435842514
  batch 600 loss: 0.5717811715602875
  batch 650 loss: 0.5800112575292588
  batch 700 loss: 0.5919485205411911
  batch 750 loss: 0.5756025779247284
  batch 800 loss: 0.579843053817749
  batch 850 loss: 0.6172782522439957
  batch 900 loss: 0.5922011977434158
LOSS train 0.59220 valid 0.78193, valid PER 23.72%
EPOCH 15:
  batch 50 loss: 0.5161119878292084
  batch 100 loss: 0.5078096979856491
  batch 150 loss: 0.5413475894927978
  batch 200 loss: 0.5377968901395798
  batch 250 loss: 0.5530953735113144
  batch 300 loss: 0.5330529218912125
  batch 350 loss: 0.5412381082773209
  batch 400 loss: 0.555621320605278
  batch 450 loss: 0.5287680202722549
  batch 500 loss: 0.5166843259334564
  batch 550 loss: 0.584907146692276
  batch 600 loss: 0.5966219806671142
  batch 650 loss: 0.5645067548751831
  batch 700 loss: 0.5354686003923416
  batch 750 loss: 0.5784018760919571
  batch 800 loss: 0.5305144697427749
  batch 850 loss: 0.5276046544313431
  batch 900 loss: 0.5290585571527481
LOSS train 0.52906 valid 0.78272, valid PER 23.44%
EPOCH 16:
  batch 50 loss: 0.5033386486768723
  batch 100 loss: 0.47760933995246885
  batch 150 loss: 0.5141585469245911
  batch 200 loss: 0.5262569850683212
  batch 250 loss: 0.5113248795270919
  batch 300 loss: 0.521865229010582
  batch 350 loss: 0.5190188300609588
  batch 400 loss: 0.5116471046209335
  batch 450 loss: 0.5322702258825303
  batch 500 loss: 0.5129906624555588
  batch 550 loss: 0.4980711805820465
  batch 600 loss: 0.544463871717453
  batch 650 loss: 0.5580375444889069
  batch 700 loss: 0.5173035234212875
  batch 750 loss: 0.5291264301538467
  batch 800 loss: 0.5283635258674622
  batch 850 loss: 0.5155230897665024
  batch 900 loss: 0.5477711528539657
LOSS train 0.54777 valid 0.77702, valid PER 22.69%
EPOCH 17:
  batch 50 loss: 0.495406054854393
  batch 100 loss: 0.46334018528461457
  batch 150 loss: 0.5149987787008286
  batch 200 loss: 0.5035303419828415
  batch 250 loss: 0.5144125908613205
  batch 300 loss: 0.4907365673780441
  batch 350 loss: 0.5416159439086914
  batch 400 loss: 0.5458764922618866
  batch 450 loss: 0.5054949820041656
  batch 500 loss: 0.5044843715429306
  batch 550 loss: 0.5065336996316909
  batch 600 loss: 0.554652094244957
  batch 650 loss: 0.5283225762844086
  batch 700 loss: 0.5312763887643814
  batch 750 loss: 0.479102823138237
  batch 800 loss: 0.5199144232273102
  batch 850 loss: 0.5092839872837067
  batch 900 loss: 0.5040171909332275
LOSS train 0.50402 valid 0.78430, valid PER 22.63%
EPOCH 18:
  batch 50 loss: 0.45420206725597384
  batch 100 loss: 0.4574301791191101
  batch 150 loss: 0.4994014686346054
  batch 200 loss: 0.46419809222221375
  batch 250 loss: 0.4535239753127098
  batch 300 loss: 0.4666136765480042
  batch 350 loss: 0.48367503643035886
  batch 400 loss: 0.4855619990825653
  batch 450 loss: 0.4980927723646164
  batch 500 loss: 0.5030789560079575
  batch 550 loss: 0.5285338217020035
  batch 600 loss: 0.508022158741951
  batch 650 loss: 0.49478714406490326
  batch 700 loss: 0.49431696176528933
  batch 750 loss: 0.516555495262146
  batch 800 loss: 0.5411841607093811
  batch 850 loss: 0.506377003788948
  batch 900 loss: 0.5066852921247482
LOSS train 0.50669 valid 0.78229, valid PER 22.51%
EPOCH 19:
  batch 50 loss: 0.456561986207962
  batch 100 loss: 0.4686660522222519
  batch 150 loss: 0.4629547068476677
  batch 200 loss: 0.4649996101856232
  batch 250 loss: 0.48513310492038725
  batch 300 loss: 0.5092689418792724
  batch 350 loss: 0.5022221213579178
  batch 400 loss: 0.46225573182106017
  batch 450 loss: 0.4537988954782486
  batch 500 loss: 0.44387992173433305
  batch 550 loss: 0.48383045256137847
  batch 600 loss: 0.4694660711288452
  batch 650 loss: 0.5029500949382782
  batch 700 loss: 0.530662055015564
  batch 750 loss: 0.4796022361516952
  batch 800 loss: 0.4772925651073456
  batch 850 loss: 0.49079971373081205
  batch 900 loss: 0.4644840154051781
LOSS train 0.46448 valid 0.77143, valid PER 22.15%
EPOCH 20:
  batch 50 loss: 0.41591723829507826
  batch 100 loss: 0.4450026398897171
  batch 150 loss: 0.43420051902532575
  batch 200 loss: 0.4319010651111603
  batch 250 loss: 0.4463801509141922
  batch 300 loss: 0.45674633502960205
  batch 350 loss: 0.44274660289287565
  batch 400 loss: 0.47588612258434293
  batch 450 loss: 0.44495049327611924
  batch 500 loss: 0.48694114089012147
  batch 550 loss: 0.49605774581432344
  batch 600 loss: 0.4784937012195587
  batch 650 loss: 0.48167033135890963
  batch 700 loss: 0.484042546749115
  batch 750 loss: 0.44968300104141234
  batch 800 loss: 0.4910718059539795
  batch 850 loss: 0.4919352775812149
  batch 900 loss: 0.48733641088008883
LOSS train 0.48734 valid 0.78252, valid PER 22.74%
[1.696953947544098, 1.1832485568523408, 1.0566562914848328, 0.9440485334396362, 0.8544405114650726, 0.857613251209259, 0.7804545199871064, 0.7246660256385803, 0.6756846469640732, 0.6571930092573166, 0.6458754801750183, 0.6565684515237808, 0.6200595718622207, 0.5922011977434158, 0.5290585571527481, 0.5477711528539657, 0.5040171909332275, 0.5066852921247482, 0.4644840154051781, 0.48733641088008883]
[tensor(1.6391, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.1645, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.0506, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9997, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9037, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8750, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8946, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8233, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8141, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.7849, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.7983, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8022, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.7924, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.7819, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.7827, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.7770, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.7843, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.7823, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.7714, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.7825, device='cuda:0', grad_fn=<DivBackward0>)]
Training finished in 8.0 minutes.
Model saved to checkpoints/20230122_142756/model_19
Loading model from checkpoints/20230122_142756/model_19
SUB: 15.67%, DEL: 5.87%, INS: 3.21%, COR: 78.47%, PER: 24.75%
