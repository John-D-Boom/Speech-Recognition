Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=3, fbank_dims=23, model_dims=128, concat=1, lr=0.003, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.2, beta1=0.9, beta2=0.999)
Total number of model parameters is 957480
EPOCH 1:
  batch 50 loss: 4.941560530662537
  batch 100 loss: 3.2804991722106935
  batch 150 loss: 3.253958849906921
  batch 200 loss: 3.1769265794754027
  batch 250 loss: 2.939404444694519
  batch 300 loss: 2.7145012998580933
  batch 350 loss: 2.6148228645324707
  batch 400 loss: 2.5394723176956178
  batch 450 loss: 2.4053594636917115
  batch 500 loss: 2.2410297918319704
  batch 550 loss: 2.1054717111587524
  batch 600 loss: 2.0453725361824038
  batch 650 loss: 1.9454091668128968
  batch 700 loss: 1.9400568008422852
  batch 750 loss: 1.8799267792701722
  batch 800 loss: 1.8660690474510193
  batch 850 loss: 1.7660479545593262
  batch 900 loss: 1.6500791025161743
LOSS train 1.65008 valid 1.65957, valid PER 49.62%
EPOCH 2:
  batch 50 loss: 1.6368817353248597
  batch 100 loss: 1.6151556301116943
  batch 150 loss: 1.5205687475204468
  batch 200 loss: 1.5525176477432252
  batch 250 loss: 1.5044084811210632
  batch 300 loss: 1.4386984276771546
  batch 350 loss: 1.4851079535484315
  batch 400 loss: 1.4136753177642822
  batch 450 loss: 1.3826448655128478
  batch 500 loss: 1.4136249136924743
  batch 550 loss: 1.4687293314933776
  batch 600 loss: 1.3890986585617064
  batch 650 loss: 1.3381350421905518
  batch 700 loss: 1.3653450918197632
  batch 750 loss: 1.3523390412330627
  batch 800 loss: 1.2838629984855652
  batch 850 loss: 1.2928943586349488
  batch 900 loss: 1.2477768301963805
LOSS train 1.24778 valid 1.25047, valid PER 38.96%
EPOCH 3:
  batch 50 loss: 1.182050039768219
  batch 100 loss: 1.2740445399284364
  batch 150 loss: 1.2726261222362518
  batch 200 loss: 1.168753024339676
  batch 250 loss: 1.1837528347969055
  batch 300 loss: 1.222835144996643
  batch 350 loss: 1.222527219057083
  batch 400 loss: 1.1880952250957488
  batch 450 loss: 1.168216850757599
  batch 500 loss: 1.1353841042518615
  batch 550 loss: 1.1417637801170348
  batch 600 loss: 1.1172008621692657
  batch 650 loss: 1.1594815242290497
  batch 700 loss: 1.1614847910404205
  batch 750 loss: 1.2041654062271119
  batch 800 loss: 1.2154115951061248
  batch 850 loss: 1.1232783794403076
  batch 900 loss: 1.1326798355579377
LOSS train 1.13268 valid 1.09387, valid PER 34.20%
EPOCH 4:
  batch 50 loss: 1.1231243622303009
  batch 100 loss: 1.0359119284152984
  batch 150 loss: 1.0653802943229675
  batch 200 loss: 1.0712184154987334
  batch 250 loss: 1.0894626796245575
  batch 300 loss: 1.0872409081459045
  batch 350 loss: 1.055841144323349
  batch 400 loss: 1.0361320769786835
  batch 450 loss: 1.0292753434181214
  batch 500 loss: 1.1004724383354187
  batch 550 loss: 1.0360123991966248
  batch 600 loss: 1.0277750205993652
  batch 650 loss: 1.0930207943916321
  batch 700 loss: 1.0800373578071594
  batch 750 loss: 1.0605641603469849
  batch 800 loss: 1.0923930788040161
  batch 850 loss: 1.0353954470157622
  batch 900 loss: 1.0173022437095642
LOSS train 1.01730 valid 1.03690, valid PER 32.58%
EPOCH 5:
  batch 50 loss: 0.9880965423583984
  batch 100 loss: 0.9858615005016327
  batch 150 loss: 1.035881096124649
  batch 200 loss: 1.0270445454120636
  batch 250 loss: 0.9647788858413696
  batch 300 loss: 0.9766293716430664
  batch 350 loss: 0.9478192782402038
  batch 400 loss: 0.9438464391231537
  batch 450 loss: 0.973413735628128
  batch 500 loss: 0.9583520138263703
  batch 550 loss: 1.0370192801952363
  batch 600 loss: 1.026830985546112
  batch 650 loss: 1.0055928361415862
  batch 700 loss: 0.9883561515808106
  batch 750 loss: 0.9926355290412903
  batch 800 loss: 0.9992967438697815
  batch 850 loss: 1.0450939154624939
  batch 900 loss: 0.958249204158783
LOSS train 0.95825 valid 0.96707, valid PER 29.82%
EPOCH 6:
  batch 50 loss: 0.9066713201999664
  batch 100 loss: 0.9636907160282135
  batch 150 loss: 0.9596775734424591
  batch 200 loss: 0.921451085805893
  batch 250 loss: 0.945922167301178
  batch 300 loss: 0.9687717270851135
  batch 350 loss: 0.9531713223457337
  batch 400 loss: 0.9361260116100312
  batch 450 loss: 0.9545814907550811
  batch 500 loss: 0.9148267877101898
  batch 550 loss: 0.9336376047134399
  batch 600 loss: 0.9168945407867432
  batch 650 loss: 0.8852108013629914
  batch 700 loss: 0.8877581775188446
  batch 750 loss: 0.9409461688995361
  batch 800 loss: 0.9445477545261383
  batch 850 loss: 0.9900852406024933
  batch 900 loss: 0.9482032263278961
LOSS train 0.94820 valid 0.97410, valid PER 30.44%
EPOCH 7:
  batch 50 loss: 0.9279150414466858
  batch 100 loss: 0.9882376575469971
  batch 150 loss: 0.9329704058170318
  batch 200 loss: 0.9109679102897644
  batch 250 loss: 0.9245195126533509
  batch 300 loss: 0.8821758997440338
  batch 350 loss: 0.9276182043552399
  batch 400 loss: 0.8770778691768646
  batch 450 loss: 0.906855914592743
  batch 500 loss: 0.8893751609325409
  batch 550 loss: 0.8786675381660461
  batch 600 loss: 0.8968700218200684
  batch 650 loss: 0.8614774060249328
  batch 700 loss: 0.8976642704010009
  batch 750 loss: 0.8862170505523682
  batch 800 loss: 0.8766460013389588
  batch 850 loss: 0.8984312760829926
  batch 900 loss: 0.8745413041114807
LOSS train 0.87454 valid 0.91339, valid PER 28.80%
EPOCH 8:
  batch 50 loss: 0.8455125176906586
  batch 100 loss: 0.8040654861927032
  batch 150 loss: 0.833270777463913
  batch 200 loss: 0.8369712662696839
  batch 250 loss: 0.8343974769115448
  batch 300 loss: 0.8337432551383972
  batch 350 loss: 0.8611884939670563
  batch 400 loss: 0.8277997553348542
  batch 450 loss: 0.8794011116027832
  batch 500 loss: 0.8418253326416015
  batch 550 loss: 0.825859442949295
  batch 600 loss: 0.8106367528438568
  batch 650 loss: 0.8251198065280915
  batch 700 loss: 0.8661315381526947
  batch 750 loss: 0.8588329708576202
  batch 800 loss: 0.8714757752418518
  batch 850 loss: 0.8469665288925171
  batch 900 loss: 0.8642874109745026
LOSS train 0.86429 valid 0.92452, valid PER 28.80%
EPOCH 9:
  batch 50 loss: 0.8813793122768402
  batch 100 loss: 0.8345123898983001
  batch 150 loss: 0.8100807774066925
  batch 200 loss: 0.8208278405666352
  batch 250 loss: 0.7965582406520844
  batch 300 loss: 0.8167888891696929
  batch 350 loss: 0.8177711635828018
  batch 400 loss: 0.8353327775001526
  batch 450 loss: 0.8561987519264221
  batch 500 loss: 0.8358695614337921
  batch 550 loss: 0.8307591044902801
  batch 600 loss: 0.8653265380859375
  batch 650 loss: 0.8483633911609649
  batch 700 loss: 0.8192693042755127
  batch 750 loss: 0.8231836199760437
  batch 800 loss: 0.8399570071697235
  batch 850 loss: 0.8159390783309937
  batch 900 loss: 0.8065311568975448
LOSS train 0.80653 valid 0.87364, valid PER 27.08%
EPOCH 10:
  batch 50 loss: 0.77834845662117
  batch 100 loss: 0.7793433052301407
  batch 150 loss: 0.8157079970836639
  batch 200 loss: 0.8219901466369629
  batch 250 loss: 0.8239167892932892
  batch 300 loss: 0.8203856122493743
  batch 350 loss: 0.8122463405132294
  batch 400 loss: 0.7745303857326508
  batch 450 loss: 0.7963884079456329
  batch 500 loss: 0.8052603304386139
  batch 550 loss: 0.8303016030788422
  batch 600 loss: 0.8334977459907532
  batch 650 loss: 0.8434533369541168
  batch 700 loss: 0.8735283279418945
  batch 750 loss: 0.8422605895996094
  batch 800 loss: 0.8292284870147705
  batch 850 loss: 0.8003122019767761
  batch 900 loss: 0.7862628751993179
LOSS train 0.78626 valid 0.88171, valid PER 27.08%
EPOCH 11:
  batch 50 loss: 0.7803915107250213
  batch 100 loss: 0.7748495030403137
  batch 150 loss: 0.7702862966060638
  batch 200 loss: 0.7742789208889007
  batch 250 loss: 0.7851958096027374
  batch 300 loss: 0.7638834887742996
  batch 350 loss: 0.8216440725326538
  batch 400 loss: 0.8062928235530853
  batch 450 loss: 0.816380478143692
  batch 500 loss: 0.8168897008895875
  batch 550 loss: 0.8365595483779907
  batch 600 loss: 0.8378914308547973
  batch 650 loss: 0.8901456749439239
  batch 700 loss: 0.8938644349575042
  batch 750 loss: 0.794053225517273
  batch 800 loss: 0.8180199551582337
  batch 850 loss: 0.7997261250019073
  batch 900 loss: 0.8179557812213898
LOSS train 0.81796 valid 0.89059, valid PER 27.26%
EPOCH 12:
  batch 50 loss: 0.7771235907077789
  batch 100 loss: 0.7357752823829651
  batch 150 loss: 0.7768832290172577
  batch 200 loss: 0.8010821223258973
  batch 250 loss: 0.7572265481948852
  batch 300 loss: 0.8159153914451599
  batch 350 loss: 0.7538149291276932
  batch 400 loss: 0.8016913723945618
  batch 450 loss: 0.7654273295402527
  batch 500 loss: 0.8050582695007324
  batch 550 loss: 0.7807852071523667
  batch 600 loss: 0.7722190910577774
  batch 650 loss: 0.7597300577163696
  batch 700 loss: 0.7706192702054977
  batch 750 loss: 0.7834810853004456
  batch 800 loss: 0.7428793609142303
  batch 850 loss: 0.7643490517139435
  batch 900 loss: 0.7678414869308472
LOSS train 0.76784 valid 0.86434, valid PER 26.66%
EPOCH 13:
  batch 50 loss: 0.7354655861854553
  batch 100 loss: 0.7168312686681747
  batch 150 loss: 0.7179268610477447
  batch 200 loss: 0.6966396230459213
  batch 250 loss: 0.7604882061481476
  batch 300 loss: 0.7903879398107528
  batch 350 loss: 0.7253221273422241
  batch 400 loss: 0.7195127826929092
  batch 450 loss: 0.7484764134883881
  batch 500 loss: 0.7510837197303772
  batch 550 loss: 0.7862582641839981
  batch 600 loss: 0.7621468555927277
  batch 650 loss: 0.7636749720573426
  batch 700 loss: 0.759128680229187
  batch 750 loss: 0.7207465851306916
  batch 800 loss: 0.7446784782409668
  batch 850 loss: 0.7219896239042282
  batch 900 loss: 0.7501424902677536
LOSS train 0.75014 valid 0.84085, valid PER 25.90%
EPOCH 14:
  batch 50 loss: 0.7172788226604462
  batch 100 loss: 0.7166739857196808
  batch 150 loss: 0.7250288391113281
  batch 200 loss: 0.7112835443019867
  batch 250 loss: 0.7198445332050324
  batch 300 loss: 0.6975608098506928
  batch 350 loss: 0.724920254945755
  batch 400 loss: 0.7716330766677857
  batch 450 loss: 0.7435479950904846
  batch 500 loss: 0.7292334651947021
  batch 550 loss: 0.7389645510911942
  batch 600 loss: 0.7190179055929184
  batch 650 loss: 0.7572188973426819
  batch 700 loss: 0.7907907497882843
  batch 750 loss: 0.7969799840450287
  batch 800 loss: 0.7551998227834702
  batch 850 loss: 0.8263371098041534
  batch 900 loss: 0.771362988948822
LOSS train 0.77136 valid 0.83997, valid PER 25.80%
EPOCH 15:
  batch 50 loss: 0.684203433394432
  batch 100 loss: 0.6907735592126847
  batch 150 loss: 0.6799052333831788
  batch 200 loss: 0.741760424375534
  batch 250 loss: 0.7195955109596253
  batch 300 loss: 0.7366572451591492
  batch 350 loss: 0.6919792890548706
  batch 400 loss: 0.7027205401659011
  batch 450 loss: 0.7236722075939178
  batch 500 loss: 0.6967750406265258
  batch 550 loss: 0.7331285333633423
  batch 600 loss: 0.7560805952548981
  batch 650 loss: 0.7512607216835022
  batch 700 loss: 0.6955999106168747
  batch 750 loss: 0.750351597070694
  batch 800 loss: 0.6927208876609803
  batch 850 loss: 0.695644965171814
  batch 900 loss: 0.6742871469259262
LOSS train 0.67429 valid 0.80902, valid PER 25.30%
EPOCH 16:
  batch 50 loss: 0.6585987132787704
  batch 100 loss: 0.6389888668060303
  batch 150 loss: 0.6606924116611481
  batch 200 loss: 0.7102926814556122
  batch 250 loss: 0.6584487652778626
  batch 300 loss: 0.6893757051229477
  batch 350 loss: 0.6853102195262909
  batch 400 loss: 0.6570432144403457
  batch 450 loss: 0.6727567422389984
  batch 500 loss: 0.6792439061403275
  batch 550 loss: 0.6674316155910492
  batch 600 loss: 0.7282936537265777
  batch 650 loss: 0.7194943428039551
  batch 700 loss: 0.7320925307273864
  batch 750 loss: 0.7162942433357239
  batch 800 loss: 0.7143417477607727
  batch 850 loss: 0.7470274573564529
  batch 900 loss: 0.7512777996063232
LOSS train 0.75128 valid 0.83977, valid PER 26.06%
EPOCH 17:
  batch 50 loss: 0.6941862571239471
  batch 100 loss: 0.6433549720048904
  batch 150 loss: 0.727119550704956
  batch 200 loss: 0.6523234677314759
  batch 250 loss: 0.6932753199338912
  batch 300 loss: 0.6815639704465866
  batch 350 loss: 0.7187698143720627
  batch 400 loss: 0.7440981984138488
  batch 450 loss: 0.694027384519577
  batch 500 loss: 0.7392632126808166
  batch 550 loss: 0.7207327318191529
  batch 600 loss: 0.744985808134079
  batch 650 loss: 0.675562658905983
  batch 700 loss: 0.6982832491397858
  batch 750 loss: 0.6530524212121963
  batch 800 loss: 0.6971436142921448
  batch 850 loss: 0.7012585359811783
  batch 900 loss: 0.6737756037712097
LOSS train 0.67378 valid 0.80740, valid PER 24.74%
EPOCH 18:
  batch 50 loss: 0.6423445200920105
  batch 100 loss: 0.6374982917308807
  batch 150 loss: 0.686042622923851
  batch 200 loss: 0.6799885576963425
  batch 250 loss: 0.6415584629774094
  batch 300 loss: 0.6840865105390549
  batch 350 loss: 0.6634469705820084
  batch 400 loss: 0.6721202683448791
  batch 450 loss: 0.7079623067378997
  batch 500 loss: 0.6878994965553283
  batch 550 loss: 0.7305286902189255
  batch 600 loss: 0.6928692436218262
  batch 650 loss: 0.6741108119487762
  batch 700 loss: 0.6928559255599975
  batch 750 loss: 0.7242707777023315
  batch 800 loss: 0.7251840376853943
  batch 850 loss: 0.7114614558219909
  batch 900 loss: 0.7060332852602005
LOSS train 0.70603 valid 0.82694, valid PER 25.62%
EPOCH 19:
  batch 50 loss: 0.639059511423111
  batch 100 loss: 0.637448080778122
  batch 150 loss: 0.6194419932365417
  batch 200 loss: 0.6238836944103241
  batch 250 loss: 0.6842963248491287
  batch 300 loss: 0.6866353875398636
  batch 350 loss: 0.6888293832540512
  batch 400 loss: 0.6657719099521637
  batch 450 loss: 0.6455851018428802
  batch 500 loss: 0.6854070895910263
  batch 550 loss: 0.7309213471412659
  batch 600 loss: 0.6898961639404297
  batch 650 loss: 0.7178770488500595
  batch 700 loss: 0.7411492049694062
  batch 750 loss: 0.7882224929332733
  batch 800 loss: 0.7565080374479294
  batch 850 loss: 0.7625772321224212
  batch 900 loss: 0.8288330745697021
LOSS train 0.82883 valid 0.90322, valid PER 28.02%
EPOCH 20:
  batch 50 loss: 0.7622881197929382
  batch 100 loss: 0.7984392869472504
  batch 150 loss: 0.7724564564228058
  batch 200 loss: 0.7411217629909516
  batch 250 loss: 0.7359776711463928
  batch 300 loss: 0.7182491427659988
  batch 350 loss: 0.6934156382083893
  batch 400 loss: 0.6881000971794129
  batch 450 loss: 0.6757173854112625
  batch 500 loss: 0.6975245225429535
  batch 550 loss: 0.6803255522251129
  batch 600 loss: 0.7061768841743469
  batch 650 loss: 0.7226284545660019
  batch 700 loss: 0.6899376571178436
  batch 750 loss: 0.670103058218956
  batch 800 loss: 0.6836662065982818
  batch 850 loss: 0.6825105804204941
  batch 900 loss: 0.6459426838159561
LOSS train 0.64594 valid 0.82703, valid PER 25.19%
[1.6500791025161743, 1.2477768301963805, 1.1326798355579377, 1.0173022437095642, 0.958249204158783, 0.9482032263278961, 0.8745413041114807, 0.8642874109745026, 0.8065311568975448, 0.7862628751993179, 0.8179557812213898, 0.7678414869308472, 0.7501424902677536, 0.771362988948822, 0.6742871469259262, 0.7512777996063232, 0.6737756037712097, 0.7060332852602005, 0.8288330745697021, 0.6459426838159561]
[tensor(1.6596, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.2505, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.0939, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.0369, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9671, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9741, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9134, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9245, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8736, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8817, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8906, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8643, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8408, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8400, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8090, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8398, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8074, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8269, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9032, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8270, device='cuda:0', grad_fn=<DivBackward0>)]
Training finished in 7.0 minutes.
Model saved to checkpoints/20230122_144422/model_17
Loading model from checkpoints/20230122_144422/model_17
SUB: 16.03%, DEL: 7.90%, INS: 2.20%, COR: 76.07%, PER: 26.12%
