Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=3, fbank_dims=23, model_dims=256, concat=1, lr=0.002, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.2, beta1=0.9, beta2=0.999)
Total number of model parameters is 3749928
EPOCH 1:
  batch 50 loss: 4.553710289001465
  batch 100 loss: 3.228749542236328
  batch 150 loss: 2.98500864982605
  batch 200 loss: 2.618304758071899
  batch 250 loss: 2.3061227083206175
  batch 300 loss: 2.0736332893371583
  batch 350 loss: 1.8731326866149902
  batch 400 loss: 1.8084071326255797
  batch 450 loss: 1.6539828276634216
  batch 500 loss: 1.5427967453002929
  batch 550 loss: 1.4818571209907532
  batch 600 loss: 1.4458526873588562
  batch 650 loss: 1.4031815886497498
  batch 700 loss: 1.3682768106460572
  batch 750 loss: 1.3018666481971741
  batch 800 loss: 1.3479381477832795
  batch 850 loss: 1.290435128211975
  batch 900 loss: 1.2235837602615356
LOSS train 1.22358 valid 1.17461, valid PER 36.89%
EPOCH 2:
  batch 50 loss: 1.2032889008522034
  batch 100 loss: 1.1898231720924377
  batch 150 loss: 1.1443881916999816
  batch 200 loss: 1.1541632497310639
  batch 250 loss: 1.1613146269321442
  batch 300 loss: 1.0836350655555724
  batch 350 loss: 1.1374223601818085
  batch 400 loss: 1.0986337924003602
  batch 450 loss: 1.0615719962120056
  batch 500 loss: 1.067658029794693
  batch 550 loss: 1.0552170050144196
  batch 600 loss: 1.0566785418987275
  batch 650 loss: 1.0341750657558442
  batch 700 loss: 1.0378445768356324
  batch 750 loss: 1.0357464146614075
  batch 800 loss: 0.9881539952754974
  batch 850 loss: 0.9755219686031341
  batch 900 loss: 0.9496627008914947
LOSS train 0.94966 valid 0.98464, valid PER 31.62%
EPOCH 3:
  batch 50 loss: 0.8918077337741852
  batch 100 loss: 0.9519547653198243
  batch 150 loss: 0.964913055896759
  batch 200 loss: 0.9093385994434356
  batch 250 loss: 0.9034280502796173
  batch 300 loss: 0.9276550233364105
  batch 350 loss: 0.9423348999023438
  batch 400 loss: 0.9183589792251587
  batch 450 loss: 0.896319009065628
  batch 500 loss: 0.8750468456745147
  batch 550 loss: 0.8989725375175476
  batch 600 loss: 0.8244431734085083
  batch 650 loss: 0.877906094789505
  batch 700 loss: 0.890031611919403
  batch 750 loss: 0.8854700374603272
  batch 800 loss: 0.9469266057014465
  batch 850 loss: 0.869988261461258
  batch 900 loss: 0.8980614984035492
LOSS train 0.89806 valid 0.91342, valid PER 29.11%
EPOCH 4:
  batch 50 loss: 0.8309290683269501
  batch 100 loss: 0.8085329163074494
  batch 150 loss: 0.81393679022789
  batch 200 loss: 0.8060233330726624
  batch 250 loss: 0.7991667926311493
  batch 300 loss: 0.8227316689491272
  batch 350 loss: 0.8013573229312897
  batch 400 loss: 0.7737939023971557
  batch 450 loss: 0.7638730370998382
  batch 500 loss: 0.8242187881469727
  batch 550 loss: 0.7643610191345215
  batch 600 loss: 0.7658932018280029
  batch 650 loss: 0.8226750373840332
  batch 700 loss: 0.8357686424255371
  batch 750 loss: 0.8316697108745575
  batch 800 loss: 0.8149016332626343
  batch 850 loss: 0.7992986404895782
  batch 900 loss: 0.8307390451431275
LOSS train 0.83074 valid 0.85825, valid PER 26.63%
EPOCH 5:
  batch 50 loss: 0.7356606388092041
  batch 100 loss: 0.6974753588438034
  batch 150 loss: 0.7420958471298218
  batch 200 loss: 0.7670949804782867
  batch 250 loss: 0.7146709287166595
  batch 300 loss: 0.754760149717331
  batch 350 loss: 0.697856233716011
  batch 400 loss: 0.7199685472249985
  batch 450 loss: 0.7209061181545258
  batch 500 loss: 0.7021540910005569
  batch 550 loss: 0.7648868286609649
  batch 600 loss: 0.7715711104869842
  batch 650 loss: 0.757743103504181
  batch 700 loss: 0.7408937335014343
  batch 750 loss: 0.7354393804073334
  batch 800 loss: 0.7632945156097413
  batch 850 loss: 0.7849118876457214
  batch 900 loss: 0.7436178755760193
LOSS train 0.74362 valid 0.81028, valid PER 25.40%
EPOCH 6:
  batch 50 loss: 0.6662696254253387
  batch 100 loss: 0.6714322972297668
  batch 150 loss: 0.6682385480403901
  batch 200 loss: 0.6532492464780808
  batch 250 loss: 0.65900186419487
  batch 300 loss: 0.6998862117528916
  batch 350 loss: 0.696590758562088
  batch 400 loss: 0.7003457313776016
  batch 450 loss: 0.7025667208433152
  batch 500 loss: 0.6683372515439987
  batch 550 loss: 0.6866896915435791
  batch 600 loss: 0.6700843942165374
  batch 650 loss: 0.6392575788497925
  batch 700 loss: 0.6665762001276017
  batch 750 loss: 0.6922261679172516
  batch 800 loss: 0.7001605290174484
  batch 850 loss: 0.7107172739505768
  batch 900 loss: 0.6974886614084244
LOSS train 0.69749 valid 0.78913, valid PER 24.52%
EPOCH 7:
  batch 50 loss: 0.5832771438360215
  batch 100 loss: 0.6435657632350922
  batch 150 loss: 0.6000992780923844
  batch 200 loss: 0.6152577883005143
  batch 250 loss: 0.6664987725019454
  batch 300 loss: 0.6260578638315201
  batch 350 loss: 0.6659205198287964
  batch 400 loss: 0.6156284892559052
  batch 450 loss: 0.6256039261817932
  batch 500 loss: 0.6150992351770401
  batch 550 loss: 0.6255389887094498
  batch 600 loss: 0.632611363530159
  batch 650 loss: 0.6429904562234878
  batch 700 loss: 0.6722397828102111
  batch 750 loss: 0.6525849056243896
  batch 800 loss: 0.6302977949380875
  batch 850 loss: 0.6343462616205215
  batch 900 loss: 0.6625410187244415
LOSS train 0.66254 valid 0.77841, valid PER 24.34%
EPOCH 8:
  batch 50 loss: 0.5757646191120148
  batch 100 loss: 0.5591134077310562
  batch 150 loss: 0.5886970788240433
  batch 200 loss: 0.5601112407445907
  batch 250 loss: 0.5753922235965728
  batch 300 loss: 0.55295734167099
  batch 350 loss: 0.5875400948524475
  batch 400 loss: 0.5930260235071182
  batch 450 loss: 0.6062825238704681
  batch 500 loss: 0.5834303903579712
  batch 550 loss: 0.6084654653072357
  batch 600 loss: 0.5841408956050873
  batch 650 loss: 0.6243433791399002
  batch 700 loss: 0.6291905856132507
  batch 750 loss: 0.6441678494215012
  batch 800 loss: 0.6702796214818955
  batch 850 loss: 0.6221368873119354
  batch 900 loss: 0.6177707749605179
LOSS train 0.61777 valid 0.78871, valid PER 24.16%
EPOCH 9:
  batch 50 loss: 0.5250528752803802
  batch 100 loss: 0.5282948678731918
  batch 150 loss: 0.5508581656217575
  batch 200 loss: 0.5562118470668793
  batch 250 loss: 0.5155706578493118
  batch 300 loss: 0.5706293588876724
  batch 350 loss: 0.5207939040660858
  batch 400 loss: 0.5510665374994278
  batch 450 loss: 0.5812184715270996
  batch 500 loss: 0.5721032464504242
  batch 550 loss: 0.561969929933548
  batch 600 loss: 0.5763619250059128
  batch 650 loss: 0.6015363436937332
  batch 700 loss: 0.5661399435997009
  batch 750 loss: 0.5735144406557083
  batch 800 loss: 0.6106408756971359
  batch 850 loss: 0.582523313164711
  batch 900 loss: 0.5424112659692765
LOSS train 0.54241 valid 0.74385, valid PER 22.98%
EPOCH 10:
  batch 50 loss: 0.4919699549674988
  batch 100 loss: 0.47664285004138945
  batch 150 loss: 0.48619456887245177
  batch 200 loss: 0.4897055333852768
  batch 250 loss: 0.4963386785984039
  batch 300 loss: 0.5042974692583084
  batch 350 loss: 0.5042209428548813
  batch 400 loss: 0.478855362534523
  batch 450 loss: 0.49325705707073214
  batch 500 loss: 0.4984572738409042
  batch 550 loss: 0.5027336806058884
  batch 600 loss: 0.5174510365724564
  batch 650 loss: 0.5231341171264648
  batch 700 loss: 0.523715900182724
  batch 750 loss: 0.5531819742918015
  batch 800 loss: 0.5476883149147034
  batch 850 loss: 0.5158757823705673
  batch 900 loss: 0.5333810365200042
LOSS train 0.53338 valid 0.75733, valid PER 22.51%
EPOCH 11:
  batch 50 loss: 0.4700870221853256
  batch 100 loss: 0.44208941161632537
  batch 150 loss: 0.44838437765836714
  batch 200 loss: 0.4259823399782181
  batch 250 loss: 0.4632295525074005
  batch 300 loss: 0.44173448503017426
  batch 350 loss: 0.559389676451683
  batch 400 loss: 0.5103374135494232
  batch 450 loss: 0.4909613811969757
  batch 500 loss: 0.5000746065378189
  batch 550 loss: 0.5100067538022995
  batch 600 loss: 0.4840721482038498
  batch 650 loss: 0.5086151909828186
  batch 700 loss: 0.5286642748117447
  batch 750 loss: 0.4941800332069397
  batch 800 loss: 0.5008309534192086
  batch 850 loss: 0.4969246125221252
  batch 900 loss: 0.5140466642379761
LOSS train 0.51405 valid 0.75165, valid PER 22.74%
EPOCH 12:
  batch 50 loss: 0.4049295872449875
  batch 100 loss: 0.40513174593448636
  batch 150 loss: 0.4151578840613365
  batch 200 loss: 0.4285085141658783
  batch 250 loss: 0.42709099560976027
  batch 300 loss: 0.42891568422317505
  batch 350 loss: 0.4249573981761932
  batch 400 loss: 0.46918385207653046
  batch 450 loss: 0.4821652030944824
  batch 500 loss: 0.476006293296814
  batch 550 loss: 0.4546705660223961
  batch 600 loss: 0.4570292776823044
  batch 650 loss: 0.46160758793354034
  batch 700 loss: 0.48212254971265794
  batch 750 loss: 0.47864526867866514
  batch 800 loss: 0.46867876172065737
  batch 850 loss: 0.489214688539505
  batch 900 loss: 0.5041830921173096
LOSS train 0.50418 valid 0.75457, valid PER 22.36%
EPOCH 13:
  batch 50 loss: 0.3638071009516716
  batch 100 loss: 0.4194014635682106
  batch 150 loss: 0.4070451211929321
  batch 200 loss: 0.37378478437662127
  batch 250 loss: 0.4007231992483139
  batch 300 loss: 0.4174370563030243
  batch 350 loss: 0.40633281677961347
  batch 400 loss: 0.41787209957838056
  batch 450 loss: 0.4242131227254868
  batch 500 loss: 0.4346849662065506
  batch 550 loss: 0.45071173191070557
  batch 600 loss: 0.4351255288720131
  batch 650 loss: 0.43728582948446276
  batch 700 loss: 0.4762575474381447
  batch 750 loss: 0.4575247409939766
  batch 800 loss: 0.5045132362842559
  batch 850 loss: 0.48197032064199447
  batch 900 loss: 0.49059937000274656
LOSS train 0.49060 valid 0.75359, valid PER 22.08%
EPOCH 14:
  batch 50 loss: 0.38733099728822706
  batch 100 loss: 0.35974357098340987
  batch 150 loss: 0.372280715405941
  batch 200 loss: 0.37238996356725695
  batch 250 loss: 0.38213730335235596
  batch 300 loss: 0.40015799343585967
  batch 350 loss: 0.40999073326587676
  batch 400 loss: 0.45430726915597913
  batch 450 loss: 0.42345754444599154
  batch 500 loss: 0.43593594670295716
  batch 550 loss: 0.43805699318647384
  batch 600 loss: 0.4062095022201538
  batch 650 loss: 0.42324987292289734
  batch 700 loss: 0.4117364725470543
  batch 750 loss: 0.4294270262122154
  batch 800 loss: 0.4253351745009422
  batch 850 loss: 0.43075295686721804
  batch 900 loss: 0.43217628955841064
LOSS train 0.43218 valid 0.74966, valid PER 21.45%
EPOCH 15:
  batch 50 loss: 0.3332612031698227
  batch 100 loss: 0.34781478583812714
  batch 150 loss: 0.35501624196767806
  batch 200 loss: 0.3689052975177765
  batch 250 loss: 0.38569399148225786
  batch 300 loss: 0.36252567619085313
  batch 350 loss: 0.37706647217273714
  batch 400 loss: 0.3695653200149536
  batch 450 loss: 0.36440463483333585
  batch 500 loss: 0.36259714514017105
  batch 550 loss: 0.41411644726991653
  batch 600 loss: 0.41495140016078946
  batch 650 loss: 0.39119045704603195
  batch 700 loss: 0.3950594028830528
  batch 750 loss: 0.4076537656784058
  batch 800 loss: 0.3888533446192741
  batch 850 loss: 0.4019763475656509
  batch 900 loss: 0.38841666132211683
LOSS train 0.38842 valid 0.76214, valid PER 22.10%
EPOCH 16:
  batch 50 loss: 0.33000605791807175
  batch 100 loss: 0.3205420619249344
  batch 150 loss: 0.3407034820318222
  batch 200 loss: 0.36040423810482025
  batch 250 loss: 0.3583397090435028
  batch 300 loss: 0.3515073385834694
  batch 350 loss: 0.3667469820380211
  batch 400 loss: 0.3651150271296501
  batch 450 loss: 0.366375986635685
  batch 500 loss: 0.37923367619514464
  batch 550 loss: 0.3656452807784081
  batch 600 loss: 0.38431177616119383
  batch 650 loss: 0.40143038749694826
  batch 700 loss: 0.35889660388231276
  batch 750 loss: 0.36520550727844237
  batch 800 loss: 0.3777596932649612
  batch 850 loss: 0.3722094306349754
  batch 900 loss: 0.3983384600281715
LOSS train 0.39834 valid 0.80487, valid PER 21.80%
EPOCH 17:
  batch 50 loss: 0.34206133633852004
  batch 100 loss: 0.2860625910758972
  batch 150 loss: 0.327846961915493
  batch 200 loss: 0.31227987974882127
  batch 250 loss: 0.32341177880764005
  batch 300 loss: 0.3331661057472229
  batch 350 loss: 0.37274057149887085
  batch 400 loss: 0.36333103746175766
  batch 450 loss: 0.3505715420842171
  batch 500 loss: 0.3866245624423027
  batch 550 loss: 0.3731788831949234
  batch 600 loss: 0.387766994535923
  batch 650 loss: 0.37063054978847504
  batch 700 loss: 0.38650286436080933
  batch 750 loss: 0.36300033748149874
  batch 800 loss: 0.37842335045337677
  batch 850 loss: 0.3647905474901199
  batch 900 loss: 0.3665160810947418
LOSS train 0.36652 valid 0.80687, valid PER 21.94%
EPOCH 18:
  batch 50 loss: 0.3052400630712509
  batch 100 loss: 0.30819340616464613
  batch 150 loss: 0.3364782339334488
  batch 200 loss: 0.3134587675333023
  batch 250 loss: 0.31103758633136747
  batch 300 loss: 0.32905982702970504
  batch 350 loss: 0.3347842308878899
  batch 400 loss: 0.34430356562137604
  batch 450 loss: 0.36278959214687345
  batch 500 loss: 0.3666064038872719
  batch 550 loss: 0.38451926738023756
  batch 600 loss: 0.38062316328287127
  batch 650 loss: 0.36032605439424514
  batch 700 loss: 0.3888395804166794
  batch 750 loss: 0.3689609098434448
  batch 800 loss: 0.3752562779188156
  batch 850 loss: 0.36583260625600816
  batch 900 loss: 0.3927643349766731
LOSS train 0.39276 valid 0.80024, valid PER 22.16%
EPOCH 19:
  batch 50 loss: 0.3095484045147896
  batch 100 loss: 0.3051962211728096
  batch 150 loss: 0.3019306318461895
  batch 200 loss: 0.3108529070019722
  batch 250 loss: 0.3403215003013611
  batch 300 loss: 0.3422455134987831
  batch 350 loss: 0.3315810015797615
  batch 400 loss: 0.32611009776592254
  batch 450 loss: 0.30262777745723723
  batch 500 loss: 0.3249362948536873
  batch 550 loss: 0.3569621542096138
  batch 600 loss: 0.3496653264760971
  batch 650 loss: 0.3642157843708992
  batch 700 loss: 0.3801839306950569
  batch 750 loss: 0.33449755638837814
  batch 800 loss: 0.3509136500954628
  batch 850 loss: 0.34998367697000504
  batch 900 loss: 0.35049975037574765
LOSS train 0.35050 valid 0.78849, valid PER 21.91%
EPOCH 20:
  batch 50 loss: 0.26806450575590135
  batch 100 loss: 0.2865390095114708
  batch 150 loss: 0.2919550105929375
  batch 200 loss: 0.3006602710485458
  batch 250 loss: 0.2927098819613457
  batch 300 loss: 0.29566836446523664
  batch 350 loss: 0.29597918927669525
  batch 400 loss: 0.31815997570753096
  batch 450 loss: 0.2963392777740955
  batch 500 loss: 0.3345153707265854
  batch 550 loss: 0.3306533193588257
  batch 600 loss: 0.31941026747226714
  batch 650 loss: 0.31640090614557265
  batch 700 loss: 0.335518316924572
  batch 750 loss: 0.3191621345281601
  batch 800 loss: 0.35270178705453875
  batch 850 loss: 0.35204931497573855
  batch 900 loss: 0.3286360776424408
LOSS train 0.32864 valid 0.81922, valid PER 21.86%
[1.2235837602615356, 0.9496627008914947, 0.8980614984035492, 0.8307390451431275, 0.7436178755760193, 0.6974886614084244, 0.6625410187244415, 0.6177707749605179, 0.5424112659692765, 0.5333810365200042, 0.5140466642379761, 0.5041830921173096, 0.49059937000274656, 0.43217628955841064, 0.38841666132211683, 0.3983384600281715, 0.3665160810947418, 0.3927643349766731, 0.35049975037574765, 0.3286360776424408]
[1.1746070384979248, 0.9846448302268982, 0.9134215116500854, 0.858250617980957, 0.8102842569351196, 0.789132297039032, 0.7784059047698975, 0.7887116074562073, 0.7438473105430603, 0.7573251128196716, 0.7516452670097351, 0.7545716166496277, 0.7535861730575562, 0.749664843082428, 0.7621377110481262, 0.8048667907714844, 0.8068670630455017, 0.8002406358718872, 0.7884865403175354, 0.8192219138145447]
Training finished in 21.0 minutes.
Model saved to checkpoints/20230125_110309/model_9
Loading model from checkpoints/20230125_110309/model_9
