Namespace(seed=123, train_json='train_fbank_speeds.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=4, fbank_dims=23, model_dims=256, concat=1, lr=0.0005, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.4, beta1=0.9, beta2=0.999, exp=0.5)
Total number of model parameters is 5326888
EPOCH 1:
  batch 50 loss: 6.143821468353272
  batch 100 loss: 3.316286792755127
  batch 150 loss: 3.296208086013794
  batch 200 loss: 3.2729871320724486
  batch 250 loss: 3.246477904319763
  batch 300 loss: 3.1890366744995116
  batch 350 loss: 3.025814118385315
  batch 400 loss: 2.839364457130432
  batch 450 loss: 2.7317310571670532
  batch 500 loss: 2.5707039260864257
  batch 550 loss: 2.4285441541671755
  batch 600 loss: 2.3377569055557252
  batch 650 loss: 2.1733151650428773
  batch 700 loss: 2.1013348293304444
  batch 750 loss: 1.9969001936912536
  batch 800 loss: 2.0000591015815736
  batch 850 loss: 1.8827370834350585
  batch 900 loss: 1.836122419834137
LOSS train 1.83612 valid 1.73196, valid PER 51.49%
loss: 0.00025
EPOCH 2:
  batch 50 loss: 1.769848222732544
  batch 100 loss: 1.6894863796234132
  batch 150 loss: 1.604704520702362
  batch 200 loss: 1.61822913646698
  batch 250 loss: 1.5655158543586731
  batch 300 loss: 1.4991892409324645
  batch 350 loss: 1.4979174041748047
  batch 400 loss: 1.4437673997879028
  batch 450 loss: 1.4191751861572266
  batch 500 loss: 1.3902819919586182
  batch 550 loss: 1.3864640712738037
  batch 600 loss: 1.3466484451293945
  batch 650 loss: 1.2624608147144318
  batch 700 loss: 1.2895723032951354
  batch 750 loss: 1.2770522713661194
  batch 800 loss: 1.2800342535972595
  batch 850 loss: 1.2562006068229676
  batch 900 loss: 1.2198071455955506
LOSS train 1.21981 valid 1.17644, valid PER 36.69%
loss: 0.000125
EPOCH 3:
  batch 50 loss: 1.1677430045604706
  batch 100 loss: 1.2180719602108
  batch 150 loss: 1.2191255700588226
  batch 200 loss: 1.1456722581386567
  batch 250 loss: 1.1280797505378723
  batch 300 loss: 1.1256032407283783
  batch 350 loss: 1.1826345074176787
  batch 400 loss: 1.156407606601715
  batch 450 loss: 1.1112671780586243
  batch 500 loss: 1.1174726593494415
  batch 550 loss: 1.0783529210090637
  batch 600 loss: 1.0715844309329987
  batch 650 loss: 1.0757759821414947
  batch 700 loss: 1.0877077376842499
  batch 750 loss: 1.063447870016098
  batch 800 loss: 1.0701319861412049
  batch 850 loss: 1.0216280901432038
  batch 900 loss: 1.061232600212097
LOSS train 1.06123 valid 0.99263, valid PER 31.64%
loss: 6.25e-05
EPOCH 4:
  batch 50 loss: 1.0286790227890015
  batch 100 loss: 0.9808976757526398
  batch 150 loss: 0.9975327467918396
  batch 200 loss: 0.9685326540470123
  batch 250 loss: 0.9627002394199371
  batch 300 loss: 0.9572069311141967
  batch 350 loss: 0.9310610032081604
  batch 400 loss: 0.9257811057567596
  batch 450 loss: 0.932969297170639
  batch 500 loss: 1.0132749891281128
  batch 550 loss: 0.9359273862838745
  batch 600 loss: 0.9170965826511384
  batch 650 loss: 0.9703714382648468
  batch 700 loss: 0.9945818293094635
  batch 750 loss: 0.9386675012111664
  batch 800 loss: 0.913916882276535
  batch 850 loss: 0.935776813030243
  batch 900 loss: 0.9412003660202026
LOSS train 0.94120 valid 0.91343, valid PER 28.62%
loss: 3.125e-05
EPOCH 5:
  batch 50 loss: 0.8923618686199188
  batch 100 loss: 0.9020842146873475
  batch 150 loss: 0.9127539336681366
  batch 200 loss: 0.921051766872406
  batch 250 loss: 0.9024436271190643
  batch 300 loss: 0.8871685063838959
  batch 350 loss: 0.8422177267074585
  batch 400 loss: 0.8221363615989685
  batch 450 loss: 0.8363269603252411
  batch 500 loss: 0.8514381796121597
  batch 550 loss: 0.8680921912193298
  batch 600 loss: 0.8824733352661133
  batch 650 loss: 0.8776942646503448
  batch 700 loss: 0.8546249592304229
  batch 750 loss: 0.8501287698745728
  batch 800 loss: 0.8925654232501984
  batch 850 loss: 0.8806421029567718
  batch 900 loss: 0.8290131932497025
LOSS train 0.82901 valid 0.83189, valid PER 26.28%
loss: 1.5625e-05
EPOCH 6:
  batch 50 loss: 0.8274170768260956
  batch 100 loss: 0.8179939961433411
  batch 150 loss: 0.7928719556331635
  batch 200 loss: 0.8125649392604828
  batch 250 loss: 0.8089033651351929
  batch 300 loss: 0.8265717005729676
  batch 350 loss: 0.8093919670581817
  batch 400 loss: 0.8148122167587281
  batch 450 loss: 0.8356510984897614
  batch 500 loss: 0.7717174410820007
  batch 550 loss: 0.7840168607234955
  batch 600 loss: 0.7753961420059204
  batch 650 loss: 0.7479004549980164
  batch 700 loss: 0.7922904860973358
  batch 750 loss: 0.802019875049591
  batch 800 loss: 0.7952423071861268
  batch 850 loss: 0.8090265047550201
  batch 900 loss: 0.7880369555950165
LOSS train 0.78804 valid 0.76537, valid PER 24.50%
loss: 7.8125e-06
EPOCH 7:
  batch 50 loss: 0.7318058800697327
  batch 100 loss: 0.7742570173740387
  batch 150 loss: 0.7333838284015656
  batch 200 loss: 0.7210437858104706
  batch 250 loss: 0.743349404335022
  batch 300 loss: 0.7044964784383774
  batch 350 loss: 0.8087493550777435
  batch 400 loss: 0.7645317304134369
  batch 450 loss: 0.7870777058601379
  batch 500 loss: 0.7515179598331452
  batch 550 loss: 0.7375311082601548
  batch 600 loss: 0.7523565936088562
  batch 650 loss: 0.7195272588729859
  batch 700 loss: 0.7304280710220337
  batch 750 loss: 0.7453364723920822
  batch 800 loss: 0.7464055722951889
  batch 850 loss: 0.7415539336204529
  batch 900 loss: 0.7504919934272766
LOSS train 0.75049 valid 0.74728, valid PER 24.28%
loss: 3.90625e-06
EPOCH 8:
  batch 50 loss: 0.6982876741886139
  batch 100 loss: 0.6923028159141541
  batch 150 loss: 0.6994070070981979
  batch 200 loss: 0.6768374222517014
  batch 250 loss: 0.6690803402662278
  batch 300 loss: 0.6611301374435424
  batch 350 loss: 0.6825155633687973
  batch 400 loss: 0.6619921755790711
  batch 450 loss: 0.7173735201358795
  batch 500 loss: 0.6730143964290619
  batch 550 loss: 0.6787444597482681
  batch 600 loss: 0.6787167739868164
  batch 650 loss: 0.7151937115192414
  batch 700 loss: 0.6972781348228455
  batch 750 loss: 0.711224518418312
  batch 800 loss: 0.7130970329046249
  batch 850 loss: 0.6797135788202285
  batch 900 loss: 0.6725540620088577
LOSS train 0.67255 valid 0.73578, valid PER 23.16%
loss: 1.953125e-06
EPOCH 9:
  batch 50 loss: 0.6276227331161499
  batch 100 loss: 0.6196682834625244
  batch 150 loss: 0.6239004647731781
  batch 200 loss: 0.6129663461446762
  batch 250 loss: 0.600034077167511
  batch 300 loss: 0.6425443267822266
  batch 350 loss: 0.6126455634832382
  batch 400 loss: 0.6610031390190124
  batch 450 loss: 0.6595005863904952
  batch 500 loss: 0.6447257554531097
  batch 550 loss: 0.6472592025995254
  batch 600 loss: 0.6383147436380386
  batch 650 loss: 0.67048220038414
  batch 700 loss: 0.639782480597496
  batch 750 loss: 0.6533794182538987
  batch 800 loss: 0.6610983687639237
  batch 850 loss: 0.6270670586824417
  batch 900 loss: 0.6248214966058732
LOSS train 0.62482 valid 0.69259, valid PER 22.27%
loss: 9.765625e-07
EPOCH 10:
  batch 50 loss: 0.5868650239706039
  batch 100 loss: 0.604912965297699
  batch 150 loss: 0.6160838210582733
  batch 200 loss: 0.6141615915298462
  batch 250 loss: 0.6123334521055221
  batch 300 loss: 0.5998190587759018
  batch 350 loss: 0.5983614629507065
  batch 400 loss: 0.5777873194217682
  batch 450 loss: 0.5948821008205414
  batch 500 loss: 0.5852134555578232
  batch 550 loss: 0.6062843561172485
  batch 600 loss: 0.6032610094547272
  batch 650 loss: 0.6362196272611618
  batch 700 loss: 0.5958878248929977
  batch 750 loss: 0.614477042555809
  batch 800 loss: 0.6128064471483231
  batch 850 loss: 0.6047442483901978
  batch 900 loss: 0.5990396761894226
LOSS train 0.59904 valid 0.70744, valid PER 22.52%
EPOCH 11:
  batch 50 loss: 0.5690754896402359
  batch 100 loss: 0.537783796787262
  batch 150 loss: 0.5561414664983749
  batch 200 loss: 0.5122944360971451
  batch 250 loss: 0.5433251368999481
  batch 300 loss: 0.5770142835378647
  batch 350 loss: 0.5781529712677002
  batch 400 loss: 0.556323721408844
  batch 450 loss: 0.5410056936740876
  batch 500 loss: 0.5519769781827927
  batch 550 loss: 0.5663703405857086
  batch 600 loss: 0.5766308224201202
  batch 650 loss: 0.5751703280210495
  batch 700 loss: 0.6506030941009522
  batch 750 loss: 0.5875992524623871
  batch 800 loss: 0.5924673295021057
  batch 850 loss: 0.5746665573120118
  batch 900 loss: 0.5893435502052307
LOSS train 0.58934 valid 0.69095, valid PER 21.31%
loss: 4.8828125e-07
EPOCH 12:
  batch 50 loss: 0.5077464699745178
  batch 100 loss: 0.5153484421968461
  batch 150 loss: 0.526496132016182
  batch 200 loss: 0.5322042292356491
  batch 250 loss: 0.5733937394618988
  batch 300 loss: 0.5706726467609405
  batch 350 loss: 0.5327210956811905
  batch 400 loss: 0.5555347859859466
  batch 450 loss: 0.5307773661613464
  batch 500 loss: 0.5530768877267838
  batch 550 loss: 0.5401993057131768
  batch 600 loss: 0.543977170586586
  batch 650 loss: 0.5502750980854034
  batch 700 loss: 0.5565390300750732
  batch 750 loss: 0.5454696613550186
  batch 800 loss: 0.5238976699113845
  batch 850 loss: 0.5616046917438507
  batch 900 loss: 0.5715965396165847
LOSS train 0.57160 valid 0.68196, valid PER 21.36%
loss: 2.44140625e-07
EPOCH 13:
  batch 50 loss: 0.4686211025714874
  batch 100 loss: 0.48038774013519286
  batch 150 loss: 0.48615010261535646
  batch 200 loss: 0.494931156039238
  batch 250 loss: 0.4991435182094574
  batch 300 loss: 0.5264108482003212
  batch 350 loss: 0.498936203122139
  batch 400 loss: 0.5223547631502151
  batch 450 loss: 0.5077060484886169
  batch 500 loss: 0.5089262640476226
  batch 550 loss: 0.5475394022464752
  batch 600 loss: 0.5204503685235977
  batch 650 loss: 0.5104453378915786
  batch 700 loss: 0.529355817437172
  batch 750 loss: 0.522830576300621
  batch 800 loss: 0.5298010247945786
  batch 850 loss: 0.48234564006328584
  batch 900 loss: 0.5157352983951569
LOSS train 0.51574 valid 0.66417, valid PER 20.45%
loss: 1.220703125e-07
EPOCH 14:
  batch 50 loss: 0.4355773574113846
  batch 100 loss: 0.45040928542613984
  batch 150 loss: 0.4537431272864342
  batch 200 loss: 0.46438779413700104
  batch 250 loss: 0.45181951522827146
  batch 300 loss: 0.4527005761861801
  batch 350 loss: 0.4686353951692581
  batch 400 loss: 0.4939603704214096
  batch 450 loss: 0.44194540023803713
  batch 500 loss: 0.47589286625385285
  batch 550 loss: 0.4800960725545883
  batch 600 loss: 0.4739461815357208
  batch 650 loss: 0.46947241127490996
  batch 700 loss: 0.4997218370437622
  batch 750 loss: 0.500840340256691
  batch 800 loss: 0.4985290864109993
  batch 850 loss: 0.498344789147377
  batch 900 loss: 0.5068563455343247
LOSS train 0.50686 valid 0.66573, valid PER 20.59%
EPOCH 15:
  batch 50 loss: 0.4293664538860321
  batch 100 loss: 0.4553441607952118
  batch 150 loss: 0.4624845597147942
  batch 200 loss: 0.46702204704284667
  batch 250 loss: 0.48950852423906327
  batch 300 loss: 0.48906331598758696
  batch 350 loss: 0.4618745797872543
  batch 400 loss: 0.4693363583087921
  batch 450 loss: 0.46752307295799256
  batch 500 loss: 0.43718848943710326
  batch 550 loss: 0.4904975652694702
  batch 600 loss: 0.4901549953222275
  batch 650 loss: 0.4742111200094223
  batch 700 loss: 0.45051545679569244
  batch 750 loss: 0.47051188588142395
  batch 800 loss: 0.4457476100325584
  batch 850 loss: 0.44193058252334594
  batch 900 loss: 0.43246273130178453
LOSS train 0.43246 valid 0.66418, valid PER 20.62%
EPOCH 16:
  batch 50 loss: 0.41659226596355436
  batch 100 loss: 0.4018664872646332
  batch 150 loss: 0.4051831740140915
  batch 200 loss: 0.4273319947719574
  batch 250 loss: 0.4256631469726562
  batch 300 loss: 0.4214840567111969
  batch 350 loss: 0.4202880936861038
  batch 400 loss: 0.41713544249534606
  batch 450 loss: 0.4295600146055222
  batch 500 loss: 0.4367247223854065
  batch 550 loss: 0.4399007758498192
  batch 600 loss: 0.4534851258993149
  batch 650 loss: 0.4491144722700119
  batch 700 loss: 0.39780086010694504
  batch 750 loss: 0.4342823123931885
  batch 800 loss: 0.4270188224315643
  batch 850 loss: 0.4163547012209892
  batch 900 loss: 0.44437326341867445
LOSS train 0.44437 valid 0.67398, valid PER 20.46%
EPOCH 17:
  batch 50 loss: 0.3994431576132774
  batch 100 loss: 0.3693146276473999
  batch 150 loss: 0.41528139799833297
  batch 200 loss: 0.3827446869015694
  batch 250 loss: 0.3935520756244659
  batch 300 loss: 0.3792983311414719
  batch 350 loss: 0.41056874036788943
  batch 400 loss: 0.4208147844672203
  batch 450 loss: 0.392036554813385
  batch 500 loss: 0.41226433545351027
  batch 550 loss: 0.4019958510994911
  batch 600 loss: 0.4065209811925888
  batch 650 loss: 0.4084406340122223
  batch 700 loss: 0.4282387831807137
  batch 750 loss: 0.416055238544941
  batch 800 loss: 0.42760693430900576
  batch 850 loss: 0.41011504173278807
  batch 900 loss: 0.4086681333184242
LOSS train 0.40867 valid 0.66862, valid PER 20.07%
[1.836122419834137, 1.2198071455955506, 1.061232600212097, 0.9412003660202026, 0.8290131932497025, 0.7880369555950165, 0.7504919934272766, 0.6725540620088577, 0.6248214966058732, 0.5990396761894226, 0.5893435502052307, 0.5715965396165847, 0.5157352983951569, 0.5068563455343247, 0.43246273130178453, 0.44437326341867445, 0.4086681333184242]
[1.7319611310958862, 1.176440954208374, 0.9926326274871826, 0.9134337306022644, 0.8318942785263062, 0.7653729915618896, 0.7472845911979675, 0.7357763648033142, 0.6925874948501587, 0.7074412107467651, 0.6909506320953369, 0.6819627285003662, 0.6641712784767151, 0.6657313108444214, 0.6641781330108643, 0.6739778518676758, 0.6686214208602905]
Training finished in 18.0 minutes.
Model saved to checkpoints/20230126_131054/model_13
Loading model from checkpoints/20230126_131054/model_13
SUB: 14.38%, DEL: 5.60%, INS: 2.84%, COR: 80.02%, PER: 22.81%
