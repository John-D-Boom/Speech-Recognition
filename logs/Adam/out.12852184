Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.0005, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.2, beta1=0.9, beta2=0.999)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 13.516219849586486
  batch 100 loss: 3.333991060256958
  batch 150 loss: 3.2816712999343873
  batch 200 loss: 3.2620486879348753
  batch 250 loss: 3.239572968482971
  batch 300 loss: 3.2027305650711058
  batch 350 loss: 3.1640289783477784
  batch 400 loss: 3.1182221078872683
  batch 450 loss: 3.0320109367370605
  batch 500 loss: 2.929304127693176
  batch 550 loss: 2.83537127494812
  batch 600 loss: 2.7518816804885864
  batch 650 loss: 2.674004259109497
  batch 700 loss: 2.631869955062866
  batch 750 loss: 2.5672869157791136
  batch 800 loss: 2.524393820762634
  batch 850 loss: 2.4704625606536865
  batch 900 loss: 2.401198358535767
LOSS train 2.40120 valid 2.35903, valid PER 84.99%
EPOCH 2:
  batch 50 loss: 2.3683681678771973
  batch 100 loss: 2.3065882682800294
  batch 150 loss: 2.210855484008789
  batch 200 loss: 2.1911731791496276
  batch 250 loss: 2.201470048427582
  batch 300 loss: 2.1320771193504333
  batch 350 loss: 2.107589392662048
  batch 400 loss: 2.076113977432251
  batch 450 loss: 2.0713889026641845
  batch 500 loss: 2.044472780227661
  batch 550 loss: 2.020656232833862
  batch 600 loss: 1.9789756298065186
  batch 650 loss: 1.9416970109939575
  batch 700 loss: 1.9440011096000671
  batch 750 loss: 1.9121754384040832
  batch 800 loss: 1.863852984905243
  batch 850 loss: 1.886858925819397
  batch 900 loss: 1.844283857345581
LOSS train 1.84428 valid 1.82597, valid PER 73.53%
EPOCH 3:
  batch 50 loss: 1.772010247707367
  batch 100 loss: 1.8284444427490234
  batch 150 loss: 1.8075907182693483
  batch 200 loss: 1.7388166856765748
  batch 250 loss: 1.7331489086151124
  batch 300 loss: 1.7343880081176757
  batch 350 loss: 1.7372392416000366
  batch 400 loss: 1.7180544328689575
  batch 450 loss: 1.695520203113556
  batch 500 loss: 1.6501575207710266
  batch 550 loss: 1.658253662586212
  batch 600 loss: 1.6168143987655639
  batch 650 loss: 1.6375164556503297
  batch 700 loss: 1.6021934366226196
  batch 750 loss: 1.640655324459076
  batch 800 loss: 1.6329515504837036
  batch 850 loss: 1.5869045734405518
  batch 900 loss: 1.591159484386444
LOSS train 1.59116 valid 1.56587, valid PER 62.41%
EPOCH 4:
  batch 50 loss: 1.588947377204895
  batch 100 loss: 1.5263707971572875
  batch 150 loss: 1.5331590700149536
  batch 200 loss: 1.539088990688324
  batch 250 loss: 1.5613351917266847
  batch 300 loss: 1.5350204253196715
  batch 350 loss: 1.5269080018997192
  batch 400 loss: 1.4864295649528503
  batch 450 loss: 1.4920637178421021
  batch 500 loss: 1.5522678542137145
  batch 550 loss: 1.4716933465003967
  batch 600 loss: 1.473887255191803
  batch 650 loss: 1.5112059140205383
  batch 700 loss: 1.527709276676178
  batch 750 loss: 1.4622573232650757
  batch 800 loss: 1.474396560192108
  batch 850 loss: 1.4450472235679626
  batch 900 loss: 1.4382091784477233
LOSS train 1.43821 valid 1.45912, valid PER 54.85%
EPOCH 5:
  batch 50 loss: 1.4601908397674561
  batch 100 loss: 1.4334673857688904
  batch 150 loss: 1.446702184677124
  batch 200 loss: 1.4459260869026185
  batch 250 loss: 1.4199358868598937
  batch 300 loss: 1.4285764074325562
  batch 350 loss: 1.3823115420341492
  batch 400 loss: 1.3803482151031494
  batch 450 loss: 1.3801580333709718
  batch 500 loss: 1.3619735860824584
  batch 550 loss: 1.4069683694839477
  batch 600 loss: 1.4108619260787965
  batch 650 loss: 1.3983809065818786
  batch 700 loss: 1.388622055053711
  batch 750 loss: 1.3708626770973205
  batch 800 loss: 1.4082952642440796
  batch 850 loss: 1.399849898815155
  batch 900 loss: 1.3512768983840941
LOSS train 1.35128 valid 1.36257, valid PER 51.21%
EPOCH 6:
  batch 50 loss: 1.3370775818824767
  batch 100 loss: 1.3727826690673828
  batch 150 loss: 1.3468461894989014
  batch 200 loss: 1.3087436950206757
  batch 250 loss: 1.3616944336891175
  batch 300 loss: 1.3539163994789123
  batch 350 loss: 1.352139139175415
  batch 400 loss: 1.3409715867042542
  batch 450 loss: 1.3513356375694274
  batch 500 loss: 1.31420285820961
  batch 550 loss: 1.3445794653892518
  batch 600 loss: 1.3152572751045226
  batch 650 loss: 1.2979476952552795
  batch 700 loss: 1.2808819901943207
  batch 750 loss: 1.329582691192627
  batch 800 loss: 1.325107092857361
  batch 850 loss: 1.3326062059402466
  batch 900 loss: 1.3392006397247314
LOSS train 1.33920 valid 1.31004, valid PER 48.36%
EPOCH 7:
  batch 50 loss: 1.276190277338028
  batch 100 loss: 1.328492157459259
  batch 150 loss: 1.2660419654846191
  batch 200 loss: 1.2611219334602355
  batch 250 loss: 1.3178679299354554
  batch 300 loss: 1.2825387823581695
  batch 350 loss: 1.2967498564720155
  batch 400 loss: 1.2669082331657409
  batch 450 loss: 1.2817320549488067
  batch 500 loss: 1.2754782247543335
  batch 550 loss: 1.2429817163944243
  batch 600 loss: 1.2742753195762635
  batch 650 loss: 1.246457635164261
  batch 700 loss: 1.2888384771347046
  batch 750 loss: 1.2537656819820404
  batch 800 loss: 1.2659802913665772
  batch 850 loss: 1.2290641641616822
  batch 900 loss: 1.276675226688385
LOSS train 1.27668 valid 1.26167, valid PER 45.55%
EPOCH 8:
  batch 50 loss: 1.25353639960289
  batch 100 loss: 1.2224455499649047
  batch 150 loss: 1.251099488735199
  batch 200 loss: 1.2421226394176483
  batch 250 loss: 1.224841674566269
  batch 300 loss: 1.1885819256305694
  batch 350 loss: 1.2092685115337372
  batch 400 loss: 1.2061578118801117
  batch 450 loss: 1.269915235042572
  batch 500 loss: 1.2374786961078643
  batch 550 loss: 1.223766793012619
  batch 600 loss: 1.1943225252628327
  batch 650 loss: 1.1926229310035705
  batch 700 loss: 1.2234720158576966
  batch 750 loss: 1.2320538640022278
  batch 800 loss: 1.223674784898758
  batch 850 loss: 1.1975388610363007
  batch 900 loss: 1.2060917270183564
LOSS train 1.20609 valid 1.22875, valid PER 43.03%
EPOCH 9:
  batch 50 loss: 1.1876526939868928
  batch 100 loss: 1.1573085951805115
  batch 150 loss: 1.1794923901557923
  batch 200 loss: 1.1668207728862763
  batch 250 loss: 1.1481740033626557
  batch 300 loss: 1.1749628376960755
  batch 350 loss: 1.1681774306297301
  batch 400 loss: 1.1967886471748352
  batch 450 loss: 1.1830101013183594
  batch 500 loss: 1.1814607214927673
  batch 550 loss: 1.1730656909942627
  batch 600 loss: 1.2130561399459838
  batch 650 loss: 1.192279839515686
  batch 700 loss: 1.160332864522934
  batch 750 loss: 1.1696060752868653
  batch 800 loss: 1.2087991893291474
  batch 850 loss: 1.1815072917938232
  batch 900 loss: 1.1498473501205444
LOSS train 1.14985 valid 1.18465, valid PER 40.93%
EPOCH 10:
  batch 50 loss: 1.1454563593864442
  batch 100 loss: 1.1556881892681121
  batch 150 loss: 1.189523309469223
  batch 200 loss: 1.1271523678302764
  batch 250 loss: 1.1444675183296205
  batch 300 loss: 1.1317247188091277
  batch 350 loss: 1.1293134260177613
  batch 400 loss: 1.0924851775169373
  batch 450 loss: 1.1100355660915375
  batch 500 loss: 1.1213694882392884
  batch 550 loss: 1.1460245203971864
  batch 600 loss: 1.1353408575057984
  batch 650 loss: 1.150287448167801
  batch 700 loss: 1.1706782603263854
  batch 750 loss: 1.159601868391037
  batch 800 loss: 1.1336662101745605
  batch 850 loss: 1.122963811159134
  batch 900 loss: 1.1121222734451295
LOSS train 1.11212 valid 1.15314, valid PER 38.58%
EPOCH 11:
  batch 50 loss: 1.116219289302826
  batch 100 loss: 1.0992782163619994
  batch 150 loss: 1.080737031698227
  batch 200 loss: 1.0921978449821472
  batch 250 loss: 1.0728853499889375
  batch 300 loss: 1.0879619789123536
  batch 350 loss: 1.1247839105129243
  batch 400 loss: 1.0854599618911742
  batch 450 loss: 1.0965048205852508
  batch 500 loss: 1.0901783835887908
  batch 550 loss: 1.1152407622337341
  batch 600 loss: 1.109139128923416
  batch 650 loss: 1.1147344517707825
  batch 700 loss: 1.1549813747406006
  batch 750 loss: 1.1043106055259704
  batch 800 loss: 1.099170686006546
  batch 850 loss: 1.100458573102951
  batch 900 loss: 1.1294432890415191
LOSS train 1.12944 valid 1.13326, valid PER 37.77%
EPOCH 12:
  batch 50 loss: 1.059005980491638
  batch 100 loss: 1.0327185130119323
  batch 150 loss: 1.0779676735401154
  batch 200 loss: 1.0816494798660279
  batch 250 loss: 1.0596315503120421
  batch 300 loss: 1.106791695356369
  batch 350 loss: 1.0773692440986633
  batch 400 loss: 1.0820002114772798
  batch 450 loss: 1.0750881588459016
  batch 500 loss: 1.083381073474884
  batch 550 loss: 1.0958867001533508
  batch 600 loss: 1.0691277408599853
  batch 650 loss: 1.0715563738346099
  batch 700 loss: 1.0630729115009308
  batch 750 loss: 1.0776061415672302
  batch 800 loss: 1.0544814240932465
  batch 850 loss: 1.065481996536255
  batch 900 loss: 1.0553383696079255
LOSS train 1.05534 valid 1.12667, valid PER 36.83%
EPOCH 13:
  batch 50 loss: 1.0304057848453523
  batch 100 loss: 1.0411226952075958
  batch 150 loss: 1.0598011994361878
  batch 200 loss: 1.0062784719467164
  batch 250 loss: 1.0233646535873413
  batch 300 loss: 1.0928515136241912
  batch 350 loss: 1.0196240520477295
  batch 400 loss: 1.0401176333427429
  batch 450 loss: 1.0690858626365662
  batch 500 loss: 1.0530158185958862
  batch 550 loss: 1.0795271694660187
  batch 600 loss: 1.0622053837776184
  batch 650 loss: 1.0536220502853393
  batch 700 loss: 1.055810751914978
  batch 750 loss: 1.00204399228096
  batch 800 loss: 1.0535383868217467
  batch 850 loss: 1.0414237761497498
  batch 900 loss: 1.0345718514919282
LOSS train 1.03457 valid 1.08233, valid PER 35.39%
EPOCH 14:
  batch 50 loss: 1.0159473896026612
  batch 100 loss: 1.0016431856155394
  batch 150 loss: 1.0082703733444214
  batch 200 loss: 1.0161150395870209
  batch 250 loss: 1.017646473646164
  batch 300 loss: 0.9909077906608581
  batch 350 loss: 1.0039119684696198
  batch 400 loss: 1.042716782093048
  batch 450 loss: 1.0081015968322753
  batch 500 loss: 0.9882567286491394
  batch 550 loss: 1.0400769484043122
  batch 600 loss: 0.9890165972709656
  batch 650 loss: 1.0607342720031738
  batch 700 loss: 1.0225610363483428
  batch 750 loss: 1.0119089245796205
  batch 800 loss: 1.0128628742694854
  batch 850 loss: 1.0471861004829406
  batch 900 loss: 1.0123373913764953
LOSS train 1.01234 valid 1.06488, valid PER 34.79%
EPOCH 15:
  batch 50 loss: 0.9780351042747497
  batch 100 loss: 0.9748946487903595
  batch 150 loss: 0.9697293090820313
  batch 200 loss: 1.0181330406665803
  batch 250 loss: 1.0123440277576448
  batch 300 loss: 0.9930302476882935
  batch 350 loss: 0.9779428565502166
  batch 400 loss: 0.9839383220672607
  batch 450 loss: 1.0120869719982146
  batch 500 loss: 1.00368589758873
  batch 550 loss: 1.040040168762207
  batch 600 loss: 1.0311821591854096
  batch 650 loss: 0.9798023211956024
  batch 700 loss: 0.9770916676521302
  batch 750 loss: 1.0093588423728943
  batch 800 loss: 1.0018453633785247
  batch 850 loss: 0.993733491897583
  batch 900 loss: 0.9250460588932037
LOSS train 0.92505 valid 1.05540, valid PER 34.50%
EPOCH 16:
  batch 50 loss: 0.9541991770267486
  batch 100 loss: 0.949643542766571
  batch 150 loss: 0.9622578859329224
  batch 200 loss: 0.9920257711410523
  batch 250 loss: 0.9536010813713074
  batch 300 loss: 0.9876146042346954
  batch 350 loss: 0.9836775314807892
  batch 400 loss: 0.9784452903270722
  batch 450 loss: 0.9946842181682587
  batch 500 loss: 0.9700857484340668
  batch 550 loss: 0.9560954904556275
  batch 600 loss: 1.0009649097919464
  batch 650 loss: 1.0028094840049744
  batch 700 loss: 0.9641646277904511
  batch 750 loss: 0.9983584916591645
  batch 800 loss: 0.970201485157013
  batch 850 loss: 0.9588729560375213
  batch 900 loss: 0.971634851694107
LOSS train 0.97163 valid 1.04626, valid PER 33.62%
EPOCH 17:
  batch 50 loss: 0.9652514505386353
  batch 100 loss: 0.9092091298103333
  batch 150 loss: 0.9707520627975463
  batch 200 loss: 0.9187555581331253
  batch 250 loss: 0.9586921274662018
  batch 300 loss: 0.9351569378376007
  batch 350 loss: 0.9589517211914063
  batch 400 loss: 0.9568258154392243
  batch 450 loss: 0.9576660132408142
  batch 500 loss: 0.9679331433773041
  batch 550 loss: 0.9558383643627166
  batch 600 loss: 0.9829231405258179
  batch 650 loss: 0.9508660686016083
  batch 700 loss: 0.9746464741230011
  batch 750 loss: 0.9155968487262726
  batch 800 loss: 0.9785479700565338
  batch 850 loss: 0.9662451672554017
  batch 900 loss: 0.95894975066185
LOSS train 0.95895 valid 1.03870, valid PER 33.26%
EPOCH 18:
  batch 50 loss: 0.9241229379177094
  batch 100 loss: 0.9047916376590729
  batch 150 loss: 0.9598785769939423
  batch 200 loss: 0.9357416403293609
  batch 250 loss: 0.9181453132629395
  batch 300 loss: 0.9512992298603058
  batch 350 loss: 0.9091705858707428
  batch 400 loss: 0.9305020761489868
  batch 450 loss: 0.9286931693553925
  batch 500 loss: 0.9399640107154846
  batch 550 loss: 0.9667282092571259
  batch 600 loss: 0.954985327720642
  batch 650 loss: 0.9038430213928222
  batch 700 loss: 0.9115860772132873
  batch 750 loss: 0.9484311628341675
  batch 800 loss: 0.9428618466854095
  batch 850 loss: 0.9465507853031159
  batch 900 loss: 0.9432882416248322
LOSS train 0.94329 valid 1.03414, valid PER 33.52%
EPOCH 19:
  batch 50 loss: 0.9090862441062927
  batch 100 loss: 0.9068543636798858
  batch 150 loss: 0.8885777008533478
  batch 200 loss: 0.9011926174163818
  batch 250 loss: 0.9372588789463043
  batch 300 loss: 0.9388823533058166
  batch 350 loss: 0.9436863005161286
  batch 400 loss: 0.9187203061580658
  batch 450 loss: 0.8771446180343628
  batch 500 loss: 0.9014955842494965
  batch 550 loss: 0.9271939671039582
  batch 600 loss: 0.939613070487976
  batch 650 loss: 0.9378774285316467
  batch 700 loss: 0.9618440008163452
  batch 750 loss: 0.9196706116199493
  batch 800 loss: 0.884146454334259
  batch 850 loss: 0.9241567885875702
  batch 900 loss: 0.9076235187053681
LOSS train 0.90762 valid 1.02177, valid PER 32.32%
EPOCH 20:
  batch 50 loss: 0.8458624124526978
  batch 100 loss: 0.89361297249794
  batch 150 loss: 0.9008523523807526
  batch 200 loss: 0.8911912560462951
  batch 250 loss: 0.9248661363124847
  batch 300 loss: 0.9177875363826752
  batch 350 loss: 0.8915031683444977
  batch 400 loss: 0.9032063996791839
  batch 450 loss: 0.8892785167694092
  batch 500 loss: 0.9044055104255676
  batch 550 loss: 0.9244308602809906
  batch 600 loss: 0.8764264333248138
  batch 650 loss: 0.9054434514045715
  batch 700 loss: 0.886372184753418
  batch 750 loss: 0.8829231882095336
  batch 800 loss: 0.9163190233707428
  batch 850 loss: 0.9185893750190735
  batch 900 loss: 0.8806647205352783
LOSS train 0.88066 valid 1.02035, valid PER 31.90%
[2.401198358535767, 1.844283857345581, 1.591159484386444, 1.4382091784477233, 1.3512768983840941, 1.3392006397247314, 1.276675226688385, 1.2060917270183564, 1.1498473501205444, 1.1121222734451295, 1.1294432890415191, 1.0553383696079255, 1.0345718514919282, 1.0123373913764953, 0.9250460588932037, 0.971634851694107, 0.95894975066185, 0.9432882416248322, 0.9076235187053681, 0.8806647205352783]
[2.359031915664673, 1.8259702920913696, 1.5658721923828125, 1.4591211080551147, 1.3625719547271729, 1.3100357055664062, 1.2616748809814453, 1.228752851486206, 1.1846473217010498, 1.1531360149383545, 1.1332601308822632, 1.1266734600067139, 1.0823317766189575, 1.0648831129074097, 1.0553998947143555, 1.0462572574615479, 1.0386972427368164, 1.034142017364502, 1.0217708349227905, 1.0203464031219482]
Training finished in 11.0 minutes.
Model saved to checkpoints/20230120_165907/model_20
Loading model from checkpoints/20230120_165907/model_20
SUB: 14.95%, DEL: 15.83%, INS: 1.72%, COR: 69.22%, PER: 32.50%
