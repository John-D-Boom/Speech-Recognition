Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.001, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.2, beta1=0.7, beta2=0.999)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 7.862769198417664
  batch 100 loss: 3.2450514888763426
  batch 150 loss: 3.118472099304199
  batch 200 loss: 3.0024241733551027
  batch 250 loss: 2.919935050010681
  batch 300 loss: 2.7837890863418577
  batch 350 loss: 2.6636709785461425
  batch 400 loss: 2.5669305610656736
  batch 450 loss: 2.5105005407333376
  batch 500 loss: 2.4140480661392214
  batch 550 loss: 2.3134868955612182
  batch 600 loss: 2.2102388954162597
  batch 650 loss: 2.1160530257225036
  batch 700 loss: 2.0757225608825682
  batch 750 loss: 2.017369041442871
  batch 800 loss: 1.965457992553711
  batch 850 loss: 1.9332265663146972
  batch 900 loss: 1.9003338599205017
LOSS train 1.90033 valid 1.86090, valid PER 73.26%
EPOCH 2:
  batch 50 loss: 1.8486486387252807
  batch 100 loss: 1.8387910723686218
  batch 150 loss: 1.7416730976104737
  batch 200 loss: 1.8988239336013795
  batch 250 loss: 1.7791971921920777
  batch 300 loss: 1.7200461983680726
  batch 350 loss: 1.7077779078483581
  batch 400 loss: 1.6830435061454774
  batch 450 loss: 1.6807238388061523
  batch 500 loss: 1.6580053448677063
  batch 550 loss: 1.6484893226623536
  batch 600 loss: 1.6292561054229737
  batch 650 loss: 1.584994568824768
  batch 700 loss: 1.655610818862915
  batch 750 loss: 1.6281763577461243
  batch 800 loss: 1.5684574389457702
  batch 850 loss: 1.5832848358154297
  batch 900 loss: 1.553635847568512
LOSS train 1.55364 valid 1.52991, valid PER 58.88%
EPOCH 3:
  batch 50 loss: 1.4986157703399658
  batch 100 loss: 1.5695936393737793
  batch 150 loss: 1.5425402235984802
  batch 200 loss: 1.468525402545929
  batch 250 loss: 1.465887167453766
  batch 300 loss: 1.4922006511688233
  batch 350 loss: 1.5041603779792785
  batch 400 loss: 1.4662805390357971
  batch 450 loss: 1.4388596796989441
  batch 500 loss: 1.4183290934562682
  batch 550 loss: 1.4324156975746154
  batch 600 loss: 1.3884564781188964
  batch 650 loss: 1.3992640995979309
  batch 700 loss: 1.3822403287887572
  batch 750 loss: 1.4334059000015258
  batch 800 loss: 1.4366827535629272
  batch 850 loss: 1.3916147065162658
  batch 900 loss: 1.404375810623169
LOSS train 1.40438 valid 1.33401, valid PER 45.98%
EPOCH 4:
  batch 50 loss: 1.3757910561561584
  batch 100 loss: 1.3009381425380706
  batch 150 loss: 1.340154745578766
  batch 200 loss: 1.3373365139961242
  batch 250 loss: 1.3340144491195678
  batch 300 loss: 1.3217090022563935
  batch 350 loss: 1.3131286025047302
  batch 400 loss: 1.2812520384788513
  batch 450 loss: 1.2899001002311707
  batch 500 loss: 1.336943256855011
  batch 550 loss: 1.2869096517562866
  batch 600 loss: 1.2465444827079772
  batch 650 loss: 1.3088613224029542
  batch 700 loss: 1.3375240659713745
  batch 750 loss: 1.2755649399757385
  batch 800 loss: 1.2651804780960083
  batch 850 loss: 1.2509193480014802
  batch 900 loss: 1.2582334184646606
LOSS train 1.25823 valid 1.23361, valid PER 39.33%
EPOCH 5:
  batch 50 loss: 1.2244706523418427
  batch 100 loss: 1.2159074020385743
  batch 150 loss: 1.2339372372627258
  batch 200 loss: 1.2665880942344665
  batch 250 loss: 1.206471552848816
  batch 300 loss: 1.2332158374786377
  batch 350 loss: 1.1951837372779845
  batch 400 loss: 1.1793063127994536
  batch 450 loss: 1.190199877023697
  batch 500 loss: 1.1672085046768188
  batch 550 loss: 1.2443230295181273
  batch 600 loss: 1.2294499897956848
  batch 650 loss: 1.1946763408184051
  batch 700 loss: 1.2003644800186157
  batch 750 loss: 1.1969604635238646
  batch 800 loss: 1.2257956302165984
  batch 850 loss: 1.2076931738853454
  batch 900 loss: 1.1952737116813659
LOSS train 1.19527 valid 1.18112, valid PER 37.92%
EPOCH 6:
  batch 50 loss: 1.1688949179649353
  batch 100 loss: 1.199145213365555
  batch 150 loss: 1.1775402879714967
  batch 200 loss: 1.121303185224533
  batch 250 loss: 1.1494985544681549
  batch 300 loss: 1.15746027469635
  batch 350 loss: 1.1502072715759277
  batch 400 loss: 1.1423926901817323
  batch 450 loss: 1.1631240928173066
  batch 500 loss: 1.1166539549827577
  batch 550 loss: 1.1592610311508178
  batch 600 loss: 1.14758176445961
  batch 650 loss: 1.1294276893138886
  batch 700 loss: 1.115360267162323
  batch 750 loss: 1.1472504663467407
  batch 800 loss: 1.1381332337856294
  batch 850 loss: 1.1483348560333253
  batch 900 loss: 1.153190585374832
LOSS train 1.15319 valid 1.11190, valid PER 37.06%
EPOCH 7:
  batch 50 loss: 1.0909158825874328
  batch 100 loss: 1.1376675319671632
  batch 150 loss: 1.0799055254459382
  batch 200 loss: 1.074008959531784
  batch 250 loss: 1.1297971498966217
  batch 300 loss: 1.1008432412147522
  batch 350 loss: 1.140709286928177
  batch 400 loss: 1.0865516567230225
  batch 450 loss: 1.0990508198738098
  batch 500 loss: 1.0978653299808503
  batch 550 loss: 1.0699442327022552
  batch 600 loss: 1.0926333391666412
  batch 650 loss: 1.064475909471512
  batch 700 loss: 1.1301685857772827
  batch 750 loss: 1.090399726629257
  batch 800 loss: 1.0925726413726806
  batch 850 loss: 1.0699021804332733
  batch 900 loss: 1.0703255200386048
LOSS train 1.07033 valid 1.08733, valid PER 34.96%
EPOCH 8:
  batch 50 loss: 1.0761775994300842
  batch 100 loss: 1.040288244485855
  batch 150 loss: 1.0764416563510895
  batch 200 loss: 1.0545064115524292
  batch 250 loss: 1.0384054839611054
  batch 300 loss: 1.0234452700614929
  batch 350 loss: 1.0643905663490296
  batch 400 loss: 1.0564835047721863
  batch 450 loss: 1.1042311298847198
  batch 500 loss: 1.0563345301151275
  batch 550 loss: 1.0800949811935425
  batch 600 loss: 1.0423535072803498
  batch 650 loss: 1.0411418354511262
  batch 700 loss: 1.064216696023941
  batch 750 loss: 1.1170063877105714
  batch 800 loss: 1.086095722913742
  batch 850 loss: 1.0605689227581023
  batch 900 loss: 1.034498668909073
LOSS train 1.03450 valid 1.07831, valid PER 35.10%
EPOCH 9:
  batch 50 loss: 1.034291785955429
  batch 100 loss: 1.0341959726810455
  batch 150 loss: 1.0278972041606904
  batch 200 loss: 1.0140975522994995
  batch 250 loss: 0.9880459725856781
  batch 300 loss: 1.029628883600235
  batch 350 loss: 1.0136521768569946
  batch 400 loss: 1.0644987523555756
  batch 450 loss: 1.0706342482566833
  batch 500 loss: 1.0272676181793212
  batch 550 loss: 1.0329272413253785
  batch 600 loss: 1.060732480287552
  batch 650 loss: 1.0171567976474762
  batch 700 loss: 1.0272853350639344
  batch 750 loss: 1.029749163389206
  batch 800 loss: 1.0628202700614928
  batch 850 loss: 1.0571581184864045
  batch 900 loss: 1.0241922163963317
LOSS train 1.02419 valid 1.04643, valid PER 33.08%
EPOCH 10:
  batch 50 loss: 0.9849572086334228
  batch 100 loss: 1.0297594392299652
  batch 150 loss: 1.0485881519317628
  batch 200 loss: 0.9688184356689453
  batch 250 loss: 0.9947028160095215
  batch 300 loss: 0.9857814466953277
  batch 350 loss: 0.9986571490764617
  batch 400 loss: 0.9694373536109925
  batch 450 loss: 0.9979169893264771
  batch 500 loss: 0.9913932609558106
  batch 550 loss: 0.9955406081676483
  batch 600 loss: 0.997658714056015
  batch 650 loss: 1.0133888852596282
  batch 700 loss: 1.0305334556102752
  batch 750 loss: 1.0179172956943512
  batch 800 loss: 0.9952535486221313
  batch 850 loss: 0.9919472420215607
  batch 900 loss: 0.9939333736896515
LOSS train 0.99393 valid 1.01498, valid PER 32.70%
EPOCH 11:
  batch 50 loss: 0.9794925487041474
  batch 100 loss: 0.9326356625556946
  batch 150 loss: 0.9671365332603454
  batch 200 loss: 0.9407465744018555
  batch 250 loss: 0.9593543386459351
  batch 300 loss: 0.9590530824661255
  batch 350 loss: 0.9911021840572357
  batch 400 loss: 0.9627016842365265
  batch 450 loss: 0.9527050220966339
  batch 500 loss: 0.9541084659099579
  batch 550 loss: 0.9849580562114716
  batch 600 loss: 0.9618201196193695
  batch 650 loss: 0.9793994557857514
  batch 700 loss: 1.0386340022087097
  batch 750 loss: 0.9739141380786895
  batch 800 loss: 0.9821108746528625
  batch 850 loss: 0.9466252183914184
  batch 900 loss: 1.0013683438301086
LOSS train 1.00137 valid 1.01641, valid PER 32.42%
EPOCH 12:
  batch 50 loss: 0.9210620868206024
  batch 100 loss: 0.9037631464004516
  batch 150 loss: 0.9475152492523193
  batch 200 loss: 0.9717355144023895
  batch 250 loss: 0.9247258090972901
  batch 300 loss: 0.9761398267745972
  batch 350 loss: 0.9304090702533722
  batch 400 loss: 0.9652600014209747
  batch 450 loss: 0.9382161843776703
  batch 500 loss: 0.9693536758422852
  batch 550 loss: 0.9521752178668976
  batch 600 loss: 0.9621226286888123
  batch 650 loss: 0.9575736963748932
  batch 700 loss: 0.9379165947437287
  batch 750 loss: 0.959829638004303
  batch 800 loss: 0.9388857400417328
  batch 850 loss: 0.9650404214859009
  batch 900 loss: 0.9816288387775421
LOSS train 0.98163 valid 0.99014, valid PER 31.68%
EPOCH 13:
  batch 50 loss: 0.906136144399643
  batch 100 loss: 0.9024200475215912
  batch 150 loss: 0.926702960729599
  batch 200 loss: 0.8947264790534973
  batch 250 loss: 0.9215530359745026
  batch 300 loss: 0.939483904838562
  batch 350 loss: 0.922726401090622
  batch 400 loss: 0.9292280018329621
  batch 450 loss: 0.9152535235881806
  batch 500 loss: 0.9119673645496369
  batch 550 loss: 0.9834859478473663
  batch 600 loss: 0.9298305439949036
  batch 650 loss: 0.9165028083324432
  batch 700 loss: 0.9276234662532806
  batch 750 loss: 0.8855211079120636
  batch 800 loss: 0.9394507825374603
  batch 850 loss: 0.8962553584575653
  batch 900 loss: 0.9240260338783264
LOSS train 0.92403 valid 0.97663, valid PER 31.23%
EPOCH 14:
  batch 50 loss: 0.8909442889690399
  batch 100 loss: 0.8896035075187683
  batch 150 loss: 0.8974743688106537
  batch 200 loss: 0.8757707953453064
  batch 250 loss: 0.8940087950229645
  batch 300 loss: 0.8773588597774505
  batch 350 loss: 0.9100750172138214
  batch 400 loss: 0.9104698848724365
  batch 450 loss: 0.872096494436264
  batch 500 loss: 0.9033594000339508
  batch 550 loss: 0.9019273865222931
  batch 600 loss: 0.894343079328537
  batch 650 loss: 0.9228942465782165
  batch 700 loss: 0.9130320477485657
  batch 750 loss: 0.9037798154354095
  batch 800 loss: 0.9033333337306977
  batch 850 loss: 0.9164371728897095
  batch 900 loss: 0.9180831146240235
LOSS train 0.91808 valid 0.97751, valid PER 31.65%
EPOCH 15:
  batch 50 loss: 0.8489294409751892
  batch 100 loss: 0.8718393564224243
  batch 150 loss: 0.8602212119102478
  batch 200 loss: 0.8898851144313812
  batch 250 loss: 0.8979705786705017
  batch 300 loss: 0.8910778844356537
  batch 350 loss: 0.8765225660800934
  batch 400 loss: 0.8987272512912751
  batch 450 loss: 0.8967778909206391
  batch 500 loss: 0.8655096018314361
  batch 550 loss: 0.906989643573761
  batch 600 loss: 0.9231973016262054
  batch 650 loss: 0.889124653339386
  batch 700 loss: 0.8773603534698486
  batch 750 loss: 0.9186317634582519
  batch 800 loss: 0.8871517884731293
  batch 850 loss: 0.8643789935112
  batch 900 loss: 0.8455595874786377
LOSS train 0.84556 valid 0.97856, valid PER 31.59%
EPOCH 16:
  batch 50 loss: 0.8692056655883789
  batch 100 loss: 0.8490402233600617
  batch 150 loss: 0.858433005809784
  batch 200 loss: 0.8921886217594147
  batch 250 loss: 0.8587894880771637
  batch 300 loss: 0.8766910421848297
  batch 350 loss: 0.8837574136257171
  batch 400 loss: 0.8489787697792053
  batch 450 loss: 0.8916622138023377
  batch 500 loss: 0.8775440406799316
  batch 550 loss: 0.8565192949771882
  batch 600 loss: 0.8865403950214386
  batch 650 loss: 0.880206035375595
  batch 700 loss: 0.8491526210308075
  batch 750 loss: 0.873808991909027
  batch 800 loss: 0.8548228192329407
  batch 850 loss: 0.8536241447925568
  batch 900 loss: 0.8524275028705597
LOSS train 0.85243 valid 0.95320, valid PER 30.28%
EPOCH 17:
  batch 50 loss: 0.8595166337490082
  batch 100 loss: 0.8004420709609985
  batch 150 loss: 0.8594149422645568
  batch 200 loss: 0.8187142419815063
  batch 250 loss: 0.8539220643043518
  batch 300 loss: 0.8318008494377136
  batch 350 loss: 0.8591292071342468
  batch 400 loss: 0.8559988045692444
  batch 450 loss: 0.8493919968605042
  batch 500 loss: 0.8617142927646637
  batch 550 loss: 0.8500717866420746
  batch 600 loss: 0.863281967639923
  batch 650 loss: 0.8280420303344727
  batch 700 loss: 0.8670386052131653
  batch 750 loss: 0.8164100778102875
  batch 800 loss: 0.8653314507007599
  batch 850 loss: 0.8504480111598969
  batch 900 loss: 0.8621057832241058
LOSS train 0.86211 valid 0.95377, valid PER 30.21%
EPOCH 18:
  batch 50 loss: 0.8011734187602997
  batch 100 loss: 0.7985816675424576
  batch 150 loss: 0.8646423625946045
  batch 200 loss: 0.8300505328178406
  batch 250 loss: 0.7955993354320526
  batch 300 loss: 0.8182705521583558
  batch 350 loss: 0.8037409710884095
  batch 400 loss: 0.8165444934368133
  batch 450 loss: 0.8472541010379792
  batch 500 loss: 0.8337160098552704
  batch 550 loss: 0.8727503991127015
  batch 600 loss: 0.8571307694911957
  batch 650 loss: 0.8169972163438797
  batch 700 loss: 0.8316319358348846
  batch 750 loss: 0.84783660531044
  batch 800 loss: 0.8576569736003876
  batch 850 loss: 0.8482080423831939
  batch 900 loss: 0.8572016042470932
LOSS train 0.85720 valid 0.96353, valid PER 30.78%
EPOCH 19:
  batch 50 loss: 0.8131155431270599
  batch 100 loss: 0.8134216451644898
  batch 150 loss: 0.799868322610855
  batch 200 loss: 0.7991642975807189
  batch 250 loss: 0.8456943118572235
  batch 300 loss: 0.8386333286762238
  batch 350 loss: 0.8325388038158417
  batch 400 loss: 0.8216548407077789
  batch 450 loss: 0.7888949632644653
  batch 500 loss: 0.7804135406017303
  batch 550 loss: 0.8269189476966858
  batch 600 loss: 0.8556193059682846
  batch 650 loss: 0.8521341407299041
  batch 700 loss: 0.8753148114681244
  batch 750 loss: 0.8176152443885804
  batch 800 loss: 0.8059741687774659
  batch 850 loss: 0.8478512012958527
  batch 900 loss: 0.8291046929359436
LOSS train 0.82910 valid 0.95240, valid PER 29.78%
EPOCH 20:
  batch 50 loss: 0.7723328065872193
  batch 100 loss: 0.7990493404865265
  batch 150 loss: 0.7928933775424958
  batch 200 loss: 0.8120609128475189
  batch 250 loss: 0.8250842988491058
  batch 300 loss: 0.7872074604034424
  batch 350 loss: 0.8005778872966767
  batch 400 loss: 0.792055401802063
  batch 450 loss: 0.7869024682044983
  batch 500 loss: 0.8194986534118652
  batch 550 loss: 0.8091552984714508
  batch 600 loss: 0.7945759212970733
  batch 650 loss: 0.8365777432918549
  batch 700 loss: 0.8067571926116943
  batch 750 loss: 0.780809828042984
  batch 800 loss: 0.8213989973068238
  batch 850 loss: 0.8317741119861602
  batch 900 loss: 0.7932656490802765
LOSS train 0.79327 valid 0.96601, valid PER 30.73%
[1.9003338599205017, 1.553635847568512, 1.404375810623169, 1.2582334184646606, 1.1952737116813659, 1.153190585374832, 1.0703255200386048, 1.034498668909073, 1.0241922163963317, 0.9939333736896515, 1.0013683438301086, 0.9816288387775421, 0.9240260338783264, 0.9180831146240235, 0.8455595874786377, 0.8524275028705597, 0.8621057832241058, 0.8572016042470932, 0.8291046929359436, 0.7932656490802765]
[1.8608981370925903, 1.529907464981079, 1.334011197090149, 1.2336112260818481, 1.181122899055481, 1.111895203590393, 1.0873316526412964, 1.078311562538147, 1.0464286804199219, 1.0149757862091064, 1.0164096355438232, 0.9901410341262817, 0.9766280651092529, 0.9775094985961914, 0.97855544090271, 0.9532040357589722, 0.953773558139801, 0.9635300040245056, 0.9523965716362, 0.9660063982009888]
Training finished in 11.0 minutes.
Model saved to checkpoints/20230120_165907/model_19
Loading model from checkpoints/20230120_165907/model_19
SUB: 16.68%, DEL: 13.04%, INS: 2.11%, COR: 70.28%, PER: 31.84%
