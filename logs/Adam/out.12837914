Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.003, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.2, beta1=0.9, beta2=0.999)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 6.042528386116028
  batch 100 loss: 3.3216409826278688
  batch 150 loss: 3.2997556018829344
  batch 200 loss: 3.1855432558059693
  batch 250 loss: 2.9697042417526247
  batch 300 loss: 2.6188345766067505
  batch 350 loss: 2.3660755014419554
  batch 400 loss: 2.2660941004753115
  batch 450 loss: 2.151481864452362
  batch 500 loss: 2.0454084277153015
  batch 550 loss: 1.9513386273384095
  batch 600 loss: 1.9234595370292664
  batch 650 loss: 1.8294256925582886
  batch 700 loss: 1.8314005732536316
  batch 750 loss: 1.7780843377113342
  batch 800 loss: 1.7579339790344237
  batch 850 loss: 1.7048975563049316
  batch 900 loss: 1.6748938798904418
LOSS train 1.67489 valid 1.66083, valid PER 64.24%
EPOCH 2:
  batch 50 loss: 1.6756168842315673
  batch 100 loss: 1.651099112033844
  batch 150 loss: 1.5892243432998656
  batch 200 loss: 1.5591245651245118
  batch 250 loss: 1.5841559982299804
  batch 300 loss: 1.538424460887909
  batch 350 loss: 1.5510284948348998
  batch 400 loss: 1.517278482913971
  batch 450 loss: 1.4897298002243042
  batch 500 loss: 1.4948412799835205
  batch 550 loss: 1.4828002333641053
  batch 600 loss: 1.4703602504730224
  batch 650 loss: 1.4164761066436768
  batch 700 loss: 1.4345373010635376
  batch 750 loss: 1.4381792902946473
  batch 800 loss: 1.3939872694015503
  batch 850 loss: 1.3951911759376525
  batch 900 loss: 1.3804543471336366
LOSS train 1.38045 valid 1.34356, valid PER 46.03%
EPOCH 3:
  batch 50 loss: 1.330795488357544
  batch 100 loss: 1.3843128347396851
  batch 150 loss: 1.3717662501335144
  batch 200 loss: 1.3329484033584595
  batch 250 loss: 1.3444097661972045
  batch 300 loss: 1.333889274597168
  batch 350 loss: 1.3405667769908904
  batch 400 loss: 1.3222124373912811
  batch 450 loss: 1.3216049695014953
  batch 500 loss: 1.292599538564682
  batch 550 loss: 1.303696378469467
  batch 600 loss: 1.2368407213687898
  batch 650 loss: 1.2627509641647339
  batch 700 loss: 1.285746419429779
  batch 750 loss: 1.287412006855011
  batch 800 loss: 1.3039912700653076
  batch 850 loss: 1.2646504652500152
  batch 900 loss: 1.2877563405036927
LOSS train 1.28776 valid 1.21827, valid PER 39.89%
EPOCH 4:
  batch 50 loss: 1.2546900248527526
  batch 100 loss: 1.1925430810451507
  batch 150 loss: 1.2123941826820372
  batch 200 loss: 1.2354088258743285
  batch 250 loss: 1.2364703178405763
  batch 300 loss: 1.243797174692154
  batch 350 loss: 1.2385776495933534
  batch 400 loss: 1.1845839548110961
  batch 450 loss: 1.1932746684551239
  batch 500 loss: 1.259140602350235
  batch 550 loss: 1.179295881986618
  batch 600 loss: 1.1849686872959138
  batch 650 loss: 1.2313852310180664
  batch 700 loss: 1.24473659157753
  batch 750 loss: 1.1861372184753418
  batch 800 loss: 1.1731806635856628
  batch 850 loss: 1.172175613641739
  batch 900 loss: 1.1952676463127136
LOSS train 1.19527 valid 1.16176, valid PER 36.77%
EPOCH 5:
  batch 50 loss: 1.161545889377594
  batch 100 loss: 1.1179484272003173
  batch 150 loss: 1.1825346112251283
  batch 200 loss: 1.22309374332428
  batch 250 loss: 1.1251272058486939
  batch 300 loss: 1.1604652535915374
  batch 350 loss: 1.1137151670455934
  batch 400 loss: 1.1188998854160308
  batch 450 loss: 1.1218203485012055
  batch 500 loss: 1.1181664776802063
  batch 550 loss: 1.1731829154491424
  batch 600 loss: 1.1456394064426423
  batch 650 loss: 1.1561339974403382
  batch 700 loss: 1.1446777188777923
  batch 750 loss: 1.1587263321876526
  batch 800 loss: 1.2310486364364623
  batch 850 loss: 1.1933377254009248
  batch 900 loss: 1.1406460690498352
LOSS train 1.14065 valid 1.12614, valid PER 35.21%
EPOCH 6:
  batch 50 loss: 1.0910468876361847
  batch 100 loss: 1.130765256881714
  batch 150 loss: 1.1212103736400605
  batch 200 loss: 1.070332831144333
  batch 250 loss: 1.104173263311386
  batch 300 loss: 1.1170836651325227
  batch 350 loss: 1.147187123298645
  batch 400 loss: 1.11074951171875
  batch 450 loss: 1.1129604041576386
  batch 500 loss: 1.0809391617774964
  batch 550 loss: 1.0981278836727142
  batch 600 loss: 1.0501497650146485
  batch 650 loss: 1.0360723674297332
  batch 700 loss: 1.0670296013355256
  batch 750 loss: 1.0944142353534698
  batch 800 loss: 1.066843501329422
  batch 850 loss: 1.0872777915000915
  batch 900 loss: 1.1182813262939453
LOSS train 1.11828 valid 1.07808, valid PER 33.63%
EPOCH 7:
  batch 50 loss: 1.0398838937282562
  batch 100 loss: 1.1089644277095794
  batch 150 loss: 1.022581478357315
  batch 200 loss: 1.041250468492508
  batch 250 loss: 1.0946201193332672
  batch 300 loss: 1.040042816400528
  batch 350 loss: 1.0679532849788667
  batch 400 loss: 1.0328012418746948
  batch 450 loss: 1.0349207603931427
  batch 500 loss: 1.0353983902931214
  batch 550 loss: 1.0159486424922943
  batch 600 loss: 1.030581475496292
  batch 650 loss: 1.0117028641700745
  batch 700 loss: 1.0929119539260865
  batch 750 loss: 1.0458604454994203
  batch 800 loss: 1.055404359102249
  batch 850 loss: 1.020526579618454
  batch 900 loss: 1.0335879588127137
LOSS train 1.03359 valid 1.04512, valid PER 33.02%
EPOCH 8:
  batch 50 loss: 1.0001193141937257
  batch 100 loss: 0.9622202086448669
  batch 150 loss: 1.0400240457057952
  batch 200 loss: 1.0079822468757629
  batch 250 loss: 0.9973365485668182
  batch 300 loss: 0.9757966983318329
  batch 350 loss: 0.9884367871284485
  batch 400 loss: 0.9909283411502838
  batch 450 loss: 1.062980247735977
  batch 500 loss: 1.009984438419342
  batch 550 loss: 1.0150859475135803
  batch 600 loss: 0.9832010507583618
  batch 650 loss: 1.02419655919075
  batch 700 loss: 1.0179951333999633
  batch 750 loss: 1.0456833493709565
  batch 800 loss: 1.0628619849681855
  batch 850 loss: 1.0102957844734193
  batch 900 loss: 1.0077716565132142
LOSS train 1.00777 valid 1.04110, valid PER 32.08%
EPOCH 9:
  batch 50 loss: 0.9493141078948975
  batch 100 loss: 0.9544833838939667
  batch 150 loss: 0.9631081807613373
  batch 200 loss: 0.9467522716522216
  batch 250 loss: 0.9583359777927398
  batch 300 loss: 0.9932510459423065
  batch 350 loss: 0.9396426975727081
  batch 400 loss: 1.0063899886608123
  batch 450 loss: 1.0233565759658814
  batch 500 loss: 0.9826627862453461
  batch 550 loss: 0.9832785737514496
  batch 600 loss: 1.0497418785095214
  batch 650 loss: 1.0260871541500092
  batch 700 loss: 0.9796233212947846
  batch 750 loss: 0.9812023293972015
  batch 800 loss: 1.040422854423523
  batch 850 loss: 0.9969185256958008
  batch 900 loss: 0.9537516021728516
LOSS train 0.95375 valid 1.02015, valid PER 31.10%
EPOCH 10:
  batch 50 loss: 0.9460189497470856
  batch 100 loss: 0.9549548768997193
  batch 150 loss: 0.9941119110584259
  batch 200 loss: 0.9416918241977692
  batch 250 loss: 0.9745735847949981
  batch 300 loss: 0.9432044959068299
  batch 350 loss: 0.93836927652359
  batch 400 loss: 0.9026188671588897
  batch 450 loss: 0.9250599026679993
  batch 500 loss: 0.9244358003139496
  batch 550 loss: 0.9451744329929351
  batch 600 loss: 0.9513266503810882
  batch 650 loss: 0.9524168229103088
  batch 700 loss: 0.9893454420566559
  batch 750 loss: 0.9904277515411377
  batch 800 loss: 0.9816101539134979
  batch 850 loss: 0.9519923949241638
  batch 900 loss: 0.981595903635025
LOSS train 0.98160 valid 1.03201, valid PER 32.25%
EPOCH 11:
  batch 50 loss: 0.9514953446388245
  batch 100 loss: 0.9324996626377106
  batch 150 loss: 0.9415885484218598
  batch 200 loss: 0.8825679087638855
  batch 250 loss: 0.9076952338218689
  batch 300 loss: 0.915345743894577
  batch 350 loss: 0.9592866551876068
  batch 400 loss: 0.9373728835582733
  batch 450 loss: 0.9471648955345153
  batch 500 loss: 0.9398133540153504
  batch 550 loss: 0.9470140099525451
  batch 600 loss: 0.934177142381668
  batch 650 loss: 0.9690163350105285
  batch 700 loss: 1.0381335628032684
  batch 750 loss: 0.9353245985507965
  batch 800 loss: 0.933872138261795
  batch 850 loss: 0.9328571236133576
  batch 900 loss: 0.9579329943656921
LOSS train 0.95793 valid 1.00600, valid PER 30.86%
EPOCH 12:
  batch 50 loss: 0.9108234226703644
  batch 100 loss: 0.900173442363739
  batch 150 loss: 0.9105427551269532
  batch 200 loss: 0.9400862836837769
  batch 250 loss: 0.894282785654068
  batch 300 loss: 0.9454064440727233
  batch 350 loss: 0.9369784069061279
  batch 400 loss: 0.9319383549690247
  batch 450 loss: 0.8712186694145203
  batch 500 loss: 0.9162368786334991
  batch 550 loss: 0.9484913671016693
  batch 600 loss: 0.9388204932212829
  batch 650 loss: 0.9041053748130798
  batch 700 loss: 0.920060932636261
  batch 750 loss: 0.9435083448886872
  batch 800 loss: 0.9019398140907288
  batch 850 loss: 0.9334981834888458
  batch 900 loss: 0.9579715847969055
LOSS train 0.95797 valid 1.00264, valid PER 31.05%
EPOCH 13:
  batch 50 loss: 0.8852547192573548
  batch 100 loss: 0.9010572516918183
  batch 150 loss: 0.9387333023548127
  batch 200 loss: 0.8566868782043457
  batch 250 loss: 0.8804287660121918
  batch 300 loss: 0.9242388522624969
  batch 350 loss: 0.8802799654006958
  batch 400 loss: 0.8840063714981079
  batch 450 loss: 0.9114615178108215
  batch 500 loss: 0.9115506458282471
  batch 550 loss: 0.952456248998642
  batch 600 loss: 0.9132779216766358
  batch 650 loss: 0.8964570510387421
  batch 700 loss: 0.9206830620765686
  batch 750 loss: 0.8939202201366424
  batch 800 loss: 0.9135749578475952
  batch 850 loss: 0.8953370583057404
  batch 900 loss: 0.889681087732315
LOSS train 0.88968 valid 0.96389, valid PER 29.88%
EPOCH 14:
  batch 50 loss: 0.8569752359390259
  batch 100 loss: 0.8260832571983338
  batch 150 loss: 0.8576363706588745
  batch 200 loss: 0.8964692401885986
  batch 250 loss: 0.8841501414775849
  batch 300 loss: 0.8919347298145294
  batch 350 loss: 0.9049190962314606
  batch 400 loss: 0.9001987910270691
  batch 450 loss: 0.8601719188690186
  batch 500 loss: 0.886973820924759
  batch 550 loss: 0.9050568425655365
  batch 600 loss: 0.9077101910114288
  batch 650 loss: 0.9193822383880615
  batch 700 loss: 0.9029857647418976
  batch 750 loss: 0.873405146598816
  batch 800 loss: 0.8922476720809936
  batch 850 loss: 0.9355351316928864
  batch 900 loss: 0.9083612704277039
LOSS train 0.90836 valid 0.99771, valid PER 30.60%
EPOCH 15:
  batch 50 loss: 0.8471135342121124
  batch 100 loss: 0.8652160143852234
  batch 150 loss: 0.8457385981082917
  batch 200 loss: 0.8590170919895173
  batch 250 loss: 0.8705018138885499
  batch 300 loss: 0.8618662488460541
  batch 350 loss: 0.8482057642936707
  batch 400 loss: 0.8721369230747222
  batch 450 loss: 0.8766140854358673
  batch 500 loss: 0.8672249245643616
  batch 550 loss: 0.9070561647415161
  batch 600 loss: 0.9207795917987823
  batch 650 loss: 0.8646678721904755
  batch 700 loss: 0.8573777234554291
  batch 750 loss: 0.9025391042232513
  batch 800 loss: 0.8572800076007843
  batch 850 loss: 0.8860965991020202
  batch 900 loss: 0.8375270354747772
LOSS train 0.83753 valid 1.01345, valid PER 31.14%
EPOCH 16:
  batch 50 loss: 0.8768727159500123
  batch 100 loss: 0.831927877664566
  batch 150 loss: 0.8667199695110321
  batch 200 loss: 0.877095741033554
  batch 250 loss: 0.8642762577533722
  batch 300 loss: 0.871554399728775
  batch 350 loss: 0.8700306284427642
  batch 400 loss: 0.8501620364189147
  batch 450 loss: 0.8772697913646698
  batch 500 loss: 0.8454664349555969
  batch 550 loss: 0.8239061570167542
  batch 600 loss: 0.8957509112358093
  batch 650 loss: 0.8725697731971741
  batch 700 loss: 0.8519271659851074
  batch 750 loss: 0.887618625164032
  batch 800 loss: 0.8538566207885743
  batch 850 loss: 0.8695029509067536
  batch 900 loss: 0.8741898787021637
LOSS train 0.87419 valid 0.96666, valid PER 29.52%
EPOCH 17:
  batch 50 loss: 0.8358527851104737
  batch 100 loss: 0.7773653411865235
  batch 150 loss: 0.8878146719932556
  batch 200 loss: 0.8342266172170639
  batch 250 loss: 0.8604358983039856
  batch 300 loss: 0.8361893594264984
  batch 350 loss: 0.8611807775497436
  batch 400 loss: 0.8783878254890441
  batch 450 loss: 0.8515385329723358
  batch 500 loss: 0.8523083007335663
  batch 550 loss: 0.8527322363853455
  batch 600 loss: 0.8773637747764588
  batch 650 loss: 0.8433640551567078
  batch 700 loss: 0.875549703836441
  batch 750 loss: 0.8187870192527771
  batch 800 loss: 0.9503715467453003
  batch 850 loss: 0.8941168129444123
  batch 900 loss: 0.8769878923892975
LOSS train 0.87699 valid 0.97080, valid PER 30.16%
EPOCH 18:
  batch 50 loss: 0.8032647371292114
  batch 100 loss: 0.8080843734741211
  batch 150 loss: 0.8584782755374909
  batch 200 loss: 0.8361279475688934
  batch 250 loss: 0.8168786287307739
  batch 300 loss: 0.841277312040329
  batch 350 loss: 0.8262975633144378
  batch 400 loss: 0.8241033720970153
  batch 450 loss: 0.8683334398269653
  batch 500 loss: 0.8453861451148987
  batch 550 loss: 0.882811028957367
  batch 600 loss: 0.8431761765480041
  batch 650 loss: 0.8298287892341614
  batch 700 loss: 0.839994546175003
  batch 750 loss: 0.8458566808700562
  batch 800 loss: 0.8554896420240402
  batch 850 loss: 0.8542593574523926
  batch 900 loss: 0.8464158916473389
LOSS train 0.84642 valid 0.97148, valid PER 29.62%
EPOCH 19:
  batch 50 loss: 0.8089495265483856
  batch 100 loss: 0.8355780827999115
  batch 150 loss: 0.8435227286815643
  batch 200 loss: 0.8195627856254578
  batch 250 loss: 0.860210566520691
  batch 300 loss: 0.8564854478836059
  batch 350 loss: 0.8537644958496093
  batch 400 loss: 0.8490687572956085
  batch 450 loss: 0.7912440049648285
  batch 500 loss: 0.807891138792038
  batch 550 loss: 0.8701341819763183
  batch 600 loss: 0.872168436050415
  batch 650 loss: 0.8675090718269348
  batch 700 loss: 0.8699456858634949
  batch 750 loss: 0.8173277735710144
  batch 800 loss: 0.8020719444751739
  batch 850 loss: 0.8522568428516388
  batch 900 loss: 0.8534666192531586
LOSS train 0.85347 valid 0.97781, valid PER 30.14%
EPOCH 20:
  batch 50 loss: 0.7828498911857605
  batch 100 loss: 0.7874506413936615
  batch 150 loss: 0.7873599487543106
  batch 200 loss: 0.8749660456180572
  batch 250 loss: 0.8467235922813415
  batch 300 loss: 0.8650754714012145
  batch 350 loss: 0.8291197955608368
  batch 400 loss: 0.8416612637043
  batch 450 loss: 0.8659487700462342
  batch 500 loss: 0.9151269662380218
  batch 550 loss: 0.84768110871315
  batch 600 loss: 0.8289269375801086
  batch 650 loss: 0.8615871620178223
  batch 700 loss: 0.8212845957279206
  batch 750 loss: 0.8407211887836457
  batch 800 loss: 0.8561301457881928
  batch 850 loss: 0.861189044713974
  batch 900 loss: 0.8454389524459839
LOSS train 0.84544 valid 0.99301, valid PER 29.94%
[1.6748938798904418, 1.3804543471336366, 1.2877563405036927, 1.1952676463127136, 1.1406460690498352, 1.1182813262939453, 1.0335879588127137, 1.0077716565132142, 0.9537516021728516, 0.981595903635025, 0.9579329943656921, 0.9579715847969055, 0.889681087732315, 0.9083612704277039, 0.8375270354747772, 0.8741898787021637, 0.8769878923892975, 0.8464158916473389, 0.8534666192531586, 0.8454389524459839]
[1.6608308553695679, 1.3435561656951904, 1.218273401260376, 1.16176438331604, 1.1261388063430786, 1.0780836343765259, 1.0451198816299438, 1.0411006212234497, 1.0201547145843506, 1.032012701034546, 1.0060014724731445, 1.0026408433914185, 0.9638916850090027, 0.9977113008499146, 1.0134544372558594, 0.9666617512702942, 0.9707999229431152, 0.9714797735214233, 0.9778077602386475, 0.9930127263069153]
Training finished in 10.0 minutes.
Model saved to checkpoints/20230120_135124/model_13
Loading model from checkpoints/20230120_135124/model_13
SUB: 17.18%, DEL: 12.93%, INS: 1.90%, COR: 69.89%, PER: 32.02%
