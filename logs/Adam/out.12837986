Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.01, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.2, beta1=0.9, beta2=0.999)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.304257969856263
  batch 100 loss: 2.927231969833374
  batch 150 loss: 2.6936397790908813
  batch 200 loss: 2.3522228360176087
  batch 250 loss: 2.199678411483765
  batch 300 loss: 2.0391195154190065
  batch 350 loss: 1.9312738966941834
  batch 400 loss: 1.9422038531303405
  batch 450 loss: 1.8602255535125733
  batch 500 loss: 1.7792193484306336
  batch 550 loss: 1.7257301473617555
  batch 600 loss: 1.7209325337409973
  batch 650 loss: 1.6347504901885985
  batch 700 loss: 1.6831981897354127
  batch 750 loss: 1.630502531528473
  batch 800 loss: 1.6453180170059205
  batch 850 loss: 1.6446512246131897
  batch 900 loss: 1.5980576872825623
LOSS train 1.59806 valid 1.51534, valid PER 50.33%
EPOCH 2:
  batch 50 loss: 1.5873424077033997
  batch 100 loss: 1.6007584857940673
  batch 150 loss: 1.51904714345932
  batch 200 loss: 1.5320326066017151
  batch 250 loss: 1.5885026502609252
  batch 300 loss: 1.5305540561676025
  batch 350 loss: 1.5946756243705749
  batch 400 loss: 1.5799405550956727
  batch 450 loss: 1.5187435173988342
  batch 500 loss: 1.5410554647445678
  batch 550 loss: 1.5038131046295167
  batch 600 loss: 1.5207386445999145
  batch 650 loss: 1.485122902393341
  batch 700 loss: 1.517878577709198
  batch 750 loss: 1.4727205014228821
  batch 800 loss: 1.4447067332267762
  batch 850 loss: 1.4485756874084472
  batch 900 loss: 1.4195031905174256
LOSS train 1.41950 valid 1.40975, valid PER 44.42%
EPOCH 3:
  batch 50 loss: 1.3924443483352662
  batch 100 loss: 1.4457623863220215
  batch 150 loss: 1.435617208480835
  batch 200 loss: 1.4275064277648926
  batch 250 loss: 1.465287218093872
  batch 300 loss: 1.4461520576477052
  batch 350 loss: 1.4440148067474365
  batch 400 loss: 1.4420196223258972
  batch 450 loss: 1.4186301803588868
  batch 500 loss: 1.3730040574073792
  batch 550 loss: 1.3926065135002137
  batch 600 loss: 1.333014395236969
  batch 650 loss: 1.4089337646961213
  batch 700 loss: 1.3875106739997864
  batch 750 loss: 1.4329131150245666
  batch 800 loss: 1.4988881421089173
  batch 850 loss: 1.4472204327583313
  batch 900 loss: 1.4555007696151734
LOSS train 1.45550 valid 1.38054, valid PER 44.41%
EPOCH 4:
  batch 50 loss: 1.4465659403800963
  batch 100 loss: 1.3755756211280823
  batch 150 loss: 1.3848492813110351
  batch 200 loss: 1.3700406193733214
  batch 250 loss: 1.3970441603660584
  batch 300 loss: 1.418061969280243
  batch 350 loss: 1.394207284450531
  batch 400 loss: 1.3480047535896302
  batch 450 loss: 1.3672169637680054
  batch 500 loss: 1.3904236364364624
  batch 550 loss: 1.3408359837532045
  batch 600 loss: 1.3642090511322023
  batch 650 loss: 1.6906656575202943
  batch 700 loss: 1.6432835459709167
  batch 750 loss: 1.5130848741531373
  batch 800 loss: 1.4881479024887085
  batch 850 loss: 1.4213927435874938
  batch 900 loss: 1.4611097502708434
LOSS train 1.46111 valid 1.41421, valid PER 42.96%
EPOCH 5:
  batch 50 loss: 1.4819542789459228
  batch 100 loss: 1.3891096591949463
  batch 150 loss: 1.4353477549552918
  batch 200 loss: 1.4481413888931274
  batch 250 loss: 1.3529949498176574
  batch 300 loss: 1.3912520170211793
  batch 350 loss: 1.3555395960807801
  batch 400 loss: 1.344587070941925
  batch 450 loss: 1.3511350631713868
  batch 500 loss: 1.3476910424232482
  batch 550 loss: 1.4113462686538696
  batch 600 loss: 1.3714634037017823
  batch 650 loss: 1.3526589846611023
  batch 700 loss: 1.3578080034255982
  batch 750 loss: 1.418573293685913
  batch 800 loss: 1.5286855983734131
  batch 850 loss: 1.4747518968582154
  batch 900 loss: 1.4184649419784545
LOSS train 1.41846 valid 1.35719, valid PER 41.41%
EPOCH 6:
  batch 50 loss: 1.3392735695838929
  batch 100 loss: 1.3595341455936432
  batch 150 loss: 1.3460953664779662
  batch 200 loss: 1.3165074872970581
  batch 250 loss: 1.3869291543960571
  batch 300 loss: 1.3942531847953796
  batch 350 loss: 1.41874103307724
  batch 400 loss: 1.3534650301933289
  batch 450 loss: 1.3618478751182557
  batch 500 loss: 1.3632984721660615
  batch 550 loss: 1.4626193499565125
  batch 600 loss: 1.4080153846740722
  batch 650 loss: 1.4044210839271545
  batch 700 loss: 1.3742016339302063
  batch 750 loss: 1.4809356451034545
  batch 800 loss: 1.5428636169433594
  batch 850 loss: 1.5161611199378968
  batch 900 loss: 1.5054909467697144
LOSS train 1.50549 valid 1.45538, valid PER 47.65%
EPOCH 7:
  batch 50 loss: 1.4525051164627074
  batch 100 loss: 1.4819088530540467
  batch 150 loss: 1.5246664357185364
  batch 200 loss: 1.7136826229095459
  batch 250 loss: 1.6648270535469054
  batch 300 loss: 1.6047491431236267
  batch 350 loss: 1.5803715109825134
  batch 400 loss: 1.5228627610206604
  batch 450 loss: 1.5150025224685668
  batch 500 loss: 1.5376274919509887
  batch 550 loss: 1.462441267967224
  batch 600 loss: 1.4961625242233276
  batch 650 loss: 1.4589781665802002
  batch 700 loss: 1.5173967456817627
  batch 750 loss: 1.5066373610496522
  batch 800 loss: 1.4973651790618896
  batch 850 loss: 1.4908332848548889
  batch 900 loss: 1.4904246973991393
LOSS train 1.49042 valid 1.46421, valid PER 53.03%
EPOCH 8:
  batch 50 loss: 1.4789328408241271
  batch 100 loss: 1.4205702066421508
  batch 150 loss: 1.4674114060401917
  batch 200 loss: 1.4560564541816712
  batch 250 loss: 1.4314106702804565
  batch 300 loss: 1.4323067688941955
  batch 350 loss: 1.4596088194847108
  batch 400 loss: 1.4600759601593019
  batch 450 loss: 1.5307910346984863
  batch 500 loss: 1.4971948790550231
  batch 550 loss: 1.4760981893539429
  batch 600 loss: 1.4488547897338868
  batch 650 loss: 1.4346712112426758
  batch 700 loss: 1.4851423597335816
  batch 750 loss: 1.4860067677497864
  batch 800 loss: 1.4811443996429443
  batch 850 loss: 1.4655621528625489
  batch 900 loss: 1.4457292366027832
LOSS train 1.44573 valid 1.43562, valid PER 52.61%
EPOCH 9:
  batch 50 loss: 1.410925760269165
  batch 100 loss: 1.417893648147583
  batch 150 loss: 1.4212483477592468
  batch 200 loss: 1.387467908859253
  batch 250 loss: 1.3675241470336914
  batch 300 loss: 1.3887130641937255
  batch 350 loss: 1.381022481918335
  batch 400 loss: 1.4096171689033508
  batch 450 loss: 1.4220527505874634
  batch 500 loss: 1.3804415464401245
  batch 550 loss: 1.4226238656044006
  batch 600 loss: 1.4629263091087341
  batch 650 loss: 1.4262036108970642
  batch 700 loss: 1.3956297707557679
  batch 750 loss: 1.395951647758484
  batch 800 loss: 1.4638222432136536
  batch 850 loss: 1.4224047136306763
  batch 900 loss: 1.3884005045890808
LOSS train 1.38840 valid 1.38213, valid PER 48.91%
EPOCH 10:
  batch 50 loss: 1.375117335319519
  batch 100 loss: 1.3974710011482239
  batch 150 loss: 1.3930226135253907
  batch 200 loss: 1.3772637820243836
  batch 250 loss: 1.357020034790039
  batch 300 loss: 1.3621976947784424
  batch 350 loss: 1.3762414741516114
  batch 400 loss: 1.353840754032135
  batch 450 loss: 1.3437252283096313
  batch 500 loss: 1.3370129084587097
  batch 550 loss: 1.367908957004547
  batch 600 loss: 1.3554690027236937
  batch 650 loss: 1.3639139938354492
  batch 700 loss: 1.3879236578941345
  batch 750 loss: 1.4058653235435485
  batch 800 loss: 1.4895499897003175
  batch 850 loss: 1.4349583315849304
  batch 900 loss: 1.3830865240097046
LOSS train 1.38309 valid 1.36146, valid PER 46.99%
EPOCH 11:
  batch 50 loss: 1.3678220677375794
  batch 100 loss: 1.3508064126968384
  batch 150 loss: 1.3213797891139984
  batch 200 loss: 1.314326992034912
  batch 250 loss: 1.3047396922111512
  batch 300 loss: 1.3177450394630432
  batch 350 loss: 1.334344129562378
  batch 400 loss: 1.3234302496910095
  batch 450 loss: 1.34558931350708
  batch 500 loss: 1.329555139541626
  batch 550 loss: 1.3575736355781556
  batch 600 loss: 1.3052692675590516
  batch 650 loss: 1.338065458536148
  batch 700 loss: 1.418880558013916
  batch 750 loss: 1.3406089973449706
  batch 800 loss: 1.3450129008293152
  batch 850 loss: 1.3263702511787414
  batch 900 loss: 1.4161716413497925
LOSS train 1.41617 valid 1.39464, valid PER 45.67%
EPOCH 12:
  batch 50 loss: 1.3692442512512206
  batch 100 loss: 1.2825366568565368
  batch 150 loss: 1.3318111324310302
  batch 200 loss: 1.3521023035049438
  batch 250 loss: 1.3349354994297027
  batch 300 loss: 1.3860967040061951
  batch 350 loss: 1.343657946586609
  batch 400 loss: 1.3108612036705016
  batch 450 loss: 1.286289085149765
  batch 500 loss: 1.3502403402328491
  batch 550 loss: 1.319760549068451
  batch 600 loss: 1.310707802772522
  batch 650 loss: 1.3141466784477234
  batch 700 loss: 1.2643380117416383
  batch 750 loss: 1.3149685227870942
  batch 800 loss: 1.2741331481933593
  batch 850 loss: 1.3065198278427124
  batch 900 loss: 1.286479775905609
LOSS train 1.28648 valid 1.31568, valid PER 41.37%
EPOCH 13:
  batch 50 loss: 1.2699675714969636
  batch 100 loss: 1.2515147471427917
  batch 150 loss: 1.282900151014328
  batch 200 loss: 1.244040813446045
  batch 250 loss: 1.3367061448097228
  batch 300 loss: 1.4203463578224182
  batch 350 loss: 1.3267649602890015
  batch 400 loss: 1.3459766244888305
  batch 450 loss: 1.3326556491851806
  batch 500 loss: 1.2979189598560332
  batch 550 loss: 1.36445272564888
  batch 600 loss: 1.3039180135726929
  batch 650 loss: 1.279860486984253
  batch 700 loss: 1.300642068386078
  batch 750 loss: 1.2599447870254517
  batch 800 loss: 1.3060786604881287
  batch 850 loss: 1.2792146265506745
  batch 900 loss: 1.2595724666118622
LOSS train 1.25957 valid 1.29473, valid PER 40.14%
EPOCH 14:
  batch 50 loss: 1.2646584248542785
  batch 100 loss: 1.2389813792705535
  batch 150 loss: 1.249813951253891
  batch 200 loss: 1.2577506685256958
  batch 250 loss: 1.2207378602027894
  batch 300 loss: 1.2293854808807374
  batch 350 loss: 1.2345541751384734
  batch 400 loss: 1.2731542110443115
  batch 450 loss: 1.2443271839618684
  batch 500 loss: 1.2430568218231202
  batch 550 loss: 1.2812023401260375
  batch 600 loss: 1.2553719615936278
  batch 650 loss: 1.2887835144996642
  batch 700 loss: 1.2690293419361114
  batch 750 loss: 1.2702174091339111
  batch 800 loss: 1.2558644723892212
  batch 850 loss: 1.3059573757648468
  batch 900 loss: 1.261393266916275
LOSS train 1.26139 valid 1.26149, valid PER 39.88%
EPOCH 15:
  batch 50 loss: 1.1866946172714234
  batch 100 loss: 1.2121185958385468
  batch 150 loss: 1.2087056279182433
  batch 200 loss: 1.258609458208084
  batch 250 loss: 1.251029477119446
  batch 300 loss: 1.224493772983551
  batch 350 loss: 1.2212325942516327
  batch 400 loss: 1.2846453404426574
  batch 450 loss: 1.2616459238529205
  batch 500 loss: 1.32129842877388
  batch 550 loss: 1.3235305666923523
  batch 600 loss: 1.3438596320152283
  batch 650 loss: 1.2738809871673584
  batch 700 loss: 1.2628368353843689
  batch 750 loss: 1.3225644254684448
  batch 800 loss: 1.2744457852840423
  batch 850 loss: 1.2505154240131378
  batch 900 loss: 1.2282925951480865
LOSS train 1.22829 valid 1.25517, valid PER 39.79%
EPOCH 16:
  batch 50 loss: 1.211421914100647
  batch 100 loss: 1.2332210981845855
  batch 150 loss: 1.2588099217414856
  batch 200 loss: 1.320088927745819
  batch 250 loss: 1.3072311902046203
  batch 300 loss: 1.3259533500671388
  batch 350 loss: 1.3064773631095887
  batch 400 loss: 1.2718700003623962
  batch 450 loss: 1.3235684370994567
  batch 500 loss: 1.3042304611206055
  batch 550 loss: 1.2574156904220581
  batch 600 loss: 1.2852573823928832
  batch 650 loss: 1.2835860884189605
  batch 700 loss: 1.2687265968322754
  batch 750 loss: 1.2964368307590484
  batch 800 loss: 1.261322226524353
  batch 850 loss: 1.2660479283332824
  batch 900 loss: 1.2824548816680907
LOSS train 1.28245 valid 1.29371, valid PER 39.53%
EPOCH 17:
  batch 50 loss: 1.2411996495723725
  batch 100 loss: 1.1870484125614167
  batch 150 loss: 1.2767009973526
  batch 200 loss: 1.208398300409317
  batch 250 loss: 1.2315434992313385
  batch 300 loss: 1.2332107996940613
  batch 350 loss: 1.2353520119190216
  batch 400 loss: 1.2494107151031495
  batch 450 loss: 1.2404847836494446
  batch 500 loss: 1.2064175605773926
  batch 550 loss: 1.2243794322013855
  batch 600 loss: 1.2587357831001282
  batch 650 loss: 1.22771542429924
  batch 700 loss: 1.2321201610565184
  batch 750 loss: 1.1831449687480926
  batch 800 loss: 1.2561152172088623
  batch 850 loss: 1.2435091125965119
  batch 900 loss: 1.218089860677719
LOSS train 1.21809 valid 1.21252, valid PER 37.46%
EPOCH 18:
  batch 50 loss: 1.1743223798274993
  batch 100 loss: 1.1934910356998443
  batch 150 loss: 1.2257815408706665
  batch 200 loss: 1.2060480666160585
  batch 250 loss: 1.2005746483802795
  batch 300 loss: 1.2228402256965638
  batch 350 loss: 1.2033815062046052
  batch 400 loss: 1.1940879726409912
  batch 450 loss: 1.2535674965381622
  batch 500 loss: 1.283972111940384
  batch 550 loss: 1.3040619742870332
  batch 600 loss: 1.2582242965698243
  batch 650 loss: 1.204688513278961
  batch 700 loss: 1.201126629114151
  batch 750 loss: 1.2347678804397584
  batch 800 loss: 1.2582967126369475
  batch 850 loss: 1.2453899908065795
  batch 900 loss: 1.2381293892860412
LOSS train 1.23813 valid 1.23617, valid PER 38.57%
EPOCH 19:
  batch 50 loss: 1.2202612960338592
  batch 100 loss: 1.2318073761463166
  batch 150 loss: 1.2436864113807677
  batch 200 loss: 1.3065237927436828
  batch 250 loss: 1.3343110656738282
  batch 300 loss: 1.2801826000213623
  batch 350 loss: 1.2943730187416076
  batch 400 loss: 1.2625640547275543
  batch 450 loss: 1.1873676347732545
  batch 500 loss: 1.236965801715851
  batch 550 loss: 1.310738397836685
  batch 600 loss: 1.2906561613082885
  batch 650 loss: 1.2596912062168122
  batch 700 loss: 1.2988352108001708
  batch 750 loss: 1.2114148366451263
  batch 800 loss: 1.21071032166481
  batch 850 loss: 1.244045488834381
  batch 900 loss: 1.2429344379901885
LOSS train 1.24293 valid 1.23959, valid PER 39.01%
EPOCH 20:
  batch 50 loss: 1.182228583097458
  batch 100 loss: 1.2335172867774964
  batch 150 loss: 1.2257547605037689
  batch 200 loss: 1.2180005192756653
  batch 250 loss: 1.2595311796665192
  batch 300 loss: 1.2308611297607421
  batch 350 loss: 1.2286515426635742
  batch 400 loss: 1.2652428472042083
  batch 450 loss: 1.2416077613830567
  batch 500 loss: 1.2487874877452851
  batch 550 loss: 1.2312031769752503
  batch 600 loss: 1.197808117866516
  batch 650 loss: 1.2414377975463866
  batch 700 loss: 1.2373215079307556
  batch 750 loss: 1.216522557735443
  batch 800 loss: 1.2324906861782075
  batch 850 loss: 1.247900356054306
  batch 900 loss: 1.1976206493377686
LOSS train 1.19762 valid 1.25518, valid PER 38.68%
[1.5980576872825623, 1.4195031905174256, 1.4555007696151734, 1.4611097502708434, 1.4184649419784545, 1.5054909467697144, 1.4904246973991393, 1.4457292366027832, 1.3884005045890808, 1.3830865240097046, 1.4161716413497925, 1.286479775905609, 1.2595724666118622, 1.261393266916275, 1.2282925951480865, 1.2824548816680907, 1.218089860677719, 1.2381293892860412, 1.2429344379901885, 1.1976206493377686]
[1.5153404474258423, 1.4097527265548706, 1.3805397748947144, 1.4142075777053833, 1.3571871519088745, 1.4553837776184082, 1.4642117023468018, 1.4356212615966797, 1.3821287155151367, 1.361464023590088, 1.3946431875228882, 1.3156843185424805, 1.2947313785552979, 1.2614877223968506, 1.2551705837249756, 1.2937133312225342, 1.2125244140625, 1.2361721992492676, 1.239586591720581, 1.255178451538086]
Training finished in 17.0 minutes.
Model saved to checkpoints/20230120_135124/model_17
Loading model from checkpoints/20230120_135124/model_17
SUB: 18.37%, DEL: 19.76%, INS: 1.47%, COR: 61.88%, PER: 39.59%
