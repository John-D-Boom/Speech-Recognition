Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.005, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.2, beta1=0.9, beta2=0.999)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.952453737258911
  batch 100 loss: 2.9959703540802
  batch 150 loss: 2.663453764915466
  batch 200 loss: 2.414073610305786
  batch 250 loss: 2.252744734287262
  batch 300 loss: 2.1183057856559753
  batch 350 loss: 1.9919616746902467
  batch 400 loss: 1.9815299558639525
  batch 450 loss: 1.885239064693451
  batch 500 loss: 1.8222788882255554
  batch 550 loss: 1.7569192123413087
  batch 600 loss: 1.7451721358299255
  batch 650 loss: 1.6419354915618896
  batch 700 loss: 1.6876899123191833
  batch 750 loss: 1.652030725479126
  batch 800 loss: 1.642009470462799
  batch 850 loss: 1.5828057050704956
  batch 900 loss: 1.5779500126838684
LOSS train 1.57795 valid 1.53469, valid PER 52.13%
EPOCH 2:
  batch 50 loss: 1.5671996068954468
  batch 100 loss: 1.516871347427368
  batch 150 loss: 1.4591970229148865
  batch 200 loss: 1.4435146641731262
  batch 250 loss: 1.4763541507720948
  batch 300 loss: 1.428899974822998
  batch 350 loss: 1.451929633617401
  batch 400 loss: 1.4398083972930908
  batch 450 loss: 1.3979117369651795
  batch 500 loss: 1.4165998458862306
  batch 550 loss: 1.4124175453186034
  batch 600 loss: 1.3818881964683534
  batch 650 loss: 1.3494205737113953
  batch 700 loss: 1.357596573829651
  batch 750 loss: 1.3587519550323486
  batch 800 loss: 1.294094786643982
  batch 850 loss: 1.3176666402816772
  batch 900 loss: 1.293868224620819
LOSS train 1.29387 valid 1.26637, valid PER 41.10%
EPOCH 3:
  batch 50 loss: 1.2436450219154358
  batch 100 loss: 1.309760844707489
  batch 150 loss: 1.325544272661209
  batch 200 loss: 1.256948083639145
  batch 250 loss: 1.2704636716842652
  batch 300 loss: 1.2526407158374786
  batch 350 loss: 1.2732253170013428
  batch 400 loss: 1.2533270299434662
  batch 450 loss: 1.237959508895874
  batch 500 loss: 1.2306121623516082
  batch 550 loss: 1.2436948001384736
  batch 600 loss: 1.2045894622802735
  batch 650 loss: 1.2050590765476228
  batch 700 loss: 1.2216789150238037
  batch 750 loss: 1.2348379588127136
  batch 800 loss: 1.254753451347351
  batch 850 loss: 1.2236701607704163
  batch 900 loss: 1.222587811946869
LOSS train 1.22259 valid 1.16080, valid PER 36.80%
EPOCH 4:
  batch 50 loss: 1.2227307772636413
  batch 100 loss: 1.1688031220436097
  batch 150 loss: 1.2106926918029786
  batch 200 loss: 1.2001460480690003
  batch 250 loss: 1.190906640291214
  batch 300 loss: 1.1929587936401367
  batch 350 loss: 1.1439980506896972
  batch 400 loss: 1.1408322286605834
  batch 450 loss: 1.1432796716690063
  batch 500 loss: 1.2325627791881562
  batch 550 loss: 1.1599485242366792
  batch 600 loss: 1.1284333181381225
  batch 650 loss: 1.1934766316413878
  batch 700 loss: 1.1782721996307373
  batch 750 loss: 1.1604082703590393
  batch 800 loss: 1.1427333390712737
  batch 850 loss: 1.1340621101856232
  batch 900 loss: 1.1392034006118774
LOSS train 1.13920 valid 1.11895, valid PER 35.20%
EPOCH 5:
  batch 50 loss: 1.1265620744228364
  batch 100 loss: 1.0924149143695832
  batch 150 loss: 1.1426109218597411
  batch 200 loss: 1.1745247185230254
  batch 250 loss: 1.1179578614234924
  batch 300 loss: 1.1351579201221467
  batch 350 loss: 1.1060174524784088
  batch 400 loss: 1.0865077900886535
  batch 450 loss: 1.1040415382385254
  batch 500 loss: 1.083924858570099
  batch 550 loss: 1.1394852828979491
  batch 600 loss: 1.1302974331378937
  batch 650 loss: 1.1228043663501739
  batch 700 loss: 1.1188247907161712
  batch 750 loss: 1.1225237023830414
  batch 800 loss: 1.1408003282546997
  batch 850 loss: 1.132748554944992
  batch 900 loss: 1.1006758570671082
LOSS train 1.10068 valid 1.10708, valid PER 34.05%
EPOCH 6:
  batch 50 loss: 1.0782204747200013
  batch 100 loss: 1.079187273979187
  batch 150 loss: 1.0794575381278992
  batch 200 loss: 1.0527388095855712
  batch 250 loss: 1.0694504415988921
  batch 300 loss: 1.0954816734790802
  batch 350 loss: 1.0866106426715851
  batch 400 loss: 1.0901872277259828
  batch 450 loss: 1.0967934441566467
  batch 500 loss: 1.0772554171085358
  batch 550 loss: 1.0958339703083038
  batch 600 loss: 1.0642855381965637
  batch 650 loss: 1.0645528101921082
  batch 700 loss: 1.059804345369339
  batch 750 loss: 1.089989379644394
  batch 800 loss: 1.0878601777553558
  batch 850 loss: 1.1018675446510315
  batch 900 loss: 1.0963869524002074
LOSS train 1.09639 valid 1.07690, valid PER 33.26%
EPOCH 7:
  batch 50 loss: 1.024318701028824
  batch 100 loss: 1.0870345306396485
  batch 150 loss: 1.028222234249115
  batch 200 loss: 1.0216637110710145
  batch 250 loss: 1.0810460245609284
  batch 300 loss: 1.082059110403061
  batch 350 loss: 1.1170223295688628
  batch 400 loss: 1.044460551738739
  batch 450 loss: 1.052375524044037
  batch 500 loss: 1.0330435883998872
  batch 550 loss: 1.0318952369689942
  batch 600 loss: 1.0521996665000914
  batch 650 loss: 1.0117184937000274
  batch 700 loss: 1.0628505206108094
  batch 750 loss: 1.0391443324089051
  batch 800 loss: 1.0215990257263183
  batch 850 loss: 1.0294222915172577
  batch 900 loss: 1.0409526097774506
LOSS train 1.04095 valid 1.04943, valid PER 32.84%
EPOCH 8:
  batch 50 loss: 1.0202023482322693
  batch 100 loss: 0.9804468309879303
  batch 150 loss: 1.0224619698524475
  batch 200 loss: 1.0126896023750305
  batch 250 loss: 1.0110294139385223
  batch 300 loss: 0.9880256009101868
  batch 350 loss: 1.0222911107540131
  batch 400 loss: 1.03379230260849
  batch 450 loss: 1.0710096144676209
  batch 500 loss: 1.0548914194107055
  batch 550 loss: 1.0548531198501587
  batch 600 loss: 0.9920071339607239
  batch 650 loss: 1.0067816269397736
  batch 700 loss: 1.0266907167434693
  batch 750 loss: 1.047337485551834
  batch 800 loss: 1.0476466703414917
  batch 850 loss: 1.0440571641921996
  batch 900 loss: 1.0243113243579864
LOSS train 1.02431 valid 1.06505, valid PER 33.77%
EPOCH 9:
  batch 50 loss: 1.01184055685997
  batch 100 loss: 0.9812748312950135
  batch 150 loss: 1.0162209165096283
  batch 200 loss: 0.9939058578014374
  batch 250 loss: 0.9776456916332245
  batch 300 loss: 1.0232252728939057
  batch 350 loss: 1.0204760324954987
  batch 400 loss: 1.0208865046501159
  batch 450 loss: 1.0415628409385682
  batch 500 loss: 1.0147480165958405
  batch 550 loss: 1.0029414117336273
  batch 600 loss: 1.0353271579742431
  batch 650 loss: 1.0495035600662233
  batch 700 loss: 1.0662075912952422
  batch 750 loss: 1.0147727823257446
  batch 800 loss: 1.0377832710742951
  batch 850 loss: 1.0404787898063659
  batch 900 loss: 1.0006981945037843
LOSS train 1.00070 valid 1.01622, valid PER 31.66%
EPOCH 10:
  batch 50 loss: 0.9444476461410523
  batch 100 loss: 0.9898404574394226
  batch 150 loss: 1.0072734832763672
  batch 200 loss: 0.9831755030155181
  batch 250 loss: 1.0509902548789978
  batch 300 loss: 1.0031760156154632
  batch 350 loss: 0.9888812589645386
  batch 400 loss: 0.9884937286376954
  batch 450 loss: 1.0142446947097778
  batch 500 loss: 1.016446865797043
  batch 550 loss: 1.0204883158206939
  batch 600 loss: 1.0131998407840728
  batch 650 loss: 1.1063668823242188
  batch 700 loss: 1.0589765787124634
  batch 750 loss: 1.0380923414230347
  batch 800 loss: 1.0148225128650665
  batch 850 loss: 1.0138610780239106
  batch 900 loss: 0.9820162081718444
LOSS train 0.98202 valid 1.03260, valid PER 31.95%
EPOCH 11:
  batch 50 loss: 0.9907018744945526
  batch 100 loss: 0.9767493855953217
  batch 150 loss: 0.968755624294281
  batch 200 loss: 0.9510069537162781
  batch 250 loss: 0.9655367302894592
  batch 300 loss: 0.9575159323215484
  batch 350 loss: 1.0102336657047273
  batch 400 loss: 0.963541511297226
  batch 450 loss: 0.9459144389629364
  batch 500 loss: 0.9603038537502289
  batch 550 loss: 0.9880916583538055
  batch 600 loss: 0.9775782322883606
  batch 650 loss: 0.9925884139537812
  batch 700 loss: 1.0667444813251494
  batch 750 loss: 0.9998265755176544
  batch 800 loss: 0.9919888043403625
  batch 850 loss: 0.9806481528282166
  batch 900 loss: 1.0250601410865783
LOSS train 1.02506 valid 1.01041, valid PER 31.78%
EPOCH 12:
  batch 50 loss: 0.9328212201595306
  batch 100 loss: 0.9388840687274933
  batch 150 loss: 0.9552772343158722
  batch 200 loss: 0.9835325074195862
  batch 250 loss: 0.9632683944702148
  batch 300 loss: 0.9929724729061127
  batch 350 loss: 0.9571528136730194
  batch 400 loss: 0.9788372850418091
  batch 450 loss: 0.9368828427791596
  batch 500 loss: 0.9728753471374512
  batch 550 loss: 0.9744290602207184
  batch 600 loss: 0.9953404688835144
  batch 650 loss: 0.9812426459789276
  batch 700 loss: 0.9517401587963105
  batch 750 loss: 0.9978262603282928
  batch 800 loss: 0.9778387939929962
  batch 850 loss: 1.018868980407715
  batch 900 loss: 1.0267520356178284
LOSS train 1.02675 valid 1.06080, valid PER 32.88%
EPOCH 13:
  batch 50 loss: 0.9428857302665711
  batch 100 loss: 0.9610702264308929
  batch 150 loss: 1.0132628345489503
  batch 200 loss: 0.9791392958164216
  batch 250 loss: 0.9769631278514862
  batch 300 loss: 1.048301328420639
  batch 350 loss: 0.9582631647586822
  batch 400 loss: 0.9627539348602295
  batch 450 loss: 0.9673084640502929
  batch 500 loss: 0.9520659530162812
  batch 550 loss: 1.0092913353443145
  batch 600 loss: 0.9750088107585907
  batch 650 loss: 0.9650728285312653
  batch 700 loss: 0.9590881478786468
  batch 750 loss: 0.9287833428382873
  batch 800 loss: 0.9724675261974335
  batch 850 loss: 0.9480530405044556
  batch 900 loss: 0.9465835154056549
LOSS train 0.94658 valid 1.02107, valid PER 31.80%
EPOCH 14:
  batch 50 loss: 0.9084865868091583
  batch 100 loss: 0.9051829016208649
  batch 150 loss: 0.9316792178153992
  batch 200 loss: 0.9220102334022522
  batch 250 loss: 0.9193321180343628
  batch 300 loss: 0.9115899121761322
  batch 350 loss: 0.9446072089672088
  batch 400 loss: 0.9343096220493317
  batch 450 loss: 0.9189405262470245
  batch 500 loss: 0.9300675332546234
  batch 550 loss: 0.9521960234642028
  batch 600 loss: 0.9424599957466125
  batch 650 loss: 0.9519456958770752
  batch 700 loss: 1.0017987728118896
  batch 750 loss: 0.9457122778892517
  batch 800 loss: 0.942078731060028
  batch 850 loss: 0.9896438264846802
  batch 900 loss: 0.9640160346031189
LOSS train 0.96402 valid 1.04229, valid PER 32.32%
EPOCH 15:
  batch 50 loss: 0.9147466731071472
  batch 100 loss: 0.9422159707546234
  batch 150 loss: 0.9140435314178467
  batch 200 loss: 0.9573247134685516
  batch 250 loss: 0.9782807970046997
  batch 300 loss: 0.9574903869628906
  batch 350 loss: 0.9251029717922211
  batch 400 loss: 0.9618501234054565
  batch 450 loss: 0.9312668430805207
  batch 500 loss: 0.9267464172840119
  batch 550 loss: 0.9858493185043335
  batch 600 loss: 1.0114477276802063
  batch 650 loss: 0.9347930085659027
  batch 700 loss: 0.9526567959785461
  batch 750 loss: 0.9989722752571106
  batch 800 loss: 0.9499908399581909
  batch 850 loss: 0.9287644863128662
  batch 900 loss: 0.8908310079574585
LOSS train 0.89083 valid 1.01288, valid PER 31.53%
EPOCH 16:
  batch 50 loss: 0.8903810942173004
  batch 100 loss: 0.8808777749538421
  batch 150 loss: 0.9273098862171173
  batch 200 loss: 0.9364154469966889
  batch 250 loss: 0.9328930974006653
  batch 300 loss: 0.9411720407009124
  batch 350 loss: 0.9411876928806305
  batch 400 loss: 0.9213767695426941
  batch 450 loss: 0.948556717634201
  batch 500 loss: 0.9635303819179535
  batch 550 loss: 0.9537779414653778
  batch 600 loss: 0.9763040947914123
  batch 650 loss: 0.9629786825180053
  batch 700 loss: 0.9565337240695954
  batch 750 loss: 0.9725424456596374
  batch 800 loss: 0.958071151971817
  batch 850 loss: 0.9777984833717346
  batch 900 loss: 0.9721165347099304
LOSS train 0.97212 valid 1.02562, valid PER 32.05%
EPOCH 17:
  batch 50 loss: 0.9850716137886047
  batch 100 loss: 0.9461195755004883
  batch 150 loss: 1.0364224696159363
  batch 200 loss: 0.9933745181560516
  batch 250 loss: 0.9690024375915527
  batch 300 loss: 0.9445109355449677
  batch 350 loss: 0.9582912790775299
  batch 400 loss: 0.9511328792572021
  batch 450 loss: 0.9655232393741607
  batch 500 loss: 0.9522045016288757
  batch 550 loss: 0.9546767568588257
  batch 600 loss: 1.0127883672714233
  batch 650 loss: 0.9472096025943756
  batch 700 loss: 0.9785140001773834
  batch 750 loss: 0.926899334192276
  batch 800 loss: 0.9705649244785309
  batch 850 loss: 0.9684287738800049
  batch 900 loss: 0.9596708655357361
LOSS train 0.95967 valid 1.05807, valid PER 32.46%
EPOCH 18:
  batch 50 loss: 0.9248242378234863
  batch 100 loss: 0.8993601989746094
  batch 150 loss: 0.9363078093528747
  batch 200 loss: 0.9282528710365295
  batch 250 loss: 0.9114160680770874
  batch 300 loss: 0.9449828672409057
  batch 350 loss: 0.9145858776569367
  batch 400 loss: 0.9081032800674439
  batch 450 loss: 0.9625821053981781
  batch 500 loss: 0.9413240504264831
  batch 550 loss: 1.0125644826889038
  batch 600 loss: 0.9805243766307831
  batch 650 loss: 0.9159110450744629
  batch 700 loss: 0.8984214890003205
  batch 750 loss: 0.957113915681839
  batch 800 loss: 0.9617661845684051
  batch 850 loss: 0.9663542914390564
  batch 900 loss: 0.9389276850223541
LOSS train 0.93893 valid 1.03633, valid PER 31.61%
EPOCH 19:
  batch 50 loss: 0.9154083836078644
  batch 100 loss: 0.9147038161754608
  batch 150 loss: 0.8844265103340149
  batch 200 loss: 0.8897602522373199
  batch 250 loss: 0.9121451592445373
  batch 300 loss: 0.9151861846446991
  batch 350 loss: 0.9353752756118774
  batch 400 loss: 0.9337288856506347
  batch 450 loss: 0.8801134467124939
  batch 500 loss: 0.9183069002628327
  batch 550 loss: 0.9765671277046204
  batch 600 loss: 0.9255215978622436
  batch 650 loss: 0.9535599136352539
  batch 700 loss: 0.957555969953537
  batch 750 loss: 0.9241607511043548
  batch 800 loss: 0.8978167140483856
  batch 850 loss: 0.93937854886055
  batch 900 loss: 0.9291480362415314
LOSS train 0.92915 valid 1.01097, valid PER 30.63%
EPOCH 20:
  batch 50 loss: 0.836216675043106
  batch 100 loss: 0.8956193482875824
  batch 150 loss: 0.8924168288707733
  batch 200 loss: 0.9142196226119995
  batch 250 loss: 0.939345326423645
  batch 300 loss: 0.9123575580120087
  batch 350 loss: 0.9098789954185486
  batch 400 loss: 0.8934973120689392
  batch 450 loss: 0.9111645233631134
  batch 500 loss: 0.919486768245697
  batch 550 loss: 0.9080515074729919
  batch 600 loss: 0.9007610213756562
  batch 650 loss: 0.9203338646888732
  batch 700 loss: 0.920133900642395
  batch 750 loss: 0.8945098280906677
  batch 800 loss: 0.9049347484111786
  batch 850 loss: 0.9152371251583099
  batch 900 loss: 0.8894171035289764
LOSS train 0.88942 valid 1.02500, valid PER 30.96%
[1.5779500126838684, 1.293868224620819, 1.222587811946869, 1.1392034006118774, 1.1006758570671082, 1.0963869524002074, 1.0409526097774506, 1.0243113243579864, 1.0006981945037843, 0.9820162081718444, 1.0250601410865783, 1.0267520356178284, 0.9465835154056549, 0.9640160346031189, 0.8908310079574585, 0.9721165347099304, 0.9596708655357361, 0.9389276850223541, 0.9291480362415314, 0.8894171035289764]
[1.5346944332122803, 1.2663733959197998, 1.1608004570007324, 1.1189512014389038, 1.1070834398269653, 1.0769017934799194, 1.0494306087493896, 1.0650534629821777, 1.0162171125411987, 1.0326015949249268, 1.0104103088378906, 1.060803771018982, 1.0210694074630737, 1.0422886610031128, 1.0128824710845947, 1.025618314743042, 1.0580658912658691, 1.0363277196884155, 1.0109680891036987, 1.024997591972351]
Training finished in 19.0 minutes.
Model saved to checkpoints/20230120_160440/model_11
Loading model from checkpoints/20230120_160440/model_11
SUB: 17.41%, DEL: 14.83%, INS: 1.54%, COR: 67.76%, PER: 33.78%
