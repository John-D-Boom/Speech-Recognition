Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.001, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.2, beta1=0.9, beta2=0.9)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 8.88125039100647
  batch 100 loss: 3.2974397325515747
  batch 150 loss: 3.1225823831558226
  batch 200 loss: 2.9885786390304565
  batch 250 loss: 2.8579185390472412
  batch 300 loss: 2.685218720436096
  batch 350 loss: 2.5033071136474607
  batch 400 loss: 2.4267925024032593
  batch 450 loss: 2.347706184387207
  batch 500 loss: 2.2095160913467407
  batch 550 loss: 2.1017962098121643
  batch 600 loss: 2.0556731939315798
  batch 650 loss: 1.9815723514556884
  batch 700 loss: 1.946533203125
  batch 750 loss: 1.9092866778373718
  batch 800 loss: 1.884093005657196
  batch 850 loss: 1.8462770509719848
  batch 900 loss: 1.8209420943260193
LOSS train 1.82094 valid 1.77314, valid PER 68.80%
EPOCH 2:
  batch 50 loss: 1.780157551765442
  batch 100 loss: 1.7610430145263671
  batch 150 loss: 1.6825598359107972
  batch 200 loss: 1.6886337614059448
  batch 250 loss: 1.6942795133590698
  batch 300 loss: 1.6531322717666626
  batch 350 loss: 1.6506942915916443
  batch 400 loss: 1.629590401649475
  batch 450 loss: 1.6255729222297668
  batch 500 loss: 1.6155731797218322
  batch 550 loss: 1.600083191394806
  batch 600 loss: 1.5738722062110901
  batch 650 loss: 1.5304396033287049
  batch 700 loss: 1.5477049875259399
  batch 750 loss: 1.5451195192337037
  batch 800 loss: 1.4958282613754272
  batch 850 loss: 1.5121368050575257
  batch 900 loss: 1.4990295815467833
LOSS train 1.49903 valid 1.46614, valid PER 52.85%
EPOCH 3:
  batch 50 loss: 1.4301523399353027
  batch 100 loss: 1.4931076264381409
  batch 150 loss: 1.4812030577659607
  batch 200 loss: 1.4322978520393372
  batch 250 loss: 1.4216646194458007
  batch 300 loss: 1.419258108139038
  batch 350 loss: 1.4154565382003783
  batch 400 loss: 1.432590069770813
  batch 450 loss: 1.4112605190277099
  batch 500 loss: 1.374480928182602
  batch 550 loss: 1.3851121735572816
  batch 600 loss: 1.3270215964317322
  batch 650 loss: 1.3553045773506165
  batch 700 loss: 1.3497409582138062
  batch 750 loss: 1.3748698163032531
  batch 800 loss: 1.366101644039154
  batch 850 loss: 1.3326842951774598
  batch 900 loss: 1.3632660162448884
LOSS train 1.36327 valid 1.29853, valid PER 43.59%
EPOCH 4:
  batch 50 loss: 1.3368263602256776
  batch 100 loss: 1.2749673843383789
  batch 150 loss: 1.2975927734375
  batch 200 loss: 1.308563370704651
  batch 250 loss: 1.3111094307899476
  batch 300 loss: 1.308075726032257
  batch 350 loss: 1.281116144657135
  batch 400 loss: 1.240575590133667
  batch 450 loss: 1.246352300643921
  batch 500 loss: 1.3262220644950866
  batch 550 loss: 1.24605526804924
  batch 600 loss: 1.239704577922821
  batch 650 loss: 1.2817021822929382
  batch 700 loss: 1.3086509573459626
  batch 750 loss: 1.2617511892318725
  batch 800 loss: 1.2466730642318726
  batch 850 loss: 1.2291616952419282
  batch 900 loss: 1.2489031755924225
LOSS train 1.24890 valid 1.21573, valid PER 39.36%
EPOCH 5:
  batch 50 loss: 1.2257524180412291
  batch 100 loss: 1.191813576221466
  batch 150 loss: 1.228223819732666
  batch 200 loss: 1.2381512904167176
  batch 250 loss: 1.1990186607837676
  batch 300 loss: 1.2151301729679107
  batch 350 loss: 1.1841403496265412
  batch 400 loss: 1.1614696550369263
  batch 450 loss: 1.1892005074024201
  batch 500 loss: 1.1568013107776642
  batch 550 loss: 1.2220688045024872
  batch 600 loss: 1.215557187795639
  batch 650 loss: 1.1978848266601563
  batch 700 loss: 1.1914898359775543
  batch 750 loss: 1.1789466619491578
  batch 800 loss: 1.2035488724708556
  batch 850 loss: 1.2024671518802643
  batch 900 loss: 1.1836947631835937
LOSS train 1.18369 valid 1.15691, valid PER 37.00%
EPOCH 6:
  batch 50 loss: 1.1455823826789855
  batch 100 loss: 1.1844682669639588
  batch 150 loss: 1.1566861879825592
  batch 200 loss: 1.1058813679218291
  batch 250 loss: 1.144968786239624
  batch 300 loss: 1.1708294999599458
  batch 350 loss: 1.1649809968471527
  batch 400 loss: 1.1415034902095795
  batch 450 loss: 1.1479316532611847
  batch 500 loss: 1.127005705833435
  batch 550 loss: 1.1432031345367433
  batch 600 loss: 1.1264888060092926
  batch 650 loss: 1.106919265985489
  batch 700 loss: 1.1272311341762542
  batch 750 loss: 1.141695067882538
  batch 800 loss: 1.1274677217006683
  batch 850 loss: 1.1462372183799743
  batch 900 loss: 1.1639622712135316
LOSS train 1.16396 valid 1.11078, valid PER 35.28%
EPOCH 7:
  batch 50 loss: 1.1039943623542785
  batch 100 loss: 1.1455532109737396
  batch 150 loss: 1.0807558000087738
  batch 200 loss: 1.0966756081581115
  batch 250 loss: 1.146130542755127
  batch 300 loss: 1.0976261281967163
  batch 350 loss: 1.1163537108898163
  batch 400 loss: 1.092233818769455
  batch 450 loss: 1.0869751870632172
  batch 500 loss: 1.0934169495105743
  batch 550 loss: 1.0613596498966218
  batch 600 loss: 1.0808341741561889
  batch 650 loss: 1.0564175713062287
  batch 700 loss: 1.1300719821453094
  batch 750 loss: 1.0828928399085997
  batch 800 loss: 1.0888993775844573
  batch 850 loss: 1.0703153491020203
  batch 900 loss: 1.0743201327323915
LOSS train 1.07432 valid 1.10346, valid PER 34.78%
EPOCH 8:
  batch 50 loss: 1.074563752412796
  batch 100 loss: 1.02495849609375
  batch 150 loss: 1.0639763188362121
  batch 200 loss: 1.0466921365261077
  batch 250 loss: 1.057853809595108
  batch 300 loss: 1.0190609622001647
  batch 350 loss: 1.0397625684738159
  batch 400 loss: 1.0533757305145264
  batch 450 loss: 1.0971856701374054
  batch 500 loss: 1.0659458243846893
  batch 550 loss: 1.0585668647289277
  batch 600 loss: 1.0132646584510803
  batch 650 loss: 1.0251578712463378
  batch 700 loss: 1.083224585056305
  batch 750 loss: 1.0742382192611695
  batch 800 loss: 1.0747820103168488
  batch 850 loss: 1.0378189957141877
  batch 900 loss: 1.0492604851722718
LOSS train 1.04926 valid 1.04833, valid PER 34.03%
EPOCH 9:
  batch 50 loss: 0.9986051201820374
  batch 100 loss: 0.9935573899745941
  batch 150 loss: 1.0064548254013062
  batch 200 loss: 1.0068406760692596
  batch 250 loss: 0.9972234070301056
  batch 300 loss: 1.031377010345459
  batch 350 loss: 0.9995112025737762
  batch 400 loss: 1.0329684138298034
  batch 450 loss: 1.0379087913036347
  batch 500 loss: 1.0177406787872314
  batch 550 loss: 1.027259019613266
  batch 600 loss: 1.0491813552379607
  batch 650 loss: 1.0491375195980073
  batch 700 loss: 1.0020389258861542
  batch 750 loss: 1.010912343263626
  batch 800 loss: 1.0725459492206573
  batch 850 loss: 1.053184609413147
  batch 900 loss: 0.9976801991462707
LOSS train 0.99768 valid 1.04546, valid PER 32.72%
EPOCH 10:
  batch 50 loss: 0.9928420531749725
  batch 100 loss: 1.0115599274635314
  batch 150 loss: 1.0239190542697907
  batch 200 loss: 0.976346207857132
  batch 250 loss: 1.0020877385139466
  batch 300 loss: 0.9962492656707763
  batch 350 loss: 0.9845565915107727
  batch 400 loss: 0.9606231093406677
  batch 450 loss: 0.9659222483634948
  batch 500 loss: 0.9767026054859161
  batch 550 loss: 0.9909998011589051
  batch 600 loss: 1.0018959629535675
  batch 650 loss: 1.0127121722698211
  batch 700 loss: 1.0084275317192077
  batch 750 loss: 1.0256065034866333
  batch 800 loss: 1.013172025680542
  batch 850 loss: 0.991022834777832
  batch 900 loss: 0.9916737866401673
LOSS train 0.99167 valid 1.02202, valid PER 31.84%
EPOCH 11:
  batch 50 loss: 0.9610724592208862
  batch 100 loss: 0.9711682295799255
  batch 150 loss: 0.955282484292984
  batch 200 loss: 0.9204484987258911
  batch 250 loss: 0.944058735370636
  batch 300 loss: 0.9571088576316833
  batch 350 loss: 0.9881974720954895
  batch 400 loss: 0.9542433893680573
  batch 450 loss: 0.9581067621707916
  batch 500 loss: 0.959310554265976
  batch 550 loss: 0.9727026164531708
  batch 600 loss: 0.9705547177791596
  batch 650 loss: 0.9743842828273773
  batch 700 loss: 1.0650131595134735
  batch 750 loss: 0.966509244441986
  batch 800 loss: 0.9719694995880127
  batch 850 loss: 0.9610203802585602
  batch 900 loss: 0.9887635278701782
LOSS train 0.98876 valid 1.01373, valid PER 31.96%
EPOCH 12:
  batch 50 loss: 0.9160900378227234
  batch 100 loss: 0.9107072198390961
  batch 150 loss: 0.9447112858295441
  batch 200 loss: 0.9733859097957611
  batch 250 loss: 0.9180712962150573
  batch 300 loss: 0.9653837382793427
  batch 350 loss: 0.9621923828125
  batch 400 loss: 0.9738457119464874
  batch 450 loss: 0.9274981057643891
  batch 500 loss: 0.9610731101036072
  batch 550 loss: 0.9568906223773956
  batch 600 loss: 0.9478461718559266
  batch 650 loss: 0.9473856806755065
  batch 700 loss: 0.9238036096096038
  batch 750 loss: 0.9534736037254333
  batch 800 loss: 0.9098379993438721
  batch 850 loss: 0.9478103959560394
  batch 900 loss: 0.948886479139328
LOSS train 0.94889 valid 0.99238, valid PER 31.35%
EPOCH 13:
  batch 50 loss: 0.8998229205608368
  batch 100 loss: 0.9121134984493255
  batch 150 loss: 0.9308094048500061
  batch 200 loss: 0.8979823613166809
  batch 250 loss: 0.9143932127952575
  batch 300 loss: 0.9349888372421264
  batch 350 loss: 0.8987268197536469
  batch 400 loss: 0.9140590822696686
  batch 450 loss: 0.9155295753479004
  batch 500 loss: 0.9248045933246613
  batch 550 loss: 0.9845335328578949
  batch 600 loss: 0.9285083520412445
  batch 650 loss: 0.9225744271278381
  batch 700 loss: 0.9536886703968048
  batch 750 loss: 0.9073372209072113
  batch 800 loss: 0.9372071397304534
  batch 850 loss: 0.9192298769950866
  batch 900 loss: 0.9263403785228729
LOSS train 0.92634 valid 0.98876, valid PER 30.46%
EPOCH 14:
  batch 50 loss: 0.8952027320861816
  batch 100 loss: 0.8690359544754028
  batch 150 loss: 0.8858270740509033
  batch 200 loss: 0.878579671382904
  batch 250 loss: 0.8867577731609344
  batch 300 loss: 0.858477441072464
  batch 350 loss: 0.8993059647083282
  batch 400 loss: 0.9068366646766662
  batch 450 loss: 0.8839139080047608
  batch 500 loss: 0.8915462005138397
  batch 550 loss: 0.9343663048744202
  batch 600 loss: 0.8944857704639435
  batch 650 loss: 0.9232550919055938
  batch 700 loss: 0.9244203543663025
  batch 750 loss: 0.8997405433654785
  batch 800 loss: 0.8991254723072052
  batch 850 loss: 0.9302504634857178
  batch 900 loss: 0.9279448652267456
LOSS train 0.92794 valid 0.96685, valid PER 30.44%
EPOCH 15:
  batch 50 loss: 0.8479393601417542
  batch 100 loss: 0.8675357878208161
  batch 150 loss: 0.8681600284576416
  batch 200 loss: 0.8947175145149231
  batch 250 loss: 0.8900219178199769
  batch 300 loss: 0.8803678572177887
  batch 350 loss: 0.8632495129108428
  batch 400 loss: 0.8988688611984252
  batch 450 loss: 0.8845258557796478
  batch 500 loss: 0.8731687080860138
  batch 550 loss: 0.9234924471378326
  batch 600 loss: 0.9311287343502045
  batch 650 loss: 0.888666626214981
  batch 700 loss: 0.8846179246902466
  batch 750 loss: 0.9305155384540558
  batch 800 loss: 0.8835935294628143
  batch 850 loss: 0.8669654393196106
  batch 900 loss: 0.8394909918308258
LOSS train 0.83949 valid 0.96608, valid PER 30.34%
EPOCH 16:
  batch 50 loss: 0.8735255515575409
  batch 100 loss: 0.8600872349739075
  batch 150 loss: 0.8651199865341187
  batch 200 loss: 0.8618428027629852
  batch 250 loss: 0.846191737651825
  batch 300 loss: 0.8714204287528992
  batch 350 loss: 0.8672090876102447
  batch 400 loss: 0.8474344825744629
  batch 450 loss: 0.8858905220031739
  batch 500 loss: 0.853660660982132
  batch 550 loss: 0.8410770237445832
  batch 600 loss: 0.8922072279453278
  batch 650 loss: 0.8797812151908875
  batch 700 loss: 0.8543202114105225
  batch 750 loss: 0.8743584096431732
  batch 800 loss: 0.8406329441070557
  batch 850 loss: 0.8552647888660431
  batch 900 loss: 0.8636330342292786
LOSS train 0.86363 valid 0.96737, valid PER 29.78%
EPOCH 17:
  batch 50 loss: 0.871043038368225
  batch 100 loss: 0.7934095549583435
  batch 150 loss: 0.8784388232231141
  batch 200 loss: 0.8215135443210602
  batch 250 loss: 0.8372478067874909
  batch 300 loss: 0.8230659854412079
  batch 350 loss: 0.8519112253189087
  batch 400 loss: 0.8699828946590423
  batch 450 loss: 0.8450574171543122
  batch 500 loss: 0.8574877321720124
  batch 550 loss: 0.8538852882385254
  batch 600 loss: 0.8787061047554016
  batch 650 loss: 0.8354649639129639
  batch 700 loss: 0.8533488368988037
  batch 750 loss: 0.8086510860919952
  batch 800 loss: 0.8813041472434997
  batch 850 loss: 0.8759328818321228
  batch 900 loss: 0.8685994148254395
LOSS train 0.86860 valid 0.96385, valid PER 29.79%
EPOCH 18:
  batch 50 loss: 0.809564710855484
  batch 100 loss: 0.8125942885875702
  batch 150 loss: 0.8623521137237549
  batch 200 loss: 0.8418862414360047
  batch 250 loss: 0.834045330286026
  batch 300 loss: 0.8250430190563202
  batch 350 loss: 0.7894760721921921
  batch 400 loss: 0.8307131898403167
  batch 450 loss: 0.8276199805736542
  batch 500 loss: 0.8400486505031586
  batch 550 loss: 0.8746095037460327
  batch 600 loss: 0.8420432102680206
  batch 650 loss: 0.809474710226059
  batch 700 loss: 0.8218265163898468
  batch 750 loss: 0.8388222754001617
  batch 800 loss: 0.8631953608989715
  batch 850 loss: 0.8600464522838592
  batch 900 loss: 0.8411242020130157
LOSS train 0.84112 valid 0.95836, valid PER 29.98%
EPOCH 19:
  batch 50 loss: 0.8169744002819062
  batch 100 loss: 0.8014932185411453
  batch 150 loss: 0.7883382397890091
  batch 200 loss: 0.7820096051692963
  batch 250 loss: 0.8310200548171998
  batch 300 loss: 0.8433785861730576
  batch 350 loss: 0.8550918519496917
  batch 400 loss: 0.8203549325466156
  batch 450 loss: 0.7816911029815674
  batch 500 loss: 0.8136244702339173
  batch 550 loss: 0.8266055577993393
  batch 600 loss: 0.8487656021118164
  batch 650 loss: 0.8523719227313995
  batch 700 loss: 0.8481664693355561
  batch 750 loss: 0.8097440934181214
  batch 800 loss: 0.8005564403533936
  batch 850 loss: 0.8399054563045502
  batch 900 loss: 0.8061065542697906
LOSS train 0.80611 valid 0.95800, valid PER 29.09%
EPOCH 20:
  batch 50 loss: 0.7638511967658996
  batch 100 loss: 0.8089546370506286
  batch 150 loss: 0.8123082786798477
  batch 200 loss: 0.8044219255447388
  batch 250 loss: 0.82362464427948
  batch 300 loss: 0.8069567632675171
  batch 350 loss: 0.778767819404602
  batch 400 loss: 0.8039807283878326
  batch 450 loss: 0.792597234249115
  batch 500 loss: 0.8209840798377991
  batch 550 loss: 0.8174680507183075
  batch 600 loss: 0.8006813472509384
  batch 650 loss: 0.8223376870155334
  batch 700 loss: 0.8144309759140015
  batch 750 loss: 0.7981917858123779
  batch 800 loss: 0.8208313298225403
  batch 850 loss: 0.8172207570075989
  batch 900 loss: 0.812155796289444
LOSS train 0.81216 valid 0.95031, valid PER 29.31%
[1.8209420943260193, 1.4990295815467833, 1.3632660162448884, 1.2489031755924225, 1.1836947631835937, 1.1639622712135316, 1.0743201327323915, 1.0492604851722718, 0.9976801991462707, 0.9916737866401673, 0.9887635278701782, 0.948886479139328, 0.9263403785228729, 0.9279448652267456, 0.8394909918308258, 0.8636330342292786, 0.8685994148254395, 0.8411242020130157, 0.8061065542697906, 0.812155796289444]
[1.7731386423110962, 1.4661442041397095, 1.2985299825668335, 1.2157264947891235, 1.1569069623947144, 1.11077880859375, 1.1034574508666992, 1.048331379890442, 1.0454566478729248, 1.0220191478729248, 1.0137286186218262, 0.9923808574676514, 0.9887606501579285, 0.9668511748313904, 0.9660757780075073, 0.9673675894737244, 0.9638490676879883, 0.9583552479743958, 0.9579954147338867, 0.9503098130226135]
Training finished in 12.0 minutes.
Model saved to checkpoints/20230120_165906/model_20
Loading model from checkpoints/20230120_165906/model_20
SUB: 16.22%, DEL: 12.37%, INS: 1.82%, COR: 71.40%, PER: 30.42%
