Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.001, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.2)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 8.94036358833313
  batch 100 loss: 3.2292428159713746
  batch 150 loss: 3.1084399747848512
  batch 200 loss: 3.0170188188552856
  batch 250 loss: 2.94368462562561
  batch 300 loss: 2.8348824882507326
  batch 350 loss: 2.7180383014678955
  batch 400 loss: 2.674186234474182
  batch 450 loss: 2.547965145111084
  batch 500 loss: 2.403468008041382
  batch 550 loss: 2.312431950569153
  batch 600 loss: 2.2502561521530153
  batch 650 loss: 2.1681693506240847
  batch 700 loss: 2.149668900966644
  batch 750 loss: 2.0750864005088805
  batch 800 loss: 2.0570141744613646
  batch 850 loss: 2.0036154341697694
  batch 900 loss: 1.9601091623306275
LOSS train 1.96011 valid 1.92120, valid PER 73.39%
EPOCH 2:
  batch 50 loss: 1.9356595754623414
  batch 100 loss: 1.8813992238044739
  batch 150 loss: 1.7962958741188049
  batch 200 loss: 1.8042327165603638
  batch 250 loss: 1.8173812484741212
  batch 300 loss: 1.7662326002120972
  batch 350 loss: 1.7725769639015199
  batch 400 loss: 1.7086622190475464
  batch 450 loss: 1.7265245628356933
  batch 500 loss: 1.709988341331482
  batch 550 loss: 1.6909928441047668
  batch 600 loss: 1.6623892641067506
  batch 650 loss: 1.6358300065994262
  batch 700 loss: 1.630921893119812
  batch 750 loss: 1.6171632051467895
  batch 800 loss: 1.57312185049057
  batch 850 loss: 1.5912927031517028
  batch 900 loss: 1.5578616619110108
LOSS train 1.55786 valid 1.54161, valid PER 56.87%
EPOCH 3:
  batch 50 loss: 1.4945017027854919
  batch 100 loss: 1.5614003348350525
  batch 150 loss: 1.5584218215942383
  batch 200 loss: 1.476289131641388
  batch 250 loss: 1.4944345927238465
  batch 300 loss: 1.4855255270004273
  batch 350 loss: 1.5043824338912963
  batch 400 loss: 1.47474440574646
  batch 450 loss: 1.4512456393241882
  batch 500 loss: 1.4153506922721864
  batch 550 loss: 1.4213533115386963
  batch 600 loss: 1.3775433611869812
  batch 650 loss: 1.3943607902526856
  batch 700 loss: 1.3788044548034668
  batch 750 loss: 1.4113267827033997
  batch 800 loss: 1.4150407266616822
  batch 850 loss: 1.3805792260169982
  batch 900 loss: 1.3732429432868958
LOSS train 1.37324 valid 1.33800, valid PER 45.69%
EPOCH 4:
  batch 50 loss: 1.363992578983307
  batch 100 loss: 1.3075210046768189
  batch 150 loss: 1.3259507131576538
  batch 200 loss: 1.324859004020691
  batch 250 loss: 1.3370841097831727
  batch 300 loss: 1.3331607222557067
  batch 350 loss: 1.3208114552497863
  batch 400 loss: 1.2571439886093139
  batch 450 loss: 1.2934488463401794
  batch 500 loss: 1.329873173236847
  batch 550 loss: 1.2634637975692748
  batch 600 loss: 1.2464627122879028
  batch 650 loss: 1.3084331023693085
  batch 700 loss: 1.3307077312469482
  batch 750 loss: 1.292795079946518
  batch 800 loss: 1.2486838364601136
  batch 850 loss: 1.2459114265441895
  batch 900 loss: 1.2591069650650024
LOSS train 1.25911 valid 1.24529, valid PER 40.10%
EPOCH 5:
  batch 50 loss: 1.2488894021511079
  batch 100 loss: 1.2255916285514832
  batch 150 loss: 1.2362428820133209
  batch 200 loss: 1.2613364553451538
  batch 250 loss: 1.2100222837924957
  batch 300 loss: 1.232229927778244
  batch 350 loss: 1.1846156120300293
  batch 400 loss: 1.1680085122585298
  batch 450 loss: 1.180230166912079
  batch 500 loss: 1.1596935629844665
  batch 550 loss: 1.2068343019485475
  batch 600 loss: 1.2162160217761993
  batch 650 loss: 1.2137732911109924
  batch 700 loss: 1.2042779433727264
  batch 750 loss: 1.1634344959259033
  batch 800 loss: 1.2218139910697936
  batch 850 loss: 1.269301700592041
  batch 900 loss: 1.1848745381832122
LOSS train 1.18487 valid 1.17292, valid PER 37.79%
EPOCH 6:
  batch 50 loss: 1.1552641916275024
  batch 100 loss: 1.178990033864975
  batch 150 loss: 1.1638691318035126
  batch 200 loss: 1.1285692000389098
  batch 250 loss: 1.158841848373413
  batch 300 loss: 1.1852987062931062
  batch 350 loss: 1.168226888179779
  batch 400 loss: 1.1327371001243591
  batch 450 loss: 1.155005975961685
  batch 500 loss: 1.126542308330536
  batch 550 loss: 1.1582751882076263
  batch 600 loss: 1.1456457710266112
  batch 650 loss: 1.1233314609527587
  batch 700 loss: 1.1040238118171692
  batch 750 loss: 1.1254026532173156
  batch 800 loss: 1.1064098739624024
  batch 850 loss: 1.1485032761096954
  batch 900 loss: 1.1566960334777832
LOSS train 1.15670 valid 1.11513, valid PER 36.07%
EPOCH 7:
  batch 50 loss: 1.0886922299861908
  batch 100 loss: 1.1483798027038574
  batch 150 loss: 1.0714307081699372
  batch 200 loss: 1.088579626083374
  batch 250 loss: 1.1445843982696533
  batch 300 loss: 1.1022195684909821
  batch 350 loss: 1.1146150577068328
  batch 400 loss: 1.0866038131713867
  batch 450 loss: 1.0864781415462494
  batch 500 loss: 1.0899289953708649
  batch 550 loss: 1.0639616668224334
  batch 600 loss: 1.0942741918563843
  batch 650 loss: 1.059103273153305
  batch 700 loss: 1.1114314639568328
  batch 750 loss: 1.0849441051483155
  batch 800 loss: 1.0741750586032868
  batch 850 loss: 1.068367463350296
  batch 900 loss: 1.090405580997467
LOSS train 1.09041 valid 1.08666, valid PER 36.08%
EPOCH 8:
  batch 50 loss: 1.0712880933284759
  batch 100 loss: 1.0260537469387054
  batch 150 loss: 1.0854359686374664
  batch 200 loss: 1.0651608633995056
  batch 250 loss: 1.0408567798137665
  batch 300 loss: 1.0275638699531555
  batch 350 loss: 1.0434188628196717
  batch 400 loss: 1.0519129097461701
  batch 450 loss: 1.093835060596466
  batch 500 loss: 1.0617709958553314
  batch 550 loss: 1.0703837728500367
  batch 600 loss: 1.0137761044502258
  batch 650 loss: 1.020080772638321
  batch 700 loss: 1.0796033012866975
  batch 750 loss: 1.079201319217682
  batch 800 loss: 1.0909332895278931
  batch 850 loss: 1.0412408435344696
  batch 900 loss: 1.026208736896515
LOSS train 1.02621 valid 1.07200, valid PER 34.32%
EPOCH 9:
  batch 50 loss: 1.0080167186260223
  batch 100 loss: 0.9995342588424683
  batch 150 loss: 1.0149175775051118
  batch 200 loss: 0.9977114415168762
  batch 250 loss: 0.9978740763664246
  batch 300 loss: 1.0319189894199372
  batch 350 loss: 1.0066501200199127
  batch 400 loss: 1.0323226761817932
  batch 450 loss: 1.0678137636184692
  batch 500 loss: 1.0367638874053955
  batch 550 loss: 1.0208445858955384
  batch 600 loss: 1.057967598438263
  batch 650 loss: 1.0326737654209137
  batch 700 loss: 0.9991936957836152
  batch 750 loss: 0.9996789157390594
  batch 800 loss: 1.0392966961860657
  batch 850 loss: 1.0341630697250366
  batch 900 loss: 0.9943668282032013
LOSS train 0.99437 valid 1.03081, valid PER 33.18%
EPOCH 10:
  batch 50 loss: 0.9779189276695252
  batch 100 loss: 0.9948834419250489
  batch 150 loss: 1.0276278400421142
  batch 200 loss: 0.9464178860187531
  batch 250 loss: 0.9779447042942047
  batch 300 loss: 0.9837059688568115
  batch 350 loss: 0.9872978961467743
  batch 400 loss: 0.9560424375534058
  batch 450 loss: 0.9677632057666778
  batch 500 loss: 0.9789651596546173
  batch 550 loss: 0.9966793870925903
  batch 600 loss: 0.9733990740776062
  batch 650 loss: 0.9929571580886841
  batch 700 loss: 1.010097326040268
  batch 750 loss: 1.0205654430389404
  batch 800 loss: 1.0064052414894105
  batch 850 loss: 0.9938487601280213
  batch 900 loss: 0.9883946228027344
LOSS train 0.98839 valid 1.02130, valid PER 32.48%
EPOCH 11:
  batch 50 loss: 0.9718833613395691
  batch 100 loss: 0.9438921904563904
  batch 150 loss: 0.9241326498985291
  batch 200 loss: 0.9289811909198761
  batch 250 loss: 0.932600257396698
  batch 300 loss: 0.9592917478084564
  batch 350 loss: 0.9953526604175568
  batch 400 loss: 0.9300355339050292
  batch 450 loss: 0.9458892440795899
  batch 500 loss: 0.9344159734249115
  batch 550 loss: 0.9635883355140686
  batch 600 loss: 0.9637987303733826
  batch 650 loss: 0.9836734926700592
  batch 700 loss: 1.0317050302028656
  batch 750 loss: 0.9784456384181976
  batch 800 loss: 0.9753703701496125
  batch 850 loss: 0.9530326926708221
  batch 900 loss: 0.9978908002376556
LOSS train 0.99789 valid 1.00417, valid PER 31.77%
EPOCH 12:
  batch 50 loss: 0.8984377694129944
  batch 100 loss: 0.8985367465019226
  batch 150 loss: 0.9368717682361603
  batch 200 loss: 0.944515745639801
  batch 250 loss: 0.923931314945221
  batch 300 loss: 0.9523912227153778
  batch 350 loss: 0.9348383736610413
  batch 400 loss: 0.9547745835781097
  batch 450 loss: 0.9404546594619752
  batch 500 loss: 0.9462430548667907
  batch 550 loss: 0.9509620547294617
  batch 600 loss: 0.9456830894947053
  batch 650 loss: 0.9384553396701812
  batch 700 loss: 0.9380210638046265
  batch 750 loss: 0.9627261459827423
  batch 800 loss: 0.8985739970207214
  batch 850 loss: 0.9296399676799774
  batch 900 loss: 0.9488857674598694
LOSS train 0.94889 valid 0.98753, valid PER 31.56%
EPOCH 13:
  batch 50 loss: 0.8917798006534576
  batch 100 loss: 0.9022341871261597
  batch 150 loss: 0.9086832511425018
  batch 200 loss: 0.896618218421936
  batch 250 loss: 0.8959589397907257
  batch 300 loss: 0.9371926176548004
  batch 350 loss: 0.905019165277481
  batch 400 loss: 0.9197247898578644
  batch 450 loss: 0.9227193677425385
  batch 500 loss: 0.8928812336921692
  batch 550 loss: 0.9529926192760467
  batch 600 loss: 0.9035012865066528
  batch 650 loss: 0.9011386692523956
  batch 700 loss: 0.9458753561973572
  batch 750 loss: 0.87894979596138
  batch 800 loss: 0.9142103469371796
  batch 850 loss: 0.8941579365730286
  batch 900 loss: 0.8929552924633026
LOSS train 0.89296 valid 0.97543, valid PER 31.26%
EPOCH 14:
  batch 50 loss: 0.8788142120838165
  batch 100 loss: 0.8447149097919464
  batch 150 loss: 0.878576397895813
  batch 200 loss: 0.8859598994255066
  batch 250 loss: 0.8585115933418274
  batch 300 loss: 0.8653800070285798
  batch 350 loss: 0.8949661302566528
  batch 400 loss: 0.9117363429069519
  batch 450 loss: 0.8782768821716309
  batch 500 loss: 0.8928153324127197
  batch 550 loss: 0.8974027347564697
  batch 600 loss: 0.8708329319953918
  batch 650 loss: 0.9016864442825318
  batch 700 loss: 0.9024813163280487
  batch 750 loss: 0.897652257680893
  batch 800 loss: 0.8995897650718689
  batch 850 loss: 0.9421385550498962
  batch 900 loss: 0.8830987930297851
LOSS train 0.88310 valid 0.97123, valid PER 30.86%
EPOCH 15:
  batch 50 loss: 0.8361180651187897
  batch 100 loss: 0.8634617757797242
  batch 150 loss: 0.867189302444458
  batch 200 loss: 0.8755072104930878
  batch 250 loss: 0.8854617190361023
  batch 300 loss: 0.887277067899704
  batch 350 loss: 0.8532549178600312
  batch 400 loss: 0.8730551505088806
  batch 450 loss: 0.8779174065589905
  batch 500 loss: 0.8663936293125153
  batch 550 loss: 0.913965003490448
  batch 600 loss: 0.9226115536689758
  batch 650 loss: 0.8632385790348053
  batch 700 loss: 0.8753662502765656
  batch 750 loss: 0.8877572119235992
  batch 800 loss: 0.8643947350978851
  batch 850 loss: 0.8830011570453644
  batch 900 loss: 0.8275795441865921
LOSS train 0.82758 valid 0.94909, valid PER 30.75%
EPOCH 16:
  batch 50 loss: 0.8323960292339325
  batch 100 loss: 0.8334425556659698
  batch 150 loss: 0.8415740203857421
  batch 200 loss: 0.866193813085556
  batch 250 loss: 0.849319224357605
  batch 300 loss: 0.861777446269989
  batch 350 loss: 0.860906172990799
  batch 400 loss: 0.8532083654403686
  batch 450 loss: 0.870882099866867
  batch 500 loss: 0.8701791250705719
  batch 550 loss: 0.8602792429924011
  batch 600 loss: 0.8691193079948425
  batch 650 loss: 0.8631600093841553
  batch 700 loss: 0.8441743731498719
  batch 750 loss: 0.8625767636299133
  batch 800 loss: 0.8477734649181365
  batch 850 loss: 0.8372952306270599
  batch 900 loss: 0.8483050382137298
LOSS train 0.84831 valid 0.94007, valid PER 30.08%
EPOCH 17:
  batch 50 loss: 0.8389851081371308
  batch 100 loss: 0.791990361213684
  batch 150 loss: 0.8424155700206757
  batch 200 loss: 0.8013586395978928
  batch 250 loss: 0.8414207911491394
  batch 300 loss: 0.841751184463501
  batch 350 loss: 0.8386476695537567
  batch 400 loss: 0.8600951790809631
  batch 450 loss: 0.8061029171943664
  batch 500 loss: 0.8574991917610169
  batch 550 loss: 0.8348749828338623
  batch 600 loss: 0.8544261038303376
  batch 650 loss: 0.8265870213508606
  batch 700 loss: 0.856025573015213
  batch 750 loss: 0.8198663341999054
  batch 800 loss: 0.848857135772705
  batch 850 loss: 0.8463452231884002
  batch 900 loss: 0.8410013914108276
LOSS train 0.84100 valid 0.93527, valid PER 29.54%
EPOCH 18:
  batch 50 loss: 0.7883026480674744
  batch 100 loss: 0.8029803311824799
  batch 150 loss: 0.8349360632896423
  batch 200 loss: 0.8191381180286408
  batch 250 loss: 0.797145447731018
  batch 300 loss: 0.8165467047691345
  batch 350 loss: 0.8007163643836975
  batch 400 loss: 0.8016752600669861
  batch 450 loss: 0.8308593714237213
  batch 500 loss: 0.8185424959659576
  batch 550 loss: 0.843906751871109
  batch 600 loss: 0.8176704847812652
  batch 650 loss: 0.8117400193214417
  batch 700 loss: 0.7923651564121247
  batch 750 loss: 0.8456506550312042
  batch 800 loss: 0.840785528421402
  batch 850 loss: 0.8286081409454346
  batch 900 loss: 0.8245202308893204
LOSS train 0.82452 valid 0.94879, valid PER 30.29%
EPOCH 19:
  batch 50 loss: 0.7992099845409393
  batch 100 loss: 0.7964083796739578
  batch 150 loss: 0.7690907275676727
  batch 200 loss: 0.7731328248977661
  batch 250 loss: 0.8109414064884186
  batch 300 loss: 0.8159766685962677
  batch 350 loss: 0.8389432996511459
  batch 400 loss: 0.8107170790433884
  batch 450 loss: 0.7689606237411499
  batch 500 loss: 0.7554491877555847
  batch 550 loss: 0.8070988547801972
  batch 600 loss: 0.8300529652833939
  batch 650 loss: 0.8171122932434082
  batch 700 loss: 0.8479663419723511
  batch 750 loss: 0.7853765261173248
  batch 800 loss: 0.7742394363880157
  batch 850 loss: 0.797581479549408
  batch 900 loss: 0.7987284874916076
LOSS train 0.79873 valid 0.92778, valid PER 29.47%
EPOCH 20:
  batch 50 loss: 0.7181212830543519
  batch 100 loss: 0.7651614415645599
  batch 150 loss: 0.7844236093759537
  batch 200 loss: 0.8062322008609771
  batch 250 loss: 0.803128981590271
  batch 300 loss: 0.7905708253383636
  batch 350 loss: 0.7601208031177521
  batch 400 loss: 0.7828615659475326
  batch 450 loss: 0.7777383041381836
  batch 500 loss: 0.8124741864204407
  batch 550 loss: 0.8006588250398636
  batch 600 loss: 0.7580264902114868
  batch 650 loss: 0.8173413360118866
  batch 700 loss: 0.793493596315384
  batch 750 loss: 0.7825468111038208
  batch 800 loss: 0.8087519586086274
  batch 850 loss: 0.8002758407592774
  batch 900 loss: 0.7589953505992889
LOSS train 0.75900 valid 0.94137, valid PER 28.88%
Training finished in 19.0 minutes.
Model saved to checkpoints/20230118_150410/model_19
Loading model from checkpoints/20230118_150410/model_19
SUB: 15.99%, DEL: 13.54%, INS: 1.50%, COR: 70.47%, PER: 31.03%
