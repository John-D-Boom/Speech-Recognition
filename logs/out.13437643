Namespace(seed=123, train_json='train_fbank_speeds.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=4, fbank_dims=23, model_dims=256, concat=1, lr=0.0005, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.4, beta1=0.9, beta2=0.999, exp=0.5)
Total number of model parameters is 5326888
EPOCH 1:
  batch 50 loss: 6.136831407546997
  batch 100 loss: 3.317833867073059
  batch 150 loss: 3.2973208475112914
  batch 200 loss: 3.2733860445022582
  batch 250 loss: 3.245746364593506
  batch 300 loss: 3.1786001253128053
  batch 350 loss: 2.999795980453491
  batch 400 loss: 2.8230222845077515
  batch 450 loss: 2.7223856449127197
  batch 500 loss: 2.543720955848694
  batch 550 loss: 2.4134967041015627
  batch 600 loss: 2.2971203565597533
  batch 650 loss: 2.1657545018196105
  batch 700 loss: 2.123494839668274
  batch 750 loss: 2.0025330209732055
  batch 800 loss: 1.94963481426239
  batch 850 loss: 1.8554816198349
  batch 900 loss: 1.7620410203933716
LOSS train 1.76204 valid 1.61628, valid PER 58.16%
loss: 0.00025
EPOCH 2:
  batch 50 loss: 1.6951972341537476
  batch 100 loss: 1.6285536003112793
  batch 150 loss: 1.5487369966506959
  batch 200 loss: 1.5463370513916015
  batch 250 loss: 1.4894182634353639
  batch 300 loss: 1.4312416839599609
  batch 350 loss: 1.4354320764541626
  batch 400 loss: 1.4080999207496643
  batch 450 loss: 1.3653004598617553
  batch 500 loss: 1.3635997581481933
  batch 550 loss: 1.3040494120121002
  batch 600 loss: 1.284692873954773
  batch 650 loss: 1.233114916086197
  batch 700 loss: 1.2507856810092925
  batch 750 loss: 1.2368804156780242
  batch 800 loss: 1.1906904995441436
  batch 850 loss: 1.199228389263153
  batch 900 loss: 1.1639738869667053
LOSS train 1.16397 valid 1.08970, valid PER 33.88%
loss: 0.000125
EPOCH 3:
  batch 50 loss: 1.0987839436531066
  batch 100 loss: 1.1883074522018433
  batch 150 loss: 1.1737500977516175
  batch 200 loss: 1.0879277992248535
  batch 250 loss: 1.1016976141929626
  batch 300 loss: 1.1414300727844238
  batch 350 loss: 1.1096934080123901
  batch 400 loss: 1.1021150243282318
  batch 450 loss: 1.0401331448554993
  batch 500 loss: 1.0499440252780914
  batch 550 loss: 1.0492473149299621
  batch 600 loss: 0.9913309705257416
  batch 650 loss: 1.047977662086487
  batch 700 loss: 1.0279938387870788
  batch 750 loss: 1.034126627445221
  batch 800 loss: 1.0625204944610596
  batch 850 loss: 1.0131362140178681
  batch 900 loss: 1.0099339604377746
LOSS train 1.00993 valid 0.94294, valid PER 29.75%
loss: 6.25e-05
EPOCH 4:
  batch 50 loss: 0.9897308135032654
  batch 100 loss: 0.9348666501045227
  batch 150 loss: 0.9538600957393646
  batch 200 loss: 0.9201085162162781
  batch 250 loss: 0.9483048510551453
  batch 300 loss: 0.9427730560302734
  batch 350 loss: 0.9339687716960907
  batch 400 loss: 0.8828067207336425
  batch 450 loss: 0.9238896584510803
  batch 500 loss: 0.9730935943126678
  batch 550 loss: 0.8858398139476776
  batch 600 loss: 0.8854010868072509
  batch 650 loss: 0.9235748302936554
  batch 700 loss: 0.8868589913845062
  batch 750 loss: 0.8858878982067108
  batch 800 loss: 0.87253662109375
  batch 850 loss: 0.9009574556350708
  batch 900 loss: 0.902492618560791
LOSS train 0.90249 valid 0.87237, valid PER 27.07%
loss: 3.125e-05
EPOCH 5:
  batch 50 loss: 0.8446213257312775
  batch 100 loss: 0.81172123670578
  batch 150 loss: 0.8650384157896042
  batch 200 loss: 0.8797750163078308
  batch 250 loss: 0.8277709782123566
  batch 300 loss: 0.8594064497947693
  batch 350 loss: 0.8093717592954636
  batch 400 loss: 0.7913799691200256
  batch 450 loss: 0.8110484433174133
  batch 500 loss: 0.7804642999172211
  batch 550 loss: 0.828741067647934
  batch 600 loss: 0.840878347158432
  batch 650 loss: 0.8362180674076081
  batch 700 loss: 0.8087793427705765
  batch 750 loss: 0.7943904709815979
  batch 800 loss: 0.8479898798465729
  batch 850 loss: 0.8243244957923889
  batch 900 loss: 0.7983102130889893
LOSS train 0.79831 valid 0.79639, valid PER 25.26%
loss: 1.5625e-05
EPOCH 6:
  batch 50 loss: 0.762743912935257
  batch 100 loss: 0.7652036702632904
  batch 150 loss: 0.772270621061325
  batch 200 loss: 0.7480654013156891
  batch 250 loss: 0.7357451701164246
  batch 300 loss: 0.7620142579078675
  batch 350 loss: 0.7582173442840576
  batch 400 loss: 0.7439565515518188
  batch 450 loss: 0.8119424867630005
  batch 500 loss: 0.7520631211996078
  batch 550 loss: 0.7618524920940399
  batch 600 loss: 0.7633436077833176
  batch 650 loss: 0.7004789471626282
  batch 700 loss: 0.74219466984272
  batch 750 loss: 0.7621473038196563
  batch 800 loss: 0.7485461837053299
  batch 850 loss: 0.7677314364910126
  batch 900 loss: 0.7889730608463288
LOSS train 0.78897 valid 0.75383, valid PER 23.68%
loss: 7.8125e-06
EPOCH 7:
  batch 50 loss: 0.7145169878005981
  batch 100 loss: 0.7375453221797943
  batch 150 loss: 0.6661005532741546
  batch 200 loss: 0.6896000623703002
  batch 250 loss: 0.7353949499130249
  batch 300 loss: 0.7097046643495559
  batch 350 loss: 0.7518401461839676
  batch 400 loss: 0.7070014202594757
  batch 450 loss: 0.6905757015943528
  batch 500 loss: 0.7019917690753936
  batch 550 loss: 0.7013919079303741
  batch 600 loss: 0.6837655633687973
  batch 650 loss: 0.7198133045434951
  batch 700 loss: 0.7061713016033173
  batch 750 loss: 0.6864292669296265
  batch 800 loss: 0.7018668413162231
  batch 850 loss: 0.7293213528394699
  batch 900 loss: 0.7342841917276383
LOSS train 0.73428 valid 0.72779, valid PER 23.15%
loss: 3.90625e-06
EPOCH 8:
  batch 50 loss: 0.6624791038036346
  batch 100 loss: 0.6256388968229294
  batch 150 loss: 0.6699293142557144
  batch 200 loss: 0.6661779505014419
  batch 250 loss: 0.6693307042121888
  batch 300 loss: 0.6603083395957947
  batch 350 loss: 0.6578443139791489
  batch 400 loss: 0.6380708348751069
  batch 450 loss: 0.6709581351280213
  batch 500 loss: 0.6664533811807632
  batch 550 loss: 0.6783680433034897
  batch 600 loss: 0.6423544430732727
  batch 650 loss: 0.6878166234493256
  batch 700 loss: 0.6713559716939926
  batch 750 loss: 0.6872334867715836
  batch 800 loss: 0.7013823503255844
  batch 850 loss: 0.6691245174407959
  batch 900 loss: 0.6730321580171585
LOSS train 0.67303 valid 0.70851, valid PER 22.30%
loss: 1.953125e-06
EPOCH 9:
  batch 50 loss: 0.6136515700817108
  batch 100 loss: 0.5857326221466065
  batch 150 loss: 0.6283166736364365
  batch 200 loss: 0.6111776739358902
  batch 250 loss: 0.6036338019371033
  batch 300 loss: 0.638786211013794
  batch 350 loss: 0.6036551505327225
  batch 400 loss: 0.6450019037723541
  batch 450 loss: 0.6745750892162323
  batch 500 loss: 0.6329903364181518
  batch 550 loss: 0.6343871402740479
  batch 600 loss: 0.6388814079761506
  batch 650 loss: 0.6397406184673309
  batch 700 loss: 0.637675684094429
  batch 750 loss: 0.6516782319545746
  batch 800 loss: 0.6661422652006149
  batch 850 loss: 0.6380637902021408
  batch 900 loss: 0.6014383393526077
LOSS train 0.60144 valid 0.67183, valid PER 21.46%
loss: 9.765625e-07
EPOCH 10:
  batch 50 loss: 0.5635749852657318
  batch 100 loss: 0.5641495245695114
  batch 150 loss: 0.5952910423278809
  batch 200 loss: 0.560536133646965
  batch 250 loss: 0.5702682566642762
  batch 300 loss: 0.5817131453752518
  batch 350 loss: 0.5889427691698075
  batch 400 loss: 0.5753982663154602
  batch 450 loss: 0.5690275025367737
  batch 500 loss: 0.5705849707126618
  batch 550 loss: 0.603459290266037
  batch 600 loss: 0.5718826580047608
  batch 650 loss: 0.6024171799421311
  batch 700 loss: 0.5850442862510681
  batch 750 loss: 0.6004729890823364
  batch 800 loss: 0.5796245330572128
  batch 850 loss: 0.5847332710027695
  batch 900 loss: 0.6179167711734772
LOSS train 0.61792 valid 0.68799, valid PER 21.10%
EPOCH 11:
  batch 50 loss: 0.5758942979574203
  batch 100 loss: 0.5608721077442169
  batch 150 loss: 0.5555403977632523
  batch 200 loss: 0.5184603834152222
  batch 250 loss: 0.5478489714860916
  batch 300 loss: 0.5498080718517303
  batch 350 loss: 0.6100808227062225
  batch 400 loss: 0.5886673724651337
  batch 450 loss: 0.5602733790874481
  batch 500 loss: 0.5600300139188766
  batch 550 loss: 0.570503900051117
  batch 600 loss: 0.556114856004715
  batch 650 loss: 0.5769917798042298
  batch 700 loss: 0.6396405106782913
  batch 750 loss: 0.5515279203653336
  batch 800 loss: 0.5709404289722443
  batch 850 loss: 0.5628396093845367
  batch 900 loss: 0.5731322282552719
LOSS train 0.57313 valid 0.70213, valid PER 21.94%
EPOCH 12:
  batch 50 loss: 0.5345551723241806
  batch 100 loss: 0.5142988097667694
  batch 150 loss: 0.5181175845861435
  batch 200 loss: 0.531323971748352
  batch 250 loss: 0.5404401290416717
  batch 300 loss: 0.5326401269435883
  batch 350 loss: 0.5169765758514404
  batch 400 loss: 0.5507568728923797
  batch 450 loss: 0.51918277323246
  batch 500 loss: 0.5391142320632935
  batch 550 loss: 0.5324064138531684
  batch 600 loss: 0.5290999507904053
  batch 650 loss: 0.5485859286785125
  batch 700 loss: 0.5344583231210709
  batch 750 loss: 0.5313771402835846
  batch 800 loss: 0.5054577898979187
  batch 850 loss: 0.5654900509119034
  batch 900 loss: 0.5564805209636688
LOSS train 0.55648 valid 0.67414, valid PER 20.88%
EPOCH 13:
  batch 50 loss: 0.4984537959098816
  batch 100 loss: 0.47534272491931917
  batch 150 loss: 0.49496858537197114
  batch 200 loss: 0.4830278754234314
  batch 250 loss: 0.4836448895931244
  batch 300 loss: 0.5137906408309937
  batch 350 loss: 0.49086658626794816
  batch 400 loss: 0.4923007532954216
  batch 450 loss: 0.47924333095550536
  batch 500 loss: 0.47920612871646884
  batch 550 loss: 0.5314700275659561
  batch 600 loss: 0.5287911653518677
  batch 650 loss: 0.5077971214056015
  batch 700 loss: 0.5106388825178146
  batch 750 loss: 0.5009228426218033
  batch 800 loss: 0.501721186041832
  batch 850 loss: 0.510534428358078
  batch 900 loss: 0.5130569171905518
LOSS train 0.51306 valid 0.65837, valid PER 20.52%
loss: 4.8828125e-07
EPOCH 14:
  batch 50 loss: 0.4612912306189537
  batch 100 loss: 0.45151977479457855
  batch 150 loss: 0.46212006628513336
  batch 200 loss: 0.478590287566185
  batch 250 loss: 0.4514161163568497
  batch 300 loss: 0.4691823470592499
  batch 350 loss: 0.4698686790466309
  batch 400 loss: 0.503378056883812
  batch 450 loss: 0.4805441910028458
  batch 500 loss: 0.4767746090888977
  batch 550 loss: 0.47711883783340453
  batch 600 loss: 0.48304046034812925
  batch 650 loss: 0.4978043830394745
  batch 700 loss: 0.502836155295372
  batch 750 loss: 0.48871125519275666
  batch 800 loss: 0.4771454495191574
  batch 850 loss: 0.5050488695502281
  batch 900 loss: 0.5010735321044922
LOSS train 0.50107 valid 0.67208, valid PER 20.78%
EPOCH 15:
  batch 50 loss: 0.4231997174024582
  batch 100 loss: 0.42928547114133836
  batch 150 loss: 0.4492064923048019
  batch 200 loss: 0.44170901596546175
  batch 250 loss: 0.4501015889644623
  batch 300 loss: 0.46318271666765215
  batch 350 loss: 0.4788121473789215
  batch 400 loss: 0.45527506589889527
  batch 450 loss: 0.45342362493276595
  batch 500 loss: 0.4644938117265701
  batch 550 loss: 0.5359645736217499
  batch 600 loss: 0.5174151641130448
  batch 650 loss: 0.5030902189016342
  batch 700 loss: 0.4654457783699036
  batch 750 loss: 0.49425017774105073
  batch 800 loss: 0.45605107605457307
  batch 850 loss: 0.4715356937050819
  batch 900 loss: 0.44711330354213713
LOSS train 0.44711 valid 0.66133, valid PER 20.40%
EPOCH 16:
  batch 50 loss: 0.41558221518993377
  batch 100 loss: 0.4028654691576958
  batch 150 loss: 0.3995656341314316
  batch 200 loss: 0.4314181888103485
  batch 250 loss: 0.41807895243167875
  batch 300 loss: 0.43007383912801744
  batch 350 loss: 0.4567177405953407
  batch 400 loss: 0.4336663568019867
  batch 450 loss: 0.4409010010957718
  batch 500 loss: 0.42367030680179596
  batch 550 loss: 0.4330723598599434
  batch 600 loss: 0.47293472826480865
  batch 650 loss: 0.4787793129682541
  batch 700 loss: 0.4093063470721245
  batch 750 loss: 0.4443884333968163
  batch 800 loss: 0.4373765802383423
  batch 850 loss: 0.44703738391399384
  batch 900 loss: 0.45846677243709566
LOSS train 0.45847 valid 0.67119, valid PER 20.36%
EPOCH 17:
  batch 50 loss: 0.42192647576332093
  batch 100 loss: 0.3722111967206001
  batch 150 loss: 0.4357946127653122
  batch 200 loss: 0.3903870204091072
  batch 250 loss: 0.3920227813720703
  batch 300 loss: 0.377667518556118
  batch 350 loss: 0.4379592999815941
  batch 400 loss: 0.4283078354597092
  batch 450 loss: 0.4329864078760147
  batch 500 loss: 0.4390724539756775
  batch 550 loss: 0.42625030875205994
  batch 600 loss: 0.43482968062162397
  batch 650 loss: 0.40346779257059096
  batch 700 loss: 0.42162501096725463
  batch 750 loss: 0.4034187111258507
  batch 800 loss: 0.4299119561910629
  batch 850 loss: 0.4389658883213997
  batch 900 loss: 0.44043543756008147
LOSS train 0.44044 valid 0.65375, valid PER 19.58%
loss: 2.44140625e-07
EPOCH 18:
  batch 50 loss: 0.36815894544124605
  batch 100 loss: 0.38174069702625274
  batch 150 loss: 0.4282862865924835
  batch 200 loss: 0.3741517463326454
  batch 250 loss: 0.3771694564819336
  batch 300 loss: 0.35035191297531126
  batch 350 loss: 0.36067477643489837
  batch 400 loss: 0.3646502023935318
  batch 450 loss: 0.38856005877256394
  batch 500 loss: 0.38282925009727475
  batch 550 loss: 0.4150486525893211
  batch 600 loss: 0.40548564970493317
  batch 650 loss: 0.3948844957351685
  batch 700 loss: 0.41250432044267654
  batch 750 loss: 0.4024763369560242
  batch 800 loss: 0.433929854631424
  batch 850 loss: 0.4291160348057747
  batch 900 loss: 0.4208112379908562
LOSS train 0.42081 valid 0.67867, valid PER 20.60%
EPOCH 19:
  batch 50 loss: 0.39514455616474153
  batch 100 loss: 0.38756261885166166
  batch 150 loss: 0.3548823365569115
  batch 200 loss: 0.3492626109719276
  batch 250 loss: 0.3843453398346901
  batch 300 loss: 0.4250717127323151
  batch 350 loss: 0.4243172690272331
  batch 400 loss: 0.38779988467693327
  batch 450 loss: 0.38102982580661776
  batch 500 loss: 0.3793501177430153
  batch 550 loss: 0.43414970695972444
  batch 600 loss: 0.4174251502752304
  batch 650 loss: 0.4196181744337082
  batch 700 loss: 0.4404471921920776
  batch 750 loss: 0.38462865382432937
  batch 800 loss: 0.39565480351448057
  batch 850 loss: 0.411616296172142
  batch 900 loss: 0.4010914427042007
LOSS train 0.40109 valid 0.67059, valid PER 19.92%
EPOCH 20:
  batch 50 loss: 0.3236245784163475
  batch 100 loss: 0.35541677683591844
  batch 150 loss: 0.3385623556375503
  batch 200 loss: 0.3549236261844635
  batch 250 loss: 0.36034737169742587
  batch 300 loss: 0.36379065543413164
  batch 350 loss: 0.3378214299678802
  batch 400 loss: 0.37344445824623107
  batch 450 loss: 0.34929317504167556
  batch 500 loss: 0.36799798369407655
  batch 550 loss: 0.3819338893890381
  batch 600 loss: 0.36881583094596865
  batch 650 loss: 0.3784668916463852
  batch 700 loss: 0.37079754620790484
  batch 750 loss: 0.3773832416534424
  batch 800 loss: 0.38895573288202284
  batch 850 loss: 0.3973898401856422
  batch 900 loss: 0.3921183693408966
LOSS train 0.39212 valid 0.69238, valid PER 20.20%
[1.7620410203933716, 1.1639738869667053, 1.0099339604377746, 0.902492618560791, 0.7983102130889893, 0.7889730608463288, 0.7342841917276383, 0.6730321580171585, 0.6014383393526077, 0.6179167711734772, 0.5731322282552719, 0.5564805209636688, 0.5130569171905518, 0.5010735321044922, 0.44711330354213713, 0.45846677243709566, 0.44043543756008147, 0.4208112379908562, 0.4010914427042007, 0.3921183693408966]
[1.6162753105163574, 1.0897022485733032, 0.9429356455802917, 0.8723733425140381, 0.7963857054710388, 0.7538292407989502, 0.7277864813804626, 0.7085098028182983, 0.6718276739120483, 0.6879907250404358, 0.7021266222000122, 0.6741430759429932, 0.6583717465400696, 0.6720770597457886, 0.6613304615020752, 0.6711919903755188, 0.6537493467330933, 0.6786668300628662, 0.6705859303474426, 0.6923781633377075]
Training finished in 24.0 minutes.
Model saved to checkpoints/20230126_130501/model_17
Loading model from checkpoints/20230126_130501/model_17
SUB: 14.10%, DEL: 5.30%, INS: 2.50%, COR: 80.61%, PER: 21.90%
