Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.5)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.2462759208679195
  batch 100 loss: 3.228894681930542
  batch 150 loss: 3.035110058784485
  batch 200 loss: 2.821351881027222
  batch 250 loss: 2.6313627624511717
  batch 300 loss: 2.4743662548065184
  batch 350 loss: 2.343163084983826
  batch 400 loss: 2.324620807170868
  batch 450 loss: 2.256548914909363
  batch 500 loss: 2.1554725122451783
  batch 550 loss: 2.1203330087661745
  batch 600 loss: 2.0776079940795897
  batch 650 loss: 2.000720450878143
  batch 700 loss: 2.014235780239105
  batch 750 loss: 1.9577589583396913
  batch 800 loss: 1.9398619723320008
  batch 850 loss: 1.8868126845359803
  batch 900 loss: 1.8812244987487794
LOSS train 1.88122 valid 1.79281, valid PER 67.73%
EPOCH 2:
  batch 50 loss: 1.8759671473503112
  batch 100 loss: 1.856884813308716
  batch 150 loss: 1.7426202034950256
  batch 200 loss: 1.7708038258552552
  batch 250 loss: 1.7588931894302369
  batch 300 loss: 1.7329224705696107
  batch 350 loss: 1.7441849136352539
  batch 400 loss: 1.700473518371582
  batch 450 loss: 1.7012441158294678
  batch 500 loss: 1.7081327962875366
  batch 550 loss: 1.671814329624176
  batch 600 loss: 1.690225396156311
  batch 650 loss: 1.6301359343528747
  batch 700 loss: 1.649452178478241
  batch 750 loss: 1.619963059425354
  batch 800 loss: 1.571413881778717
  batch 850 loss: 1.6025230860710145
  batch 900 loss: 1.5849466133117676
LOSS train 1.58495 valid 1.53112, valid PER 57.17%
EPOCH 3:
  batch 50 loss: 1.5180756092071532
  batch 100 loss: 1.601507487297058
  batch 150 loss: 1.575569713115692
  batch 200 loss: 1.5187274742126464
  batch 250 loss: 1.5612693309783936
  batch 300 loss: 1.5488011956214904
  batch 350 loss: 1.5602447628974914
  batch 400 loss: 1.5351010107994079
  batch 450 loss: 1.520790572166443
  batch 500 loss: 1.4876483511924743
  batch 550 loss: 1.495942814350128
  batch 600 loss: 1.4483012795448302
  batch 650 loss: 1.4822160983085633
  batch 700 loss: 1.4818690490722657
  batch 750 loss: 1.510243639945984
  batch 800 loss: 1.4952970743179321
  batch 850 loss: 1.4721712279319763
  batch 900 loss: 1.4677930855751038
LOSS train 1.46779 valid 1.36019, valid PER 51.07%
EPOCH 4:
  batch 50 loss: 1.4811182808876038
  batch 100 loss: 1.3875886821746826
  batch 150 loss: 1.4319694900512696
  batch 200 loss: 1.4257541823387145
  batch 250 loss: 1.438615801334381
  batch 300 loss: 1.4197298049926759
  batch 350 loss: 1.4162577724456786
  batch 400 loss: 1.3706753754615784
  batch 450 loss: 1.3917728853225708
  batch 500 loss: 1.4503489446640014
  batch 550 loss: 1.3779686856269837
  batch 600 loss: 1.3672362089157104
  batch 650 loss: 1.4425678753852844
  batch 700 loss: 1.4460688877105712
  batch 750 loss: 1.3840996384620667
  batch 800 loss: 1.373841314315796
  batch 850 loss: 1.3513342547416687
  batch 900 loss: 1.3812381219863892
LOSS train 1.38124 valid 1.29419, valid PER 46.81%
EPOCH 5:
  batch 50 loss: 1.3624904608726502
  batch 100 loss: 1.3358454847335814
  batch 150 loss: 1.3635190343856811
  batch 200 loss: 1.369789593219757
  batch 250 loss: 1.334237003326416
  batch 300 loss: 1.361965081691742
  batch 350 loss: 1.3125040471553802
  batch 400 loss: 1.2977109622955323
  batch 450 loss: 1.3053812885284424
  batch 500 loss: 1.2964174449443817
  batch 550 loss: 1.340275945663452
  batch 600 loss: 1.3299656891822815
  batch 650 loss: 1.3700161898136138
  batch 700 loss: 1.324838501214981
  batch 750 loss: 1.316911506652832
  batch 800 loss: 1.3666940927505493
  batch 850 loss: 1.3333704686164856
  batch 900 loss: 1.2929822087287903
LOSS train 1.29298 valid 1.24054, valid PER 44.53%
EPOCH 6:
  batch 50 loss: 1.2576082825660706
  batch 100 loss: 1.299536554813385
  batch 150 loss: 1.2939471089839936
  batch 200 loss: 1.2388804101943969
  batch 250 loss: 1.3019189453125
  batch 300 loss: 1.296128591299057
  batch 350 loss: 1.3144064521789551
  batch 400 loss: 1.2970567679405212
  batch 450 loss: 1.2991749846935272
  batch 500 loss: 1.2569775152206422
  batch 550 loss: 1.281094172000885
  batch 600 loss: 1.2579610514640809
  batch 650 loss: 1.2554950046539306
  batch 700 loss: 1.2460318660736085
  batch 750 loss: 1.286120400428772
  batch 800 loss: 1.2638523292541504
  batch 850 loss: 1.2964072966575622
  batch 900 loss: 1.29133544921875
LOSS train 1.29134 valid 1.20121, valid PER 42.73%
EPOCH 7:
  batch 50 loss: 1.2343239510059356
  batch 100 loss: 1.2977453756332398
  batch 150 loss: 1.2155540335178374
  batch 200 loss: 1.2239031791687012
  batch 250 loss: 1.2682646334171295
  batch 300 loss: 1.2427697968482971
  batch 350 loss: 1.2553343820571898
  batch 400 loss: 1.222817724943161
  batch 450 loss: 1.2354023945331574
  batch 500 loss: 1.220630520582199
  batch 550 loss: 1.2226991713047028
  batch 600 loss: 1.232809692621231
  batch 650 loss: 1.1781410336494447
  batch 700 loss: 1.2440524911880493
  batch 750 loss: 1.2227987718582154
  batch 800 loss: 1.215927324295044
  batch 850 loss: 1.20226593375206
  batch 900 loss: 1.216683931350708
LOSS train 1.21668 valid 1.19842, valid PER 41.39%
EPOCH 8:
  batch 50 loss: 1.2123892819881439
  batch 100 loss: 1.1824666225910188
  batch 150 loss: 1.220645282268524
  batch 200 loss: 1.1924987256526947
  batch 250 loss: 1.1750349974632264
  batch 300 loss: 1.1875346159934999
  batch 350 loss: 1.1846617138385773
  batch 400 loss: 1.1767897284030915
  batch 450 loss: 1.2440452075004578
  batch 500 loss: 1.2130404686927796
  batch 550 loss: 1.1804063856601714
  batch 600 loss: 1.1628688991069793
  batch 650 loss: 1.1821943628787994
  batch 700 loss: 1.2213390183448791
  batch 750 loss: 1.1945829689502716
  batch 800 loss: 1.2158114647865295
  batch 850 loss: 1.1723682034015654
  batch 900 loss: 1.1850096917152404
LOSS train 1.18501 valid 1.14689, valid PER 38.74%
EPOCH 9:
  batch 50 loss: 1.1552092635631561
  batch 100 loss: 1.1361459815502166
  batch 150 loss: 1.1712658250331878
  batch 200 loss: 1.1213620901107788
  batch 250 loss: 1.1105175387859345
  batch 300 loss: 1.1717800855636598
  batch 350 loss: 1.1384277403354646
  batch 400 loss: 1.1885084438323974
  batch 450 loss: 1.1772445797920228
  batch 500 loss: 1.157629157304764
  batch 550 loss: 1.1505492305755616
  batch 600 loss: 1.1708881974220275
  batch 650 loss: 1.1648952865600586
  batch 700 loss: 1.152840768098831
  batch 750 loss: 1.1602469658851624
  batch 800 loss: 1.1997205090522767
  batch 850 loss: 1.16504021525383
  batch 900 loss: 1.1435753178596497
LOSS train 1.14358 valid 1.09585, valid PER 36.28%
EPOCH 10:
  batch 50 loss: 1.1285727834701538
  batch 100 loss: 1.1439636313915253
  batch 150 loss: 1.1791129505634308
  batch 200 loss: 1.1218298745155335
  batch 250 loss: 1.120109976530075
  batch 300 loss: 1.121605852842331
  batch 350 loss: 1.1176965749263763
  batch 400 loss: 1.0789835500717162
  batch 450 loss: 1.1055029451847076
  batch 500 loss: 1.118753252029419
  batch 550 loss: 1.1175124728679657
  batch 600 loss: 1.0851133251190186
  batch 650 loss: 1.1434872031211853
  batch 700 loss: 1.1470782852172852
  batch 750 loss: 1.1562715780735016
  batch 800 loss: 1.1489980983734132
  batch 850 loss: 1.1255679774284362
  batch 900 loss: 1.1123056995868683
LOSS train 1.11231 valid 1.09608, valid PER 36.50%
EPOCH 11:
  batch 50 loss: 1.0881097555160522
  batch 100 loss: 1.0854941964149476
  batch 150 loss: 1.071120262145996
  batch 200 loss: 1.0551225411891938
  batch 250 loss: 1.0581550097465515
  batch 300 loss: 1.0831721341609954
  batch 350 loss: 1.1215596771240235
  batch 400 loss: 1.0870225024223328
  batch 450 loss: 1.085540828704834
  batch 500 loss: 1.0773228979110718
  batch 550 loss: 1.1185679233074188
  batch 600 loss: 1.1059997880458832
  batch 650 loss: 1.1151318204402925
  batch 700 loss: 1.1910257196426393
  batch 750 loss: 1.1059814941883088
  batch 800 loss: 1.131265981197357
  batch 850 loss: 1.1170638239383697
  batch 900 loss: 1.12821950674057
LOSS train 1.12822 valid 1.05443, valid PER 34.81%
EPOCH 12:
  batch 50 loss: 1.0530480551719665
  batch 100 loss: 1.0288038194179534
  batch 150 loss: 1.0800284588336944
  batch 200 loss: 1.1036613154411317
  batch 250 loss: 1.0700022745132447
  batch 300 loss: 1.1079794597625732
  batch 350 loss: 1.0852981579303742
  batch 400 loss: 1.067776062488556
  batch 450 loss: 1.060229194164276
  batch 500 loss: 1.094385542869568
  batch 550 loss: 1.0784358310699462
  batch 600 loss: 1.094218316078186
  batch 650 loss: 1.0869631457328797
  batch 700 loss: 1.06400750041008
  batch 750 loss: 1.0891685199737549
  batch 800 loss: 1.0464196872711182
  batch 850 loss: 1.0809212255477905
  batch 900 loss: 1.08011070728302
LOSS train 1.08011 valid 1.05569, valid PER 33.42%
EPOCH 13:
  batch 50 loss: 1.0507264482975005
  batch 100 loss: 1.0336913228034974
  batch 150 loss: 1.07268616437912
  batch 200 loss: 1.023693231344223
  batch 250 loss: 1.0653323531150818
  batch 300 loss: 1.1051099061965943
  batch 350 loss: 1.0269143331050872
  batch 400 loss: 1.0407342112064362
  batch 450 loss: 1.0884704852104188
  batch 500 loss: 1.050404634475708
  batch 550 loss: 1.093026760816574
  batch 600 loss: 1.0778971588611603
  batch 650 loss: 1.0609478187561034
  batch 700 loss: 1.085241255760193
  batch 750 loss: 1.0249576771259308
  batch 800 loss: 1.0579735779762267
  batch 850 loss: 1.0452438831329345
  batch 900 loss: 1.040938321352005
LOSS train 1.04094 valid 1.04186, valid PER 33.80%
EPOCH 14:
  batch 50 loss: 1.0161846661567688
  batch 100 loss: 0.9958871853351593
  batch 150 loss: 1.04240079164505
  batch 200 loss: 1.0279901146888732
  batch 250 loss: 1.039395443201065
  batch 300 loss: 1.0364938378334045
  batch 350 loss: 1.0218329894542695
  batch 400 loss: 1.0603708267211913
  batch 450 loss: 1.0241736233234406
  batch 500 loss: 1.0181084299087524
  batch 550 loss: 1.0504534327983857
  batch 600 loss: 1.047390739917755
  batch 650 loss: 1.0499263739585876
  batch 700 loss: 1.063918731212616
  batch 750 loss: 1.0240661025047302
  batch 800 loss: 1.0503187704086303
  batch 850 loss: 1.1002920734882355
  batch 900 loss: 1.0445162999629973
LOSS train 1.04452 valid 1.00977, valid PER 33.06%
EPOCH 15:
  batch 50 loss: 0.991921489238739
  batch 100 loss: 1.0084883654117585
  batch 150 loss: 1.0118925070762634
  batch 200 loss: 1.033313317298889
  batch 250 loss: 1.035891788005829
  batch 300 loss: 1.0397168576717377
  batch 350 loss: 1.0039007914066316
  batch 400 loss: 1.0305202329158782
  batch 450 loss: 1.0150940036773681
  batch 500 loss: 1.017695119380951
  batch 550 loss: 1.064586284160614
  batch 600 loss: 1.066780732870102
  batch 650 loss: 1.0275681221485138
  batch 700 loss: 1.0160937690734864
  batch 750 loss: 1.066210720539093
  batch 800 loss: 1.0239574790000916
  batch 850 loss: 1.0190284514427186
  batch 900 loss: 0.9984890282154083
LOSS train 0.99849 valid 1.03432, valid PER 34.62%
EPOCH 16:
  batch 50 loss: 0.9922016561031342
  batch 100 loss: 0.9563191354274749
  batch 150 loss: 1.026135971546173
  batch 200 loss: 1.021874623298645
  batch 250 loss: 1.010875986814499
  batch 300 loss: 1.0298702907562256
  batch 350 loss: 1.017664166688919
  batch 400 loss: 0.9929388844966889
  batch 450 loss: 1.035963464975357
  batch 500 loss: 1.0182202816009522
  batch 550 loss: 0.9850338721275329
  batch 600 loss: 1.038416143655777
  batch 650 loss: 1.0104652428627015
  batch 700 loss: 0.9888366734981537
  batch 750 loss: 1.0387588608264924
  batch 800 loss: 1.0063970911502838
  batch 850 loss: 0.993946670293808
  batch 900 loss: 1.0303275549411774
LOSS train 1.03033 valid 0.99137, valid PER 31.98%
EPOCH 17:
  batch 50 loss: 0.9856530058383942
  batch 100 loss: 0.9311167359352112
  batch 150 loss: 1.02072558760643
  batch 200 loss: 0.9741204833984375
  batch 250 loss: 0.9983959436416626
  batch 300 loss: 0.987214229106903
  batch 350 loss: 0.9816618812084198
  batch 400 loss: 1.0112566030025483
  batch 450 loss: 0.9948266816139221
  batch 500 loss: 1.011729507446289
  batch 550 loss: 0.9803489959239959
  batch 600 loss: 1.0218944501876832
  batch 650 loss: 0.9908412921428681
  batch 700 loss: 0.9973834395408631
  batch 750 loss: 0.9863124299049377
  batch 800 loss: 1.0054966306686401
  batch 850 loss: 1.005002692937851
  batch 900 loss: 1.0064474999904633
LOSS train 1.00645 valid 0.99830, valid PER 31.26%
EPOCH 18:
  batch 50 loss: 0.9619942474365234
  batch 100 loss: 0.9304007434844971
  batch 150 loss: 1.0005297207832335
  batch 200 loss: 0.9886816883087158
  batch 250 loss: 0.9614559006690979
  batch 300 loss: 0.9688313770294189
  batch 350 loss: 0.9522888767719269
  batch 400 loss: 1.0022741973400116
  batch 450 loss: 1.0086294102668762
  batch 500 loss: 0.9838994228839875
  batch 550 loss: 1.026158103942871
  batch 600 loss: 0.9893431377410888
  batch 650 loss: 0.9606948077678681
  batch 700 loss: 0.9532411217689514
  batch 750 loss: 0.9734814167022705
  batch 800 loss: 1.0034384298324586
  batch 850 loss: 1.0025308430194855
  batch 900 loss: 0.9949857521057129
LOSS train 0.99499 valid 0.99605, valid PER 31.94%
EPOCH 19:
  batch 50 loss: 0.9439253413677215
  batch 100 loss: 0.9620067775249481
  batch 150 loss: 0.9324427676200867
  batch 200 loss: 0.9451157426834107
  batch 250 loss: 0.9897770285606384
  batch 300 loss: 0.9833092033863068
  batch 350 loss: 0.9790923857688903
  batch 400 loss: 0.9647896146774292
  batch 450 loss: 0.9149114000797272
  batch 500 loss: 0.9569939601421357
  batch 550 loss: 0.9951100766658783
  batch 600 loss: 0.9943026876449585
  batch 650 loss: 1.0136001408100128
  batch 700 loss: 1.022100270986557
  batch 750 loss: 0.9538040018081665
  batch 800 loss: 0.9341121733188629
  batch 850 loss: 0.9575829648971558
  batch 900 loss: 0.953097904920578
LOSS train 0.95310 valid 0.99661, valid PER 31.60%
EPOCH 20:
  batch 50 loss: 0.9257328915596008
  batch 100 loss: 0.953571230173111
  batch 150 loss: 0.9433442211151123
  batch 200 loss: 0.9326342356204986
  batch 250 loss: 0.9724552190303802
  batch 300 loss: 0.9385215461254119
  batch 350 loss: 0.9377909290790558
  batch 400 loss: 0.9615027379989624
  batch 450 loss: 0.950083419084549
  batch 500 loss: 0.9644348275661468
  batch 550 loss: 0.9539133489131928
  batch 600 loss: 0.9381047713756562
  batch 650 loss: 0.9868417346477508
  batch 700 loss: 0.9484793448448181
  batch 750 loss: 0.9684464132785797
  batch 800 loss: 0.9608834159374237
  batch 850 loss: 0.9816882538795472
  batch 900 loss: 0.9423161721229554
LOSS train 0.94232 valid 0.98495, valid PER 31.58%
Training finished in 3.0 minutes.
Model saved to checkpoints/20230115_205518/model_20
Loading model from checkpoints/20230115_205518/model_20
SUB: 14.82%, DEL: 16.99%, INS: 1.61%, COR: 68.19%, PER: 33.42%
