Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.2)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.1287819242477415
  batch 100 loss: 3.172965703010559
  batch 150 loss: 3.048309688568115
  batch 200 loss: 2.9240819787979127
  batch 250 loss: 2.8398331928253175
  batch 300 loss: 2.672716202735901
  batch 350 loss: 2.516397924423218
  batch 400 loss: 2.46067512512207
  batch 450 loss: 2.4126949596405027
  batch 500 loss: 2.303472332954407
  batch 550 loss: 2.227875714302063
  batch 600 loss: 2.157772834300995
  batch 650 loss: 2.029434440135956
  batch 700 loss: 2.0260983419418337
  batch 750 loss: 1.9551405787467957
  batch 800 loss: 1.933369836807251
  batch 850 loss: 1.8659442830085755
  batch 900 loss: 1.8444699716567994
LOSS train 1.84447 valid 1.78176, valid PER 68.10%
EPOCH 2:
  batch 50 loss: 1.8230678510665894
  batch 100 loss: 1.8039949607849122
  batch 150 loss: 1.6714273071289063
  batch 200 loss: 1.7048052620887757
  batch 250 loss: 1.6903136205673217
  batch 300 loss: 1.6637492346763612
  batch 350 loss: 1.6589567399024963
  batch 400 loss: 1.6159891819953918
  batch 450 loss: 1.6282274794578553
  batch 500 loss: 1.581517825126648
  batch 550 loss: 1.5734228110313415
  batch 600 loss: 1.574326024055481
  batch 650 loss: 1.5240546989440917
  batch 700 loss: 1.5302821969985962
  batch 750 loss: 1.514489803314209
  batch 800 loss: 1.4638810420036317
  batch 850 loss: 1.493466351032257
  batch 900 loss: 1.4449878334999084
LOSS train 1.44499 valid 1.39735, valid PER 47.17%
EPOCH 3:
  batch 50 loss: 1.3911118507385254
  batch 100 loss: 1.4311961436271667
  batch 150 loss: 1.4545590019226073
  batch 200 loss: 1.350821270942688
  batch 250 loss: 1.3952706480026245
  batch 300 loss: 1.4038857460021972
  batch 350 loss: 1.4223377180099488
  batch 400 loss: 1.3581903207302093
  batch 450 loss: 1.3447664189338684
  batch 500 loss: 1.3450843036174773
  batch 550 loss: 1.3565788650512696
  batch 600 loss: 1.2854852962493897
  batch 650 loss: 1.3032605135440827
  batch 700 loss: 1.308395791053772
  batch 750 loss: 1.3241820788383485
  batch 800 loss: 1.3254372715950011
  batch 850 loss: 1.295589188337326
  batch 900 loss: 1.293718626499176
LOSS train 1.29372 valid 1.22897, valid PER 39.03%
EPOCH 4:
  batch 50 loss: 1.2865802669525146
  batch 100 loss: 1.244547165632248
  batch 150 loss: 1.2933761358261109
  batch 200 loss: 1.2448699116706847
  batch 250 loss: 1.2619177508354187
  batch 300 loss: 1.2393839538097382
  batch 350 loss: 1.239956135749817
  batch 400 loss: 1.2026121950149535
  batch 450 loss: 1.2319154024124146
  batch 500 loss: 1.276597490310669
  batch 550 loss: 1.2137642991542816
  batch 600 loss: 1.1792734694480895
  batch 650 loss: 1.264865369796753
  batch 700 loss: 1.2714541947841644
  batch 750 loss: 1.193544430732727
  batch 800 loss: 1.1964926314353943
  batch 850 loss: 1.1957711935043336
  batch 900 loss: 1.1888392269611359
LOSS train 1.18884 valid 1.14727, valid PER 36.86%
EPOCH 5:
  batch 50 loss: 1.1751978135108947
  batch 100 loss: 1.1525174486637115
  batch 150 loss: 1.1742385983467103
  batch 200 loss: 1.188800617456436
  batch 250 loss: 1.1591753947734833
  batch 300 loss: 1.175014444589615
  batch 350 loss: 1.136915694475174
  batch 400 loss: 1.1347853696346284
  batch 450 loss: 1.1180082964897156
  batch 500 loss: 1.1111115312576294
  batch 550 loss: 1.1597508406639099
  batch 600 loss: 1.1864021170139312
  batch 650 loss: 1.1590637242794037
  batch 700 loss: 1.1568472588062286
  batch 750 loss: 1.1556626486778259
  batch 800 loss: 1.1761369967460633
  batch 850 loss: 1.1798265945911408
  batch 900 loss: 1.1603375792503356
LOSS train 1.16034 valid 1.13985, valid PER 36.44%
EPOCH 6:
  batch 50 loss: 1.1615357172489167
  batch 100 loss: 1.225605195760727
  batch 150 loss: 1.1437688744068146
  batch 200 loss: 1.0970925116539
  batch 250 loss: 1.1239758288860322
  batch 300 loss: 1.1450518798828124
  batch 350 loss: 1.150820835828781
  batch 400 loss: 1.1264980220794678
  batch 450 loss: 1.1451182782649993
  batch 500 loss: 1.095545597076416
  batch 550 loss: 1.1167873990535737
  batch 600 loss: 1.1045678520202638
  batch 650 loss: 1.0835353326797486
  batch 700 loss: 1.074719659090042
  batch 750 loss: 1.1377953863143921
  batch 800 loss: 1.1307983684539795
  batch 850 loss: 1.1291286385059356
  batch 900 loss: 1.1219925260543824
LOSS train 1.12199 valid 1.07434, valid PER 35.28%
EPOCH 7:
  batch 50 loss: 1.054778698682785
  batch 100 loss: 1.1202777326107025
  batch 150 loss: 1.0503016459941863
  batch 200 loss: 1.0660550808906555
  batch 250 loss: 1.1104902279376985
  batch 300 loss: 1.0743762469291687
  batch 350 loss: 1.1137703144550324
  batch 400 loss: 1.1131756603717804
  batch 450 loss: 1.0760411858558654
  batch 500 loss: 1.0714774763584136
  batch 550 loss: 1.0572862899303437
  batch 600 loss: 1.0613042998313904
  batch 650 loss: 1.038972189426422
  batch 700 loss: 1.1160547721385956
  batch 750 loss: 1.067037912607193
  batch 800 loss: 1.060415894985199
  batch 850 loss: 1.0610817861557007
  batch 900 loss: 1.064095332622528
LOSS train 1.06410 valid 1.06806, valid PER 35.15%
EPOCH 8:
  batch 50 loss: 1.0532980060577393
  batch 100 loss: 1.0115660285949708
  batch 150 loss: 1.0668454563617706
  batch 200 loss: 1.0665056371688844
  batch 250 loss: 1.0400239872932433
  batch 300 loss: 1.030274531841278
  batch 350 loss: 1.0604917895793915
  batch 400 loss: 1.0278595030307769
  batch 450 loss: 1.1102296674251557
  batch 500 loss: 1.0671792387962342
  batch 550 loss: 1.0482769525051117
  batch 600 loss: 0.9995169687271118
  batch 650 loss: 1.017182972431183
  batch 700 loss: 1.0611332833766938
  batch 750 loss: 1.0609244668483735
  batch 800 loss: 1.074666337966919
  batch 850 loss: 1.0310167503356933
  batch 900 loss: 1.0484920954704284
LOSS train 1.04849 valid 1.07362, valid PER 33.78%
EPOCH 9:
  batch 50 loss: 1.0161888670921326
  batch 100 loss: 0.9780572736263276
  batch 150 loss: 0.999154554605484
  batch 200 loss: 0.9816189348697663
  batch 250 loss: 1.018945722579956
  batch 300 loss: 1.02915123462677
  batch 350 loss: 0.989581526517868
  batch 400 loss: 1.0189705681800842
  batch 450 loss: 1.0415169024467468
  batch 500 loss: 0.9941303384304047
  batch 550 loss: 1.0182001960277558
  batch 600 loss: 1.0543680906295776
  batch 650 loss: 1.041454930305481
  batch 700 loss: 1.0092765498161316
  batch 750 loss: 1.0336435723304749
  batch 800 loss: 1.0439677262306213
  batch 850 loss: 1.0443951630592345
  batch 900 loss: 1.0188718378543853
LOSS train 1.01887 valid 1.06538, valid PER 33.99%
EPOCH 10:
  batch 50 loss: 1.0177742481231689
  batch 100 loss: 1.0915096426010131
  batch 150 loss: 1.0637685644626618
  batch 200 loss: 0.9904770493507385
  batch 250 loss: 0.9862261629104614
  batch 300 loss: 0.9745439982414246
  batch 350 loss: 0.9838422000408172
  batch 400 loss: 0.9601819336414337
  batch 450 loss: 0.9752114367485046
  batch 500 loss: 0.9904427599906921
  batch 550 loss: 0.9914934706687927
  batch 600 loss: 0.9684278750419617
  batch 650 loss: 1.0094402050971985
  batch 700 loss: 1.0210145974159242
  batch 750 loss: 0.9926499879360199
  batch 800 loss: 1.0085289347171784
  batch 850 loss: 0.9950985515117645
  batch 900 loss: 0.966066300868988
LOSS train 0.96607 valid 1.01518, valid PER 33.19%
EPOCH 11:
  batch 50 loss: 0.9420451772212982
  batch 100 loss: 0.9356740868091583
  batch 150 loss: 0.9385125517845154
  batch 200 loss: 0.9229956364631653
  batch 250 loss: 0.9403397011756897
  batch 300 loss: 0.9590168213844299
  batch 350 loss: 0.9943047785758972
  batch 400 loss: 0.9607366144657135
  batch 450 loss: 0.9397617185115814
  batch 500 loss: 0.9609259438514709
  batch 550 loss: 0.9775585949420929
  batch 600 loss: 0.9655579221248627
  batch 650 loss: 0.9688212156295777
  batch 700 loss: 1.0309348571300507
  batch 750 loss: 0.9460580050945282
  batch 800 loss: 0.9720840585231781
  batch 850 loss: 0.9600561308860779
  batch 900 loss: 0.9722328424453736
LOSS train 0.97223 valid 1.00064, valid PER 31.82%
EPOCH 12:
  batch 50 loss: 0.9381718754768371
  batch 100 loss: 0.906586867570877
  batch 150 loss: 0.9206962728500366
  batch 200 loss: 0.9727290999889374
  batch 250 loss: 0.9346119558811188
  batch 300 loss: 0.98599360704422
  batch 350 loss: 0.9536844336986542
  batch 400 loss: 0.9858044517040253
  batch 450 loss: 0.9499849808216095
  batch 500 loss: 0.9476450145244598
  batch 550 loss: 0.982344571352005
  batch 600 loss: 0.9711571407318115
  batch 650 loss: 0.9708829414844513
  batch 700 loss: 0.9473422181606292
  batch 750 loss: 0.9705638325214386
  batch 800 loss: 0.9045428156852722
  batch 850 loss: 0.9573959136009216
  batch 900 loss: 0.9655408918857574
LOSS train 0.96554 valid 0.99069, valid PER 31.62%
EPOCH 13:
  batch 50 loss: 0.9075018441677094
  batch 100 loss: 0.9124065339565277
  batch 150 loss: 0.9314318287372589
  batch 200 loss: 0.903921571969986
  batch 250 loss: 0.8982834780216217
  batch 300 loss: 0.9574152147769928
  batch 350 loss: 0.9052852892875671
  batch 400 loss: 0.9282288920879364
  batch 450 loss: 0.9596846163272857
  batch 500 loss: 0.9093247735500336
  batch 550 loss: 1.039974534511566
  batch 600 loss: 0.9621393322944641
  batch 650 loss: 0.9383277487754822
  batch 700 loss: 0.9889547514915467
  batch 750 loss: 0.9511618340015411
  batch 800 loss: 0.9578857696056366
  batch 850 loss: 0.941282387971878
  batch 900 loss: 0.9298172438144684
LOSS train 0.92982 valid 0.97394, valid PER 30.78%
EPOCH 14:
  batch 50 loss: 0.9009866428375244
  batch 100 loss: 0.9158499252796173
  batch 150 loss: 0.9151613855361939
  batch 200 loss: 0.9131622684001922
  batch 250 loss: 0.8881021285057068
  batch 300 loss: 0.8711354970932007
  batch 350 loss: 0.8882693135738373
  batch 400 loss: 0.9156976819038392
  batch 450 loss: 0.8715196007490158
  batch 500 loss: 0.908762719631195
  batch 550 loss: 0.9501518440246582
  batch 600 loss: 0.9385302758216858
  batch 650 loss: 0.9399625039100648
  batch 700 loss: 0.9478851437568665
  batch 750 loss: 0.9174229061603546
  batch 800 loss: 0.9385264575481415
  batch 850 loss: 0.9807233917713165
  batch 900 loss: 0.9266335558891297
LOSS train 0.92663 valid 0.96760, valid PER 31.28%
EPOCH 15:
  batch 50 loss: 0.8718599951267243
  batch 100 loss: 0.8935225749015808
  batch 150 loss: 0.8632501447200776
  batch 200 loss: 0.9310464870929718
  batch 250 loss: 0.9019976341724396
  batch 300 loss: 0.9258739411830902
  batch 350 loss: 0.8757871794700622
  batch 400 loss: 0.9032599174976349
  batch 450 loss: 0.9147644293308258
  batch 500 loss: 0.9024184334278107
  batch 550 loss: 0.931015065908432
  batch 600 loss: 0.9282022571563721
  batch 650 loss: 0.8883044064044953
  batch 700 loss: 0.9111094295978546
  batch 750 loss: 0.9356821393966674
  batch 800 loss: 0.8895544850826264
  batch 850 loss: 0.9073905158042908
  batch 900 loss: 0.8733714985847473
LOSS train 0.87337 valid 0.97134, valid PER 31.53%
EPOCH 16:
  batch 50 loss: 0.878464263677597
  batch 100 loss: 0.8737870061397552
  batch 150 loss: 0.8920584881305694
  batch 200 loss: 0.9033343994617462
  batch 250 loss: 0.8844494879245758
  batch 300 loss: 0.8966965043544769
  batch 350 loss: 0.885083669424057
  batch 400 loss: 0.8669496273994446
  batch 450 loss: 0.9194465243816375
  batch 500 loss: 0.908196827173233
  batch 550 loss: 0.888204562664032
  batch 600 loss: 0.9031240892410278
  batch 650 loss: 0.905727481842041
  batch 700 loss: 0.8727570462226868
  batch 750 loss: 0.8878190398216248
  batch 800 loss: 0.8768095886707306
  batch 850 loss: 0.8596536886692047
  batch 900 loss: 0.8886269164085389
LOSS train 0.88863 valid 0.94153, valid PER 30.36%
EPOCH 17:
  batch 50 loss: 0.8550034046173096
  batch 100 loss: 0.8162388503551483
  batch 150 loss: 0.906189376115799
  batch 200 loss: 0.8591480410099029
  batch 250 loss: 0.8768449819087982
  batch 300 loss: 0.8670640349388122
  batch 350 loss: 0.8778931164741516
  batch 400 loss: 0.899311408996582
  batch 450 loss: 0.8671337676048279
  batch 500 loss: 0.8563373327255249
  batch 550 loss: 0.8861059367656707
  batch 600 loss: 0.9269021940231323
  batch 650 loss: 0.8947331392765046
  batch 700 loss: 0.9006287682056427
  batch 750 loss: 0.8632705354690552
  batch 800 loss: 0.9251288282871246
  batch 850 loss: 0.9009962892532348
  batch 900 loss: 0.8901458394527435
LOSS train 0.89015 valid 0.95632, valid PER 30.08%
EPOCH 18:
  batch 50 loss: 0.8678040492534638
  batch 100 loss: 0.8698505246639252
  batch 150 loss: 0.9077035903930664
  batch 200 loss: 0.8549407994747162
  batch 250 loss: 0.8525174844264984
  batch 300 loss: 0.8874010002613067
  batch 350 loss: 0.852954843044281
  batch 400 loss: 0.8771507549285888
  batch 450 loss: 0.9172931575775146
  batch 500 loss: 0.9181074273586273
  batch 550 loss: 0.9395112574100495
  batch 600 loss: 0.96596142411232
  batch 650 loss: 0.9230827736854553
  batch 700 loss: 0.8939579260349274
  batch 750 loss: 0.8929782617092132
  batch 800 loss: 0.9109089934825897
  batch 850 loss: 0.9278113448619842
  batch 900 loss: 0.916289085149765
LOSS train 0.91629 valid 0.96791, valid PER 30.78%
EPOCH 19:
  batch 50 loss: 0.8658321690559387
  batch 100 loss: 0.8768852245807648
  batch 150 loss: 0.8368592703342438
  batch 200 loss: 0.8324891245365142
  batch 250 loss: 0.8856131064891816
  batch 300 loss: 0.882739634513855
  batch 350 loss: 0.8983240866661072
  batch 400 loss: 0.8852003014087677
  batch 450 loss: 0.8618238043785095
  batch 500 loss: 0.8572095561027527
  batch 550 loss: 0.8852154684066772
  batch 600 loss: 0.8791096043586731
  batch 650 loss: 0.9023605334758759
  batch 700 loss: 0.9007136929035187
  batch 750 loss: 0.8550506508350373
  batch 800 loss: 0.8245273017883301
  batch 850 loss: 0.8698665368556976
  batch 900 loss: 0.8450512707233429
LOSS train 0.84505 valid 0.97646, valid PER 30.34%
EPOCH 20:
  batch 50 loss: 0.8150964248180389
  batch 100 loss: 0.8559689390659332
  batch 150 loss: 0.8260707545280457
  batch 200 loss: 0.8634769439697265
  batch 250 loss: 0.8877909326553345
  batch 300 loss: 0.8467219066619873
  batch 350 loss: 0.8312401604652405
  batch 400 loss: 0.8493857216835022
  batch 450 loss: 0.842528578042984
  batch 500 loss: 0.8681712281703949
  batch 550 loss: 0.8445633316040039
  batch 600 loss: 0.8380511116981506
  batch 650 loss: 0.8709972417354583
  batch 700 loss: 0.9025506222248078
  batch 750 loss: 0.8568830394744873
  batch 800 loss: 0.8896211862564087
  batch 850 loss: 0.8986229026317596
  batch 900 loss: 0.8694836628437043
LOSS train 0.86948 valid 0.98000, valid PER 31.34%
Training finished in 3.0 minutes.
Model saved to checkpoints/20230115_204918/model_16
Loading model from checkpoints/20230115_204918/model_16
SUB: 16.11%, DEL: 13.92%, INS: 1.85%, COR: 69.97%, PER: 31.88%
