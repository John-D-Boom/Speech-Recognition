Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.1)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.123910021781922
  batch 100 loss: 3.1782565927505493
  batch 150 loss: 3.1764973735809328
  batch 200 loss: 2.9601124334335327
  batch 250 loss: 2.8879915857315064
  batch 300 loss: 2.6861175107955932
  batch 350 loss: 2.5483874607086183
  batch 400 loss: 2.452112340927124
  batch 450 loss: 2.412589554786682
  batch 500 loss: 2.799217743873596
  batch 550 loss: 2.5291660594940186
  batch 600 loss: 2.356675307750702
  batch 650 loss: 2.227124361991882
  batch 700 loss: 2.3144022178649903
  batch 750 loss: 2.181877336502075
  batch 800 loss: 2.0913845777511595
  batch 850 loss: 2.0774860620498656
  batch 900 loss: 1.9477039933204652
LOSS train 1.94770 valid 1.91650, valid PER 69.76%
EPOCH 2:
  batch 50 loss: 1.951775505542755
  batch 100 loss: 1.9455707693099975
  batch 150 loss: 1.7877664256095886
  batch 200 loss: 1.8318895673751832
  batch 250 loss: 1.8323942589759827
  batch 300 loss: 1.7446829199790954
  batch 350 loss: 1.768158893585205
  batch 400 loss: 1.701842453479767
  batch 450 loss: 1.6870517802238465
  batch 500 loss: 1.789142496585846
  batch 550 loss: 1.7160374307632447
  batch 600 loss: 1.6912457251548767
  batch 650 loss: 1.6458812642097473
  batch 700 loss: 1.6478385639190674
  batch 750 loss: 1.614885756969452
  batch 800 loss: 1.5562800645828248
  batch 850 loss: 1.5776054334640504
  batch 900 loss: 1.5258545875549316
LOSS train 1.52585 valid 1.72664, valid PER 55.28%
EPOCH 3:
  batch 50 loss: 1.5622917532920837
  batch 100 loss: 1.5925135469436646
  batch 150 loss: 1.5571228003501891
  batch 200 loss: 1.5208353686332703
  batch 250 loss: 1.5318263602256774
  batch 300 loss: 1.519309253692627
  batch 350 loss: 1.4766084551811218
  batch 400 loss: 1.4490761160850525
  batch 450 loss: 1.4120733904838563
  batch 500 loss: 1.3845955657958984
  batch 550 loss: 1.4048425507545472
  batch 600 loss: 1.317165253162384
  batch 650 loss: 1.3465437054634095
  batch 700 loss: 1.347437129020691
  batch 750 loss: 1.3710959362983703
  batch 800 loss: 1.3640212190151215
  batch 850 loss: 1.3397738814353943
  batch 900 loss: 1.3539214265346526
LOSS train 1.35392 valid 1.27625, valid PER 40.77%
EPOCH 4:
  batch 50 loss: 1.3343430829048157
  batch 100 loss: 1.2914534747600555
  batch 150 loss: 1.2822615444660186
  batch 200 loss: 1.2676861667633057
  batch 250 loss: 1.2480182480812072
  batch 300 loss: 1.2654021906852722
  batch 350 loss: 1.2685096120834352
  batch 400 loss: 1.1920413899421691
  batch 450 loss: 1.232369990348816
  batch 500 loss: 1.2571275413036347
  batch 550 loss: 1.20569237947464
  batch 600 loss: 1.1850265049934388
  batch 650 loss: 1.247135922908783
  batch 700 loss: 1.2789148366451264
  batch 750 loss: 1.2301946556568146
  batch 800 loss: 1.1996021449565888
  batch 850 loss: 1.181950113773346
  batch 900 loss: 1.1947809743881226
LOSS train 1.19478 valid 1.18657, valid PER 36.61%
EPOCH 5:
  batch 50 loss: 1.1925512528419495
  batch 100 loss: 1.1650941479206085
  batch 150 loss: 1.1856819427013396
  batch 200 loss: 1.1983943247795106
  batch 250 loss: 1.1528406727313996
  batch 300 loss: 1.1863531696796417
  batch 350 loss: 1.1246941912174224
  batch 400 loss: 1.1307031273841859
  batch 450 loss: 1.1204541504383088
  batch 500 loss: 1.0948826730251313
  batch 550 loss: 1.1614020407199859
  batch 600 loss: 1.1788454723358155
  batch 650 loss: 1.1553539550304412
  batch 700 loss: 1.1397438621520997
  batch 750 loss: 1.1209416747093202
  batch 800 loss: 1.1708495604991913
  batch 850 loss: 1.1539033031463624
  batch 900 loss: 1.1141700506210328
LOSS train 1.11417 valid 1.11284, valid PER 35.80%
EPOCH 6:
  batch 50 loss: 1.0976679491996766
  batch 100 loss: 1.0995736336708068
  batch 150 loss: 1.1173627662658692
  batch 200 loss: 1.0590333378314971
  batch 250 loss: 1.1000350940227508
  batch 300 loss: 1.1216135656833648
  batch 350 loss: 1.1089017271995545
  batch 400 loss: 1.084996064901352
  batch 450 loss: 1.1174337768554687
  batch 500 loss: 1.0649349200725555
  batch 550 loss: 1.0967543470859527
  batch 600 loss: 1.1034993374347686
  batch 650 loss: 1.0724031662940978
  batch 700 loss: 1.0615348815917969
  batch 750 loss: 1.1111623740196228
  batch 800 loss: 1.106909955739975
  batch 850 loss: 1.1245140397548676
  batch 900 loss: 1.1277649354934693
LOSS train 1.12776 valid 1.09397, valid PER 35.59%
EPOCH 7:
  batch 50 loss: 1.0679220497608184
  batch 100 loss: 1.1114724481105804
  batch 150 loss: 1.034178900718689
  batch 200 loss: 1.040987173318863
  batch 250 loss: 1.0876095795631409
  batch 300 loss: 1.0470348906517029
  batch 350 loss: 1.0892990577220916
  batch 400 loss: 1.045836330652237
  batch 450 loss: 1.052057374715805
  batch 500 loss: 1.0545601487159728
  batch 550 loss: 1.0451830840110778
  batch 600 loss: 1.0576142466068268
  batch 650 loss: 1.0397134613990784
  batch 700 loss: 1.0743820178508758
  batch 750 loss: 1.0565520894527436
  batch 800 loss: 1.040718287229538
  batch 850 loss: 1.0417485415935517
  batch 900 loss: 1.0222059512138366
LOSS train 1.02221 valid 1.05693, valid PER 34.50%
EPOCH 8:
  batch 50 loss: 1.0085063004493713
  batch 100 loss: 0.9919178998470306
  batch 150 loss: 1.0184311246871949
  batch 200 loss: 1.0089520156383514
  batch 250 loss: 0.9981925547122955
  batch 300 loss: 0.9749630951881408
  batch 350 loss: 1.0084681749343871
  batch 400 loss: 1.0056613171100617
  batch 450 loss: 1.0814344716072082
  batch 500 loss: 1.0266911375522614
  batch 550 loss: 1.0102423191070558
  batch 600 loss: 0.9910610282421112
  batch 650 loss: 0.999638831615448
  batch 700 loss: 1.0567489981651306
  batch 750 loss: 1.0364528000354767
  batch 800 loss: 1.042163212299347
  batch 850 loss: 0.9928493142127991
  batch 900 loss: 1.014893491268158
LOSS train 1.01489 valid 1.03940, valid PER 32.70%
EPOCH 9:
  batch 50 loss: 0.965707973241806
  batch 100 loss: 0.9524721682071686
  batch 150 loss: 0.9754289734363556
  batch 200 loss: 0.9731553745269775
  batch 250 loss: 0.9579861748218537
  batch 300 loss: 1.0010032117366792
  batch 350 loss: 0.9395290267467499
  batch 400 loss: 1.0001392650604248
  batch 450 loss: 1.01640634059906
  batch 500 loss: 0.9764047980308532
  batch 550 loss: 0.9881298017501831
  batch 600 loss: 1.021651417016983
  batch 650 loss: 0.9874442946910859
  batch 700 loss: 0.9734923529624939
  batch 750 loss: 0.9763976192474365
  batch 800 loss: 0.993240784406662
  batch 850 loss: 0.9973586881160736
  batch 900 loss: 0.9967623674869537
LOSS train 0.99676 valid 1.02776, valid PER 31.86%
EPOCH 10:
  batch 50 loss: 0.933497132062912
  batch 100 loss: 0.9758428168296814
  batch 150 loss: 1.0391157591342925
  batch 200 loss: 0.941650527715683
  batch 250 loss: 0.92205233335495
  batch 300 loss: 0.9337039947509765
  batch 350 loss: 0.9446133947372437
  batch 400 loss: 0.9202746939659119
  batch 450 loss: 0.9407988870143891
  batch 500 loss: 0.9653923809528351
  batch 550 loss: 0.9535686755180359
  batch 600 loss: 0.9619968891143799
  batch 650 loss: 0.9813246846199035
  batch 700 loss: 0.9748428332805633
  batch 750 loss: 0.964219673871994
  batch 800 loss: 0.9664012134075165
  batch 850 loss: 0.9424432647228241
  batch 900 loss: 0.9411733400821686
LOSS train 0.94117 valid 1.01380, valid PER 32.02%
EPOCH 11:
  batch 50 loss: 0.9371468341350555
  batch 100 loss: 0.9301729357242584
  batch 150 loss: 0.915425705909729
  batch 200 loss: 0.888010915517807
  batch 250 loss: 0.9050120532512664
  batch 300 loss: 0.9006216549873352
  batch 350 loss: 0.9760934317111969
  batch 400 loss: 0.9185693442821503
  batch 450 loss: 0.9151545631885528
  batch 500 loss: 0.9222057974338531
  batch 550 loss: 0.9208073890209199
  batch 600 loss: 0.9063147008419037
  batch 650 loss: 0.929896547794342
  batch 700 loss: 0.9820554459095001
  batch 750 loss: 0.9114921987056732
  batch 800 loss: 0.9472254955768585
  batch 850 loss: 0.972069560289383
  batch 900 loss: 0.9882659924030304
LOSS train 0.98827 valid 1.01305, valid PER 31.69%
EPOCH 12:
  batch 50 loss: 0.8973925471305847
  batch 100 loss: 0.874821983575821
  batch 150 loss: 0.9175312376022339
  batch 200 loss: 0.9307337272167205
  batch 250 loss: 0.9053093242645264
  batch 300 loss: 0.9415886545181275
  batch 350 loss: 0.9269308412075042
  batch 400 loss: 0.9892368304729462
  batch 450 loss: 0.9207256495952606
  batch 500 loss: 0.9621107792854309
  batch 550 loss: 0.9344815719127655
  batch 600 loss: 0.9416457724571228
  batch 650 loss: 0.9147302711009979
  batch 700 loss: 0.9009949731826782
  batch 750 loss: 0.9380807530879974
  batch 800 loss: 0.9037394666671753
  batch 850 loss: 0.9264064955711365
  batch 900 loss: 0.9278578829765319
LOSS train 0.92786 valid 0.99346, valid PER 30.64%
EPOCH 13:
  batch 50 loss: 0.8731818056106567
  batch 100 loss: 0.8721949183940887
  batch 150 loss: 0.8994579339027404
  batch 200 loss: 0.8482953178882598
  batch 250 loss: 0.8632023739814758
  batch 300 loss: 0.8961535501480102
  batch 350 loss: 0.846505012512207
  batch 400 loss: 0.8659263527393342
  batch 450 loss: 0.8943365693092347
  batch 500 loss: 0.9328674149513244
  batch 550 loss: 0.9754166269302368
  batch 600 loss: 0.8972074341773987
  batch 650 loss: 0.8901053762435913
  batch 700 loss: 0.9168021154403686
  batch 750 loss: 0.8765490341186524
  batch 800 loss: 0.9043018066883087
  batch 850 loss: 0.9005045771598816
  batch 900 loss: 0.9001659154891968
LOSS train 0.90017 valid 0.97064, valid PER 30.16%
EPOCH 14:
  batch 50 loss: 0.8558281469345093
  batch 100 loss: 0.8444967710971832
  batch 150 loss: 0.8758462798595429
  batch 200 loss: 0.8578508245944977
  batch 250 loss: 0.8782974004745483
  batch 300 loss: 0.8652535212039948
  batch 350 loss: 0.8767040741443634
  batch 400 loss: 0.8966914141178131
  batch 450 loss: 0.8430285060405731
  batch 500 loss: 0.8849610304832458
  batch 550 loss: 0.893215411901474
  batch 600 loss: 0.8787954592704773
  batch 650 loss: 0.885993869304657
  batch 700 loss: 0.9147186875343323
  batch 750 loss: 0.8845170450210571
  batch 800 loss: 0.8747960376739502
  batch 850 loss: 0.9257773172855377
  batch 900 loss: 0.8968733763694763
LOSS train 0.89687 valid 0.98451, valid PER 31.31%
EPOCH 15:
  batch 50 loss: 0.7984876608848572
  batch 100 loss: 0.8369857323169708
  batch 150 loss: 0.8250797307491302
  batch 200 loss: 0.8783777773380279
  batch 250 loss: 0.904621000289917
  batch 300 loss: 0.8829316926002503
  batch 350 loss: 0.8485232055187225
  batch 400 loss: 0.8850716722011566
  batch 450 loss: 0.8777488327026367
  batch 500 loss: 0.8705346822738648
  batch 550 loss: 0.9270029938220978
  batch 600 loss: 0.8926259958744049
  batch 650 loss: 0.8500633752346038
  batch 700 loss: 0.8661039131879806
  batch 750 loss: 0.8769364953041077
  batch 800 loss: 0.8490176606178284
  batch 850 loss: 0.8520041012763977
  batch 900 loss: 0.8239491933584213
LOSS train 0.82395 valid 0.98042, valid PER 30.92%
EPOCH 16:
  batch 50 loss: 0.8445531558990479
  batch 100 loss: 0.8018532931804657
  batch 150 loss: 0.8714574074745178
  batch 200 loss: 0.8703966760635375
  batch 250 loss: 0.8310051000118256
  batch 300 loss: 0.8652087390422821
  batch 350 loss: 0.8736626660823822
  batch 400 loss: 0.8365337860584259
  batch 450 loss: 0.8614447128772735
  batch 500 loss: 0.8362399995326996
  batch 550 loss: 0.8318053984642029
  batch 600 loss: 0.8519694495201111
  batch 650 loss: 0.8565930271148682
  batch 700 loss: 0.8401857435703277
  batch 750 loss: 0.8552874231338501
  batch 800 loss: 0.8454248797893524
  batch 850 loss: 0.8156629502773285
  batch 900 loss: 0.8478318440914154
LOSS train 0.84783 valid 0.97144, valid PER 30.26%
EPOCH 17:
  batch 50 loss: 0.8358618235588073
  batch 100 loss: 0.7850037002563477
  batch 150 loss: 0.8772186815738678
  batch 200 loss: 0.8112995398044586
  batch 250 loss: 0.8321301662921905
  batch 300 loss: 0.821763994693756
  batch 350 loss: 0.8346695113182068
  batch 400 loss: 0.843581246137619
  batch 450 loss: 0.8507297086715698
  batch 500 loss: 0.8410148048400878
  batch 550 loss: 0.8427998387813568
  batch 600 loss: 0.8444952702522278
  batch 650 loss: 0.8190119552612305
  batch 700 loss: 0.8453914523124695
  batch 750 loss: 0.8153775680065155
  batch 800 loss: 0.8561257219314575
  batch 850 loss: 0.8236410868167877
  batch 900 loss: 0.8309892129898071
LOSS train 0.83099 valid 0.95018, valid PER 29.76%
EPOCH 18:
  batch 50 loss: 0.8271876609325409
  batch 100 loss: 0.7874669718742371
  batch 150 loss: 0.8568984973430633
  batch 200 loss: 0.8223443102836608
  batch 250 loss: 0.8109117603302002
  batch 300 loss: 0.8295092356204986
  batch 350 loss: 0.824760788679123
  batch 400 loss: 0.8907679188251495
  batch 450 loss: 0.9606407356262207
  batch 500 loss: 0.9027760422229767
  batch 550 loss: 0.9107096672058106
  batch 600 loss: 0.8985451948642731
  batch 650 loss: 0.850083475112915
  batch 700 loss: 0.8330456650257111
  batch 750 loss: 0.8471101117134094
  batch 800 loss: 0.869620726108551
  batch 850 loss: 0.8615036034584045
  batch 900 loss: 0.847315810918808
LOSS train 0.84732 valid 0.94784, valid PER 29.58%
EPOCH 19:
  batch 50 loss: 0.803756138086319
  batch 100 loss: 0.8100319814682007
  batch 150 loss: 0.7840782058238983
  batch 200 loss: 0.7790226984024048
  batch 250 loss: 0.8259261512756347
  batch 300 loss: 0.824961119890213
  batch 350 loss: 0.8423853945732117
  batch 400 loss: 0.8236910057067871
  batch 450 loss: 0.7668523991107941
  batch 500 loss: 0.7981512594223023
  batch 550 loss: 0.8201172268390655
  batch 600 loss: 0.8612166357040405
  batch 650 loss: 0.8466544246673584
  batch 700 loss: 0.8781546258926392
  batch 750 loss: 0.8086634624004364
  batch 800 loss: 0.7909963715076447
  batch 850 loss: 0.8431893122196198
  batch 900 loss: 0.8421533513069153
LOSS train 0.84215 valid 0.95125, valid PER 29.78%
EPOCH 20:
  batch 50 loss: 0.7499735087156296
  batch 100 loss: 0.7922467339038849
  batch 150 loss: 0.7822012668848037
  batch 200 loss: 0.7848755162954331
  batch 250 loss: 0.812399959564209
  batch 300 loss: 0.8011799705028534
  batch 350 loss: 0.7827621001005173
  batch 400 loss: 0.7864575159549713
  batch 450 loss: 0.7854343688488007
  batch 500 loss: 0.8004558777809143
  batch 550 loss: 0.7924644684791565
  batch 600 loss: 0.7992206931114196
  batch 650 loss: 0.8818297576904297
  batch 700 loss: 0.8038116645812988
  batch 750 loss: 0.8105849289894104
  batch 800 loss: 0.861633894443512
  batch 850 loss: 0.8402956867218018
  batch 900 loss: 0.8117542159557343
LOSS train 0.81175 valid 0.96960, valid PER 29.91%
Training finished in 4.0 minutes.
Model saved to checkpoints/20230115_202942/model_18
Loading model from checkpoints/20230115_202942/model_18
SUB: 16.25%, DEL: 13.49%, INS: 1.79%, COR: 70.26%, PER: 31.53%
