Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.3)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.290940098762512
  batch 100 loss: 3.2925259399414064
  batch 150 loss: 3.1763439702987672
  batch 200 loss: 2.995164165496826
  batch 250 loss: 2.7644610738754274
  batch 300 loss: 2.579494767189026
  batch 350 loss: 2.426700768470764
  batch 400 loss: 2.374533233642578
  batch 450 loss: 2.3236886405944825
  batch 500 loss: 2.1985861611366273
  batch 550 loss: 2.1338755655288697
  batch 600 loss: 2.076595664024353
  batch 650 loss: 2.0241510701179504
  batch 700 loss: 1.9984164643287659
  batch 750 loss: 1.936247866153717
  batch 800 loss: 1.909246952533722
  batch 850 loss: 1.8621743822097778
  batch 900 loss: 1.8500250744819642
LOSS train 1.85003 valid 1.83572, valid PER 69.51%
EPOCH 2:
  batch 50 loss: 1.846795380115509
  batch 100 loss: 1.7942631483078002
  batch 150 loss: 1.6830052304267884
  batch 200 loss: 1.722359585762024
  batch 250 loss: 1.7015537476539613
  batch 300 loss: 1.6704033398628235
  batch 350 loss: 1.662469472885132
  batch 400 loss: 1.6318467140197754
  batch 450 loss: 1.6325688290596008
  batch 500 loss: 1.6026139402389525
  batch 550 loss: 1.6025273084640503
  batch 600 loss: 1.617298800945282
  batch 650 loss: 1.5414379525184632
  batch 700 loss: 1.565762891769409
  batch 750 loss: 1.5357899284362793
  batch 800 loss: 1.4995453763008117
  batch 850 loss: 1.5192248272895812
  batch 900 loss: 1.468602213859558
LOSS train 1.46860 valid 1.43418, valid PER 49.51%
EPOCH 3:
  batch 50 loss: 1.4290005707740783
  batch 100 loss: 1.483419690132141
  batch 150 loss: 1.4904573059082031
  batch 200 loss: 1.4052179861068725
  batch 250 loss: 1.4318431305885315
  batch 300 loss: 1.421063723564148
  batch 350 loss: 1.4193400287628173
  batch 400 loss: 1.404017858505249
  batch 450 loss: 1.3680281257629394
  batch 500 loss: 1.357771816253662
  batch 550 loss: 1.3606434559822083
  batch 600 loss: 1.3095321989059447
  batch 650 loss: 1.3442801213264466
  batch 700 loss: 1.3445742011070252
  batch 750 loss: 1.3810270380973817
  batch 800 loss: 1.355746021270752
  batch 850 loss: 1.3417776370048522
  batch 900 loss: 1.3151796531677247
LOSS train 1.31518 valid 1.23181, valid PER 40.23%
EPOCH 4:
  batch 50 loss: 1.3107441532611848
  batch 100 loss: 1.253220430612564
  batch 150 loss: 1.2869382119178772
  batch 200 loss: 1.2532620668411254
  batch 250 loss: 1.319135503768921
  batch 300 loss: 1.2916388630867004
  batch 350 loss: 1.280462565422058
  batch 400 loss: 1.2202586221694947
  batch 450 loss: 1.2440799510478973
  batch 500 loss: 1.288723020553589
  batch 550 loss: 1.2458481645584107
  batch 600 loss: 1.2047309923171996
  batch 650 loss: 1.2799492681026459
  batch 700 loss: 1.30896742105484
  batch 750 loss: 1.2338637793064118
  batch 800 loss: 1.2289537179470063
  batch 850 loss: 1.2132192265987396
  batch 900 loss: 1.2365357172489166
LOSS train 1.23654 valid 1.16090, valid PER 36.92%
EPOCH 5:
  batch 50 loss: 1.2089926671981812
  batch 100 loss: 1.2017079389095306
  batch 150 loss: 1.2040004360675811
  batch 200 loss: 1.2291820418834687
  batch 250 loss: 1.1909377133846284
  batch 300 loss: 1.211006337404251
  batch 350 loss: 1.158965835571289
  batch 400 loss: 1.1452538907527923
  batch 450 loss: 1.1668003273010255
  batch 500 loss: 1.1634062695503236
  batch 550 loss: 1.207629190683365
  batch 600 loss: 1.1903943228721618
  batch 650 loss: 1.2073818504810334
  batch 700 loss: 1.1912796890735626
  batch 750 loss: 1.166522912979126
  batch 800 loss: 1.194321072101593
  batch 850 loss: 1.2009894847869873
  batch 900 loss: 1.1635413920879365
LOSS train 1.16354 valid 1.11421, valid PER 36.28%
EPOCH 6:
  batch 50 loss: 1.13783163189888
  batch 100 loss: 1.156189913749695
  batch 150 loss: 1.127036064863205
  batch 200 loss: 1.1130014193058013
  batch 250 loss: 1.1465612947940826
  batch 300 loss: 1.1671675753593445
  batch 350 loss: 1.1619484293460847
  batch 400 loss: 1.1564153230190277
  batch 450 loss: 1.1503348553180694
  batch 500 loss: 1.0848538255691529
  batch 550 loss: 1.1200529539585113
  batch 600 loss: 1.132804253101349
  batch 650 loss: 1.0933524560928345
  batch 700 loss: 1.086849834918976
  batch 750 loss: 1.1302204608917237
  batch 800 loss: 1.1123012113571167
  batch 850 loss: 1.1610308408737182
  batch 900 loss: 1.1697921752929688
LOSS train 1.16979 valid 1.07005, valid PER 35.67%
EPOCH 7:
  batch 50 loss: 1.0692508852481841
  batch 100 loss: 1.1297542572021484
  batch 150 loss: 1.0648023295402527
  batch 200 loss: 1.0537679266929627
  batch 250 loss: 1.1198047649860383
  batch 300 loss: 1.08396249294281
  batch 350 loss: 1.104921293258667
  batch 400 loss: 1.0944249439239502
  batch 450 loss: 1.1019761610031127
  batch 500 loss: 1.1002421855926514
  batch 550 loss: 1.0711698651313781
  batch 600 loss: 1.0631506955623626
  batch 650 loss: 1.043621678352356
  batch 700 loss: 1.1113246095180511
  batch 750 loss: 1.0799998486042022
  batch 800 loss: 1.079907476902008
  batch 850 loss: 1.0576625645160675
  batch 900 loss: 1.0560728895664215
LOSS train 1.05607 valid 1.05486, valid PER 34.36%
EPOCH 8:
  batch 50 loss: 1.0680116021633148
  batch 100 loss: 1.0292638409137727
  batch 150 loss: 1.0679471814632415
  batch 200 loss: 1.057586760520935
  batch 250 loss: 1.044204088449478
  batch 300 loss: 1.029936192035675
  batch 350 loss: 1.0805220592021942
  batch 400 loss: 1.0580979645252229
  batch 450 loss: 1.0902135515213012
  batch 500 loss: 1.083154295682907
  batch 550 loss: 1.0835845577716827
  batch 600 loss: 1.0378703022003173
  batch 650 loss: 1.0326940774917603
  batch 700 loss: 1.0643408763408662
  batch 750 loss: 1.0767702412605287
  batch 800 loss: 1.0715668904781341
  batch 850 loss: 1.0170984435081483
  batch 900 loss: 1.049878672361374
LOSS train 1.04988 valid 1.07917, valid PER 33.95%
EPOCH 9:
  batch 50 loss: 1.0252946722507477
  batch 100 loss: 1.008305777311325
  batch 150 loss: 1.0241446948051454
  batch 200 loss: 1.0054227316379547
  batch 250 loss: 1.0180403733253478
  batch 300 loss: 1.0547371077537537
  batch 350 loss: 1.0017440962791442
  batch 400 loss: 1.0480606508255006
  batch 450 loss: 1.0695027101039887
  batch 500 loss: 1.0317405200004577
  batch 550 loss: 1.0416627728939056
  batch 600 loss: 1.0504172480106353
  batch 650 loss: 1.0355529880523682
  batch 700 loss: 1.0180894446372986
  batch 750 loss: 1.0469887256622314
  batch 800 loss: 1.049230501651764
  batch 850 loss: 1.0695378029346465
  batch 900 loss: 1.0222718966007234
LOSS train 1.02227 valid 1.01563, valid PER 33.49%
EPOCH 10:
  batch 50 loss: 0.9874979400634766
  batch 100 loss: 1.0071508824825286
  batch 150 loss: 1.040414571762085
  batch 200 loss: 0.9840636765956878
  batch 250 loss: 1.0040347898006439
  batch 300 loss: 0.9955582213401795
  batch 350 loss: 0.9916432344913483
  batch 400 loss: 0.9642818605899811
  batch 450 loss: 0.9923524212837219
  batch 500 loss: 1.0115240573883058
  batch 550 loss: 1.0234332966804505
  batch 600 loss: 1.0146761536598206
  batch 650 loss: 1.0109116816520691
  batch 700 loss: 1.021637578010559
  batch 750 loss: 1.015176556110382
  batch 800 loss: 1.0017094564437867
  batch 850 loss: 1.0186031138896943
  batch 900 loss: 1.0008932113647462
LOSS train 1.00089 valid 1.01763, valid PER 33.13%
EPOCH 11:
  batch 50 loss: 0.95867107629776
  batch 100 loss: 0.9634778249263763
  batch 150 loss: 0.9821851050853729
  batch 200 loss: 0.9570246434211731
  batch 250 loss: 0.9701788711547852
  batch 300 loss: 0.9821797406673431
  batch 350 loss: 1.0105352866649628
  batch 400 loss: 0.9644887566566467
  batch 450 loss: 0.9524896538257599
  batch 500 loss: 0.9657404804229737
  batch 550 loss: 0.9764633357524872
  batch 600 loss: 0.9576964342594146
  batch 650 loss: 0.9798640108108521
  batch 700 loss: 1.0927472054958343
  batch 750 loss: 0.9830116128921509
  batch 800 loss: 1.0016106235980988
  batch 850 loss: 1.0035859274864196
  batch 900 loss: 1.0220881593227387
LOSS train 1.02209 valid 0.98530, valid PER 31.61%
EPOCH 12:
  batch 50 loss: 0.9174542880058288
  batch 100 loss: 0.9200650751590729
  batch 150 loss: 0.9477865993976593
  batch 200 loss: 0.9794307208061218
  batch 250 loss: 0.9292976152896881
  batch 300 loss: 0.961881091594696
  batch 350 loss: 0.9373596775531768
  batch 400 loss: 0.9551494741439819
  batch 450 loss: 0.9432417905330658
  batch 500 loss: 0.953671760559082
  batch 550 loss: 0.9497238445281982
  batch 600 loss: 0.9567914915084839
  batch 650 loss: 0.9547776091098785
  batch 700 loss: 0.9392599272727966
  batch 750 loss: 0.9779958045482635
  batch 800 loss: 0.9144313383102417
  batch 850 loss: 0.9442085385322571
  batch 900 loss: 0.9786192095279693
LOSS train 0.97862 valid 1.00962, valid PER 32.54%
EPOCH 13:
  batch 50 loss: 0.8978348934650421
  batch 100 loss: 0.9312781190872192
  batch 150 loss: 0.9596659481525421
  batch 200 loss: 0.9174357283115387
  batch 250 loss: 0.9225614130496979
  batch 300 loss: 0.9540928483009339
  batch 350 loss: 0.9052558588981628
  batch 400 loss: 0.9132266056537628
  batch 450 loss: 0.9438003981113434
  batch 500 loss: 0.9085222601890564
  batch 550 loss: 0.9740493273735047
  batch 600 loss: 0.9332918858528138
  batch 650 loss: 0.9215425395965576
  batch 700 loss: 0.9478709185123444
  batch 750 loss: 0.9133110845088959
  batch 800 loss: 0.9410643577575684
  batch 850 loss: 0.9188471519947052
  batch 900 loss: 0.9355093854665756
LOSS train 0.93551 valid 0.96066, valid PER 30.78%
EPOCH 14:
  batch 50 loss: 0.8891644442081451
  batch 100 loss: 0.8706793820858002
  batch 150 loss: 0.9266062140464782
  batch 200 loss: 1.003933346271515
  batch 250 loss: 0.9393318545818329
  batch 300 loss: 0.904282603263855
  batch 350 loss: 0.9190634894371033
  batch 400 loss: 0.9635788643360138
  batch 450 loss: 0.9134593033790588
  batch 500 loss: 0.922147421836853
  batch 550 loss: 0.9519131016731263
  batch 600 loss: 0.949528352022171
  batch 650 loss: 0.9316352963447571
  batch 700 loss: 0.9543179857730866
  batch 750 loss: 0.9263979542255402
  batch 800 loss: 0.955072067975998
  batch 850 loss: 0.9972718036174775
  batch 900 loss: 0.9496097362041473
LOSS train 0.94961 valid 0.95904, valid PER 30.96%
EPOCH 15:
  batch 50 loss: 0.9000425601005554
  batch 100 loss: 0.9225707912445068
  batch 150 loss: 0.860662294626236
  batch 200 loss: 0.9376849782466888
  batch 250 loss: 0.9403008222579956
  batch 300 loss: 0.9379654705524445
  batch 350 loss: 0.9094399249553681
  batch 400 loss: 0.927435907125473
  batch 450 loss: 0.930486900806427
  batch 500 loss: 0.9009471309185028
  batch 550 loss: 0.9936392223834991
  batch 600 loss: 1.015486695766449
  batch 650 loss: 0.9303184795379639
  batch 700 loss: 0.9089226150512695
  batch 750 loss: 0.9612704861164093
  batch 800 loss: 0.9055573678016663
  batch 850 loss: 0.915036859512329
  batch 900 loss: 0.8691188973188401
LOSS train 0.86912 valid 0.98320, valid PER 31.67%
EPOCH 16:
  batch 50 loss: 0.8841436088085175
  batch 100 loss: 0.886793577671051
  batch 150 loss: 0.8928467226028443
  batch 200 loss: 0.9273223662376404
  batch 250 loss: 0.8978052091598511
  batch 300 loss: 0.9214470672607422
  batch 350 loss: 0.9336071968078613
  batch 400 loss: 0.8988717401027679
  batch 450 loss: 0.9182921874523163
  batch 500 loss: 0.9050277769565582
  batch 550 loss: 0.9077604484558105
  batch 600 loss: 0.9397859156131745
  batch 650 loss: 0.9595355558395385
  batch 700 loss: 0.9168409621715545
  batch 750 loss: 0.9292481374740601
  batch 800 loss: 0.8996190965175629
  batch 850 loss: 0.8946409690380096
  batch 900 loss: 0.9158079910278321
LOSS train 0.91581 valid 1.01517, valid PER 32.53%
EPOCH 17:
  batch 50 loss: 0.8961832749843598
  batch 100 loss: 0.93143639087677
  batch 150 loss: 0.9770541143417358
  batch 200 loss: 0.9359697997570038
  batch 250 loss: 0.9584159457683563
  batch 300 loss: 0.9341708278656006
  batch 350 loss: 0.9197675025463105
  batch 400 loss: 0.9701484787464142
  batch 450 loss: 0.9260434234142303
  batch 500 loss: 0.9315449845790863
  batch 550 loss: 0.9079560267925263
  batch 600 loss: 0.9445016586780548
  batch 650 loss: 0.9235778868198394
  batch 700 loss: 0.9531826341152191
  batch 750 loss: 0.9058076459169387
  batch 800 loss: 0.9559753406047821
  batch 850 loss: 0.9247809326648713
  batch 900 loss: 0.9370810842514038
LOSS train 0.93708 valid 0.98621, valid PER 31.10%
EPOCH 18:
  batch 50 loss: 0.9140191626548767
  batch 100 loss: 0.8998897755146027
  batch 150 loss: 0.9367369103431702
  batch 200 loss: 0.9516466653347015
  batch 250 loss: 0.9255694699287415
  batch 300 loss: 0.9175789654254913
  batch 350 loss: 0.8955422520637513
  batch 400 loss: 0.9902455532550811
  batch 450 loss: 1.0597074818611145
  batch 500 loss: 0.9988855731487274
  batch 550 loss: 1.0378562414646149
  batch 600 loss: 0.9921637463569641
  batch 650 loss: 0.977459293603897
  batch 700 loss: 0.9389743638038636
  batch 750 loss: 0.9738857984542847
  batch 800 loss: 0.9726995372772217
  batch 850 loss: 0.9452002727985382
  batch 900 loss: 0.9493057513237
LOSS train 0.94931 valid 0.98255, valid PER 31.79%
EPOCH 19:
  batch 50 loss: 0.9105081844329834
  batch 100 loss: 0.9174501419067382
  batch 150 loss: 0.9347579264640808
  batch 200 loss: 0.9361974561214447
  batch 250 loss: 0.9414998185634613
  batch 300 loss: 0.9372017681598663
  batch 350 loss: 0.9492618274688721
  batch 400 loss: 0.9372788345813752
  batch 450 loss: 0.8799762463569641
  batch 500 loss: 0.9099653708934784
  batch 550 loss: 0.9191456270217896
  batch 600 loss: 0.9168939185142517
  batch 650 loss: 0.9334831929206848
  batch 700 loss: 0.9357519853115082
  batch 750 loss: 0.8641627657413483
  batch 800 loss: 0.8702232170104981
  batch 850 loss: 0.9096411967277527
  batch 900 loss: 0.8798915612697601
LOSS train 0.87989 valid 0.95132, valid PER 30.42%
EPOCH 20:
  batch 50 loss: 0.8355032926797867
  batch 100 loss: 0.8625997841358185
  batch 150 loss: 0.8897859835624695
  batch 200 loss: 0.8911135065555572
  batch 250 loss: 0.9260294604301452
  batch 300 loss: 0.8974806272983551
  batch 350 loss: 0.8764249187707901
  batch 400 loss: 0.8789620769023895
  batch 450 loss: 0.8857207357883453
  batch 500 loss: 0.9078028690814972
  batch 550 loss: 0.8839780604839325
  batch 600 loss: 0.863287695646286
  batch 650 loss: 0.8948278546333313
  batch 700 loss: 0.8673750126361847
  batch 750 loss: 0.8474198520183563
  batch 800 loss: 0.8791149628162384
  batch 850 loss: 0.9046186447143555
  batch 900 loss: 0.8891857326030731
LOSS train 0.88919 valid 0.95673, valid PER 30.78%
Training finished in 3.0 minutes.
Model saved to checkpoints/20230115_205448/model_19
Loading model from checkpoints/20230115_205448/model_19
SUB: 16.75%, DEL: 14.11%, INS: 1.84%, COR: 69.14%, PER: 32.70%
