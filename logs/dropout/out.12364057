Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.9)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.125912246704101
  batch 100 loss: 3.2553057622909547
  batch 150 loss: 3.138884325027466
  batch 200 loss: 3.0277222204208374
  batch 250 loss: 2.9065317153930663
  batch 300 loss: 2.7679512166976927
  batch 350 loss: 2.6797045183181765
  batch 400 loss: 2.6015682363510133
  batch 450 loss: 2.5616162252426147
  batch 500 loss: 2.486927785873413
  batch 550 loss: 2.4496161937713623
  batch 600 loss: 2.445057888031006
  batch 650 loss: 2.3923308563232424
  batch 700 loss: 2.396898822784424
  batch 750 loss: 2.4092959451675413
  batch 800 loss: 2.3638426303863525
  batch 850 loss: 2.356274380683899
  batch 900 loss: 2.347664465904236
LOSS train 2.34766 valid 2.46005, valid PER 83.21%
EPOCH 2:
  batch 50 loss: 2.3567056035995484
  batch 100 loss: 2.331316509246826
  batch 150 loss: 2.2357561016082763
  batch 200 loss: 2.2772268533706663
  batch 250 loss: 2.28967875957489
  batch 300 loss: 2.2561949062347413
  batch 350 loss: 2.2461736011505127
  batch 400 loss: 2.2066836547851563
  batch 450 loss: 2.230865521430969
  batch 500 loss: 2.210371811389923
  batch 550 loss: 2.212745909690857
  batch 600 loss: 2.188309197425842
  batch 650 loss: 2.1736877584457397
  batch 700 loss: 2.1787019753456116
  batch 750 loss: 2.182063543796539
  batch 800 loss: 2.1426571154594423
  batch 850 loss: 2.1666932702064514
  batch 900 loss: 2.1329475355148317
LOSS train 2.13295 valid 2.14983, valid PER 78.88%
EPOCH 3:
  batch 50 loss: 2.097271370887756
  batch 100 loss: 2.152593483924866
  batch 150 loss: 2.151065249443054
  batch 200 loss: 2.0891280651092528
  batch 250 loss: 2.116556370258331
  batch 300 loss: 2.1367069602012636
  batch 350 loss: 2.1190767955780028
  batch 400 loss: 2.1002961468696593
  batch 450 loss: 2.09247629404068
  batch 500 loss: 2.073122487068176
  batch 550 loss: 2.07209600687027
  batch 600 loss: 2.035971519947052
  batch 650 loss: 2.0631615567207335
  batch 700 loss: 2.0436503052711488
  batch 750 loss: 2.0869209814071654
  batch 800 loss: 2.0889729833602906
  batch 850 loss: 2.0286525321006774
  batch 900 loss: 2.056358828544617
LOSS train 2.05636 valid 2.06188, valid PER 77.50%
EPOCH 4:
  batch 50 loss: 2.045951631069183
  batch 100 loss: 2.002136206626892
  batch 150 loss: 2.0445000863075258
  batch 200 loss: 2.0170843815803527
  batch 250 loss: 2.0421290302276613
  batch 300 loss: 2.015731062889099
  batch 350 loss: 2.015443379878998
  batch 400 loss: 1.9718891978263855
  batch 450 loss: 2.0066958451271057
  batch 500 loss: 2.0461647629737856
  batch 550 loss: 1.989322829246521
  batch 600 loss: 1.9862955570220948
  batch 650 loss: 2.016705050468445
  batch 700 loss: 2.059399688243866
  batch 750 loss: 2.003999261856079
  batch 800 loss: 2.001252944469452
  batch 850 loss: 1.9467411160469055
  batch 900 loss: 1.9733883213996888
LOSS train 1.97339 valid 1.93910, valid PER 76.10%
EPOCH 5:
  batch 50 loss: 1.9901044178009033
  batch 100 loss: 1.9507912874221802
  batch 150 loss: 1.9664726114273072
  batch 200 loss: 1.9911343050003052
  batch 250 loss: 1.9546810293197632
  batch 300 loss: 1.9774640607833862
  batch 350 loss: 1.9427255845069886
  batch 400 loss: 1.9497978138923644
  batch 450 loss: 1.9282641696929932
  batch 500 loss: 1.9196547555923462
  batch 550 loss: 1.961190514564514
  batch 600 loss: 1.9548184752464295
  batch 650 loss: 1.9457741093635559
  batch 700 loss: 1.9366900396347047
  batch 750 loss: 1.9274027395248412
  batch 800 loss: 1.9650838041305543
  batch 850 loss: 1.9326084637641907
  batch 900 loss: 1.9117362189292908
LOSS train 1.91174 valid 1.90154, valid PER 74.54%
EPOCH 6:
  batch 50 loss: 1.9137214636802673
  batch 100 loss: 1.929263892173767
  batch 150 loss: 1.895979084968567
  batch 200 loss: 1.8696677136421203
  batch 250 loss: 1.9153949165344237
  batch 300 loss: 1.9231266808509826
  batch 350 loss: 1.9245224261283875
  batch 400 loss: 1.908506109714508
  batch 450 loss: 1.9120571327209472
  batch 500 loss: 1.895301399230957
  batch 550 loss: 1.903459873199463
  batch 600 loss: 1.8871508502960206
  batch 650 loss: 1.8805321168899536
  batch 700 loss: 1.878568081855774
  batch 750 loss: 1.92162832736969
  batch 800 loss: 1.892954137325287
  batch 850 loss: 1.9031349968910218
  batch 900 loss: 1.8940027570724487
LOSS train 1.89400 valid 1.86179, valid PER 75.12%
EPOCH 7:
  batch 50 loss: 1.874705846309662
  batch 100 loss: 1.902930223941803
  batch 150 loss: 1.8907528138160705
  batch 200 loss: 1.8432816529273988
  batch 250 loss: 1.8928011798858642
  batch 300 loss: 1.8702615022659301
  batch 350 loss: 1.8725755548477172
  batch 400 loss: 1.8563594031333923
  batch 450 loss: 1.9049053978919983
  batch 500 loss: 1.879578778743744
  batch 550 loss: 1.8243429160118103
  batch 600 loss: 1.848366665840149
  batch 650 loss: 1.8089507508277893
  batch 700 loss: 1.883689556121826
  batch 750 loss: 1.8526949024200439
  batch 800 loss: 1.852133538722992
  batch 850 loss: 1.8229553365707398
  batch 900 loss: 1.8454390239715577
LOSS train 1.84544 valid 1.85390, valid PER 73.82%
EPOCH 8:
  batch 50 loss: 1.847642970085144
  batch 100 loss: 1.8220429635047912
  batch 150 loss: 1.8740443396568298
  batch 200 loss: 1.840091326236725
  batch 250 loss: 1.8265078520774842
  batch 300 loss: 1.8162895298004151
  batch 350 loss: 1.8222075057029725
  batch 400 loss: 1.8000279664993286
  batch 450 loss: 1.8838524174690248
  batch 500 loss: 1.8584721851348878
  batch 550 loss: 1.8468911838531494
  batch 600 loss: 1.7918386936187745
  batch 650 loss: 1.8114688277244568
  batch 700 loss: 1.8465231084823608
  batch 750 loss: 1.833806426525116
  batch 800 loss: 1.8308155989646913
  batch 850 loss: 1.8047081518173218
  batch 900 loss: 1.8283650612831115
LOSS train 1.82837 valid 1.78410, valid PER 70.78%
EPOCH 9:
  batch 50 loss: 1.8251594018936157
  batch 100 loss: 1.8016540861129762
  batch 150 loss: 1.8122833585739135
  batch 200 loss: 1.7713601803779602
  batch 250 loss: 1.7945487427711486
  batch 300 loss: 1.8351201343536376
  batch 350 loss: 1.8032422041893006
  batch 400 loss: 1.8240622115135192
  batch 450 loss: 1.8037692308425903
  batch 500 loss: 1.7904530692100524
  batch 550 loss: 1.8029465985298156
  batch 600 loss: 1.8338062119483949
  batch 650 loss: 1.803004217147827
  batch 700 loss: 1.786630427837372
  batch 750 loss: 1.7813401317596436
  batch 800 loss: 1.8515490555763245
  batch 850 loss: 1.8255818510055541
  batch 900 loss: 1.7829626321792602
LOSS train 1.78296 valid 1.74082, valid PER 71.25%
EPOCH 10:
  batch 50 loss: 1.784280812740326
  batch 100 loss: 1.8242902970314026
  batch 150 loss: 1.824020562171936
  batch 200 loss: 1.788309133052826
  batch 250 loss: 1.7995407676696777
  batch 300 loss: 1.784048137664795
  batch 350 loss: 1.7600616979599
  batch 400 loss: 1.7711672711372375
  batch 450 loss: 1.7646699094772338
  batch 500 loss: 1.7617759561538697
  batch 550 loss: 1.7571580266952516
  batch 600 loss: 1.7587344002723695
  batch 650 loss: 1.7788054323196412
  batch 700 loss: 1.8046599125862122
  batch 750 loss: 1.7962170052528381
  batch 800 loss: 1.7671343970298767
  batch 850 loss: 1.7646402621269226
  batch 900 loss: 1.7437585616111755
LOSS train 1.74376 valid 1.85769, valid PER 71.71%
EPOCH 11:
  batch 50 loss: 1.7813023948669433
  batch 100 loss: 1.7586216187477113
  batch 150 loss: 1.7232874131202698
  batch 200 loss: 1.732929356098175
  batch 250 loss: 1.7212580609321595
  batch 300 loss: 1.7632206511497497
  batch 350 loss: 1.7741830515861512
  batch 400 loss: 1.7431511688232422
  batch 450 loss: 1.7455263686180116
  batch 500 loss: 1.7533685564994812
  batch 550 loss: 1.7674804472923278
  batch 600 loss: 1.7650355982780457
  batch 650 loss: 1.7634594249725342
  batch 700 loss: 1.8252375388145448
  batch 750 loss: 1.7353035926818847
  batch 800 loss: 1.7636251425743104
  batch 850 loss: 1.736966485977173
  batch 900 loss: 1.7731557202339172
LOSS train 1.77316 valid 1.67396, valid PER 68.06%
EPOCH 12:
  batch 50 loss: 1.7383317017555238
  batch 100 loss: 1.6939433598518372
  batch 150 loss: 1.7223875713348389
  batch 200 loss: 1.754842348098755
  batch 250 loss: 1.7299346399307252
  batch 300 loss: 1.7760530972480775
  batch 350 loss: 1.725787696838379
  batch 400 loss: 1.731353976726532
  batch 450 loss: 1.7363797569274901
  batch 500 loss: 1.7381821656227112
  batch 550 loss: 1.7499480080604553
  batch 600 loss: 1.7409526586532593
  batch 650 loss: 1.720444164276123
  batch 700 loss: 1.7262671279907227
  batch 750 loss: 1.769007751941681
  batch 800 loss: 1.7056150031089783
  batch 850 loss: 1.7189511227607728
  batch 900 loss: 1.7089309740066527
LOSS train 1.70893 valid 1.63159, valid PER 65.98%
EPOCH 13:
  batch 50 loss: 1.7032797718048096
  batch 100 loss: 1.7264661931991576
  batch 150 loss: 1.7257112765312195
  batch 200 loss: 1.6777668046951293
  batch 250 loss: 1.716699583530426
  batch 300 loss: 1.7476742434501649
  batch 350 loss: 1.6991749382019044
  batch 400 loss: 1.713982377052307
  batch 450 loss: 1.723497657775879
  batch 500 loss: 1.718999547958374
  batch 550 loss: 1.7811079168319701
  batch 600 loss: 1.7531753778457642
  batch 650 loss: 1.7125886058807374
  batch 700 loss: 1.7339504885673522
  batch 750 loss: 1.691429817676544
  batch 800 loss: 1.732428104877472
  batch 850 loss: 1.7365362286567687
  batch 900 loss: 1.7104313898086547
LOSS train 1.71043 valid 1.59142, valid PER 65.37%
EPOCH 14:
  batch 50 loss: 1.6977031135559082
  batch 100 loss: 1.700355715751648
  batch 150 loss: 1.7065058994293212
  batch 200 loss: 1.714107506275177
  batch 250 loss: 1.7271246266365052
  batch 300 loss: 1.6746326613426208
  batch 350 loss: 1.6946593284606934
  batch 400 loss: 1.722702944278717
  batch 450 loss: 1.6874871778488159
  batch 500 loss: 1.6950191617012025
  batch 550 loss: 1.7199682021141052
  batch 600 loss: 1.6984687948226929
  batch 650 loss: 1.7233689403533936
  batch 700 loss: 1.7149974036216735
  batch 750 loss: 1.6916735339164735
  batch 800 loss: 1.7099033331871032
  batch 850 loss: 1.7534660911560058
  batch 900 loss: 1.701890320777893
LOSS train 1.70189 valid 1.66336, valid PER 66.61%
EPOCH 15:
  batch 50 loss: 1.6727847409248353
  batch 100 loss: 1.697056429386139
  batch 150 loss: 1.6925412797927857
  batch 200 loss: 1.7148992681503297
  batch 250 loss: 1.7185137033462525
  batch 300 loss: 1.686108181476593
  batch 350 loss: 1.6568250203132628
  batch 400 loss: 1.6889060544967651
  batch 450 loss: 1.69418847322464
  batch 500 loss: 1.7168622899055481
  batch 550 loss: 1.742151939868927
  batch 600 loss: 1.7161939239501953
  batch 650 loss: 1.6866309309005738
  batch 700 loss: 1.678077311515808
  batch 750 loss: 1.737890021800995
  batch 800 loss: 1.6711053133010865
  batch 850 loss: 1.6659344506263734
  batch 900 loss: 1.6239862489700316
LOSS train 1.62399 valid 1.69376, valid PER 67.56%
EPOCH 16:
  batch 50 loss: 1.6641935729980468
  batch 100 loss: 1.6546849727630615
  batch 150 loss: 1.6910325217247009
  batch 200 loss: 1.690475332736969
  batch 250 loss: 1.6519738221168518
  batch 300 loss: 1.7251861929893493
  batch 350 loss: 1.7004857397079467
  batch 400 loss: 1.6702728629112245
  batch 450 loss: 1.7065099716186523
  batch 500 loss: 1.6713424015045166
  batch 550 loss: 1.6377405667304992
  batch 600 loss: 1.6871327400207519
  batch 650 loss: 1.6966030049324035
  batch 700 loss: 1.65028489112854
  batch 750 loss: 1.7168585395812987
  batch 800 loss: 1.6662767910957337
  batch 850 loss: 1.660446388721466
  batch 900 loss: 1.6968916726112366
LOSS train 1.69689 valid 1.57945, valid PER 63.04%
EPOCH 17:
  batch 50 loss: 1.6523148941993713
  batch 100 loss: 1.6257215213775635
  batch 150 loss: 1.7190579414367675
  batch 200 loss: 1.652152225971222
  batch 250 loss: 1.6633902168273926
  batch 300 loss: 1.6657076001167297
  batch 350 loss: 1.6628369450569154
  batch 400 loss: 1.675572144985199
  batch 450 loss: 1.6781083750724792
  batch 500 loss: 1.670239486694336
  batch 550 loss: 1.664260172843933
  batch 600 loss: 1.6858482623100282
  batch 650 loss: 1.650060360431671
  batch 700 loss: 1.6678419971466065
  batch 750 loss: 1.6095593452453614
  batch 800 loss: 1.6862924098968506
  batch 850 loss: 1.6548968982696532
  batch 900 loss: 1.676289074420929
LOSS train 1.67629 valid 1.56078, valid PER 60.87%
EPOCH 18:
  batch 50 loss: 1.6548368859291076
  batch 100 loss: 1.6203118205070495
  batch 150 loss: 1.6949976563453675
  batch 200 loss: 1.6552624917030334
  batch 250 loss: 1.6493300151824952
  batch 300 loss: 1.6526154422760009
  batch 350 loss: 1.6303264021873474
  batch 400 loss: 1.6189229798316955
  batch 450 loss: 1.6676799702644347
  batch 500 loss: 1.66433780670166
  batch 550 loss: 1.6913175535202027
  batch 600 loss: 1.636011152267456
  batch 650 loss: 1.6206696510314942
  batch 700 loss: 1.592133593559265
  batch 750 loss: 1.6581360006332397
  batch 800 loss: 1.6630382180213927
  batch 850 loss: 1.6541590619087219
  batch 900 loss: 1.6368805623054505
LOSS train 1.63688 valid 1.49849, valid PER 59.99%
EPOCH 19:
  batch 50 loss: 1.651497905254364
  batch 100 loss: 1.6605485558509827
  batch 150 loss: 1.6200754642486572
  batch 200 loss: 1.6039619255065918
  batch 250 loss: 1.654255473613739
  batch 300 loss: 1.6257674193382263
  batch 350 loss: 1.660082721710205
  batch 400 loss: 1.6359731578826904
  batch 450 loss: 1.601226909160614
  batch 500 loss: 1.628542194366455
  batch 550 loss: 1.6604509663581848
  batch 600 loss: 1.6485211324691773
  batch 650 loss: 1.648146426677704
  batch 700 loss: 1.6689718794822692
  batch 750 loss: 1.6059865903854371
  batch 800 loss: 1.5896833610534669
  batch 850 loss: 1.653216094970703
  batch 900 loss: 1.6411242246627809
LOSS train 1.64112 valid 1.54395, valid PER 60.65%
EPOCH 20:
  batch 50 loss: 1.583260498046875
  batch 100 loss: 1.6177679538726806
  batch 150 loss: 1.6330439782142638
  batch 200 loss: 1.6331963157653808
  batch 250 loss: 1.6593244314193725
  batch 300 loss: 1.6317146635055542
  batch 350 loss: 1.6154175543785094
  batch 400 loss: 1.6185221934318543
  batch 450 loss: 1.6414616680145264
  batch 500 loss: 1.6357351446151733
  batch 550 loss: 1.6296565794944764
  batch 600 loss: 1.5790485191345214
  batch 650 loss: 1.627546844482422
  batch 700 loss: 1.639340069293976
  batch 750 loss: 1.6336450910568237
  batch 800 loss: 1.632841658592224
  batch 850 loss: 1.6364431285858154
  batch 900 loss: 1.6380687952041626
LOSS train 1.63807 valid 1.55255, valid PER 58.96%
Training finished in 3.0 minutes.
Model saved to checkpoints/20230115_205521/model_18
Loading model from checkpoints/20230115_205521/model_18
SUB: 5.12%, DEL: 56.21%, INS: 0.07%, COR: 38.68%, PER: 61.39%
