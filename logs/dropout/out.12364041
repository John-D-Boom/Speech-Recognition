Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.4)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.202432217597962
  batch 100 loss: 3.304126806259155
  batch 150 loss: 3.260936670303345
  batch 200 loss: 3.083200626373291
  batch 250 loss: 2.8606042861938477
  batch 300 loss: 2.695565452575684
  batch 350 loss: 2.530127458572388
  batch 400 loss: 2.4705639410018922
  batch 450 loss: 2.407579207420349
  batch 500 loss: 2.3070380592346194
  batch 550 loss: 2.266417601108551
  batch 600 loss: 2.1773428964614867
  batch 650 loss: 2.111605260372162
  batch 700 loss: 2.0853216528892515
  batch 750 loss: 2.0256974864006043
  batch 800 loss: 1.9919321131706238
  batch 850 loss: 1.9447679400444031
  batch 900 loss: 1.943195822238922
LOSS train 1.94320 valid 1.91053, valid PER 68.72%
EPOCH 2:
  batch 50 loss: 1.901912727355957
  batch 100 loss: 1.8651152777671813
  batch 150 loss: 1.7300453615188598
  batch 200 loss: 1.7754192900657655
  batch 250 loss: 1.7751569747924805
  batch 300 loss: 1.732948362827301
  batch 350 loss: 1.719379994869232
  batch 400 loss: 1.6736874198913574
  batch 450 loss: 1.6968756747245788
  batch 500 loss: 1.646963460445404
  batch 550 loss: 1.6548986482620238
  batch 600 loss: 1.6412036514282227
  batch 650 loss: 1.5885015273094176
  batch 700 loss: 1.6225258541107177
  batch 750 loss: 1.5899238801002502
  batch 800 loss: 1.5484408235549927
  batch 850 loss: 1.5860031986236571
  batch 900 loss: 1.5176968336105348
LOSS train 1.51770 valid 1.46995, valid PER 51.07%
EPOCH 3:
  batch 50 loss: 1.4643075251579285
  batch 100 loss: 1.5539889311790467
  batch 150 loss: 1.5412830471992494
  batch 200 loss: 1.4787667727470397
  batch 250 loss: 1.489183051586151
  batch 300 loss: 1.4928403067588807
  batch 350 loss: 1.499750428199768
  batch 400 loss: 1.491745901107788
  batch 450 loss: 1.434338128566742
  batch 500 loss: 1.4317723262310027
  batch 550 loss: 1.4284035801887511
  batch 600 loss: 1.382697308063507
  batch 650 loss: 1.410213713645935
  batch 700 loss: 1.4175038456916809
  batch 750 loss: 1.471215100288391
  batch 800 loss: 1.4164244055747985
  batch 850 loss: 1.405217137336731
  batch 900 loss: 1.392181146144867
LOSS train 1.39218 valid 1.30956, valid PER 43.80%
EPOCH 4:
  batch 50 loss: 1.41666188955307
  batch 100 loss: 1.3343239569664
  batch 150 loss: 1.3570152401924134
  batch 200 loss: 1.3408359122276305
  batch 250 loss: 1.3291820144653321
  batch 300 loss: 1.336906042098999
  batch 350 loss: 1.3515618205070496
  batch 400 loss: 1.2865321111679078
  batch 450 loss: 1.335006492137909
  batch 500 loss: 1.3601907205581665
  batch 550 loss: 1.327445409297943
  batch 600 loss: 1.2814533042907714
  batch 650 loss: 1.3860814595222473
  batch 700 loss: 1.3615862703323365
  batch 750 loss: 1.3116493320465088
  batch 800 loss: 1.2951079964637757
  batch 850 loss: 1.2759641456604003
  batch 900 loss: 1.2905051136016845
LOSS train 1.29051 valid 1.20802, valid PER 39.39%
EPOCH 5:
  batch 50 loss: 1.2887798190116881
  batch 100 loss: 1.2795955896377564
  batch 150 loss: 1.3019030785560608
  batch 200 loss: 1.3156988191604615
  batch 250 loss: 1.2488492155075073
  batch 300 loss: 1.2641671216487884
  batch 350 loss: 1.2399015510082245
  batch 400 loss: 1.2498360228538514
  batch 450 loss: 1.2285530626773835
  batch 500 loss: 1.2282822120189667
  batch 550 loss: 1.2656581091880799
  batch 600 loss: 1.3019195628166198
  batch 650 loss: 1.241204936504364
  batch 700 loss: 1.3015095782279968
  batch 750 loss: 1.2681409120559692
  batch 800 loss: 1.2802750658988953
  batch 850 loss: 1.2655373334884643
  batch 900 loss: 1.2413886928558349
LOSS train 1.24139 valid 1.16751, valid PER 38.22%
EPOCH 6:
  batch 50 loss: 1.2187057614326477
  batch 100 loss: 1.2099298977851867
  batch 150 loss: 1.2179563164710998
  batch 200 loss: 1.1699439907073974
  batch 250 loss: 1.2042939865589142
  batch 300 loss: 1.2229070353507996
  batch 350 loss: 1.2323190808296203
  batch 400 loss: 1.177541856765747
  batch 450 loss: 1.2289959144592286
  batch 500 loss: 1.1697993397712707
  batch 550 loss: 1.2241461539268494
  batch 600 loss: 1.2236249959468841
  batch 650 loss: 1.2052068424224853
  batch 700 loss: 1.1723146414756775
  batch 750 loss: 1.2478904187679292
  batch 800 loss: 1.2165141034126281
  batch 850 loss: 1.2447234892845154
  batch 900 loss: 1.2483528232574463
LOSS train 1.24835 valid 1.15975, valid PER 38.35%
EPOCH 7:
  batch 50 loss: 1.1648374104499817
  batch 100 loss: 1.2601890575885772
  batch 150 loss: 1.1778545606136321
  batch 200 loss: 1.1700600183010101
  batch 250 loss: 1.1994117379188538
  batch 300 loss: 1.1710189402103424
  batch 350 loss: 1.199521939754486
  batch 400 loss: 1.1584739017486572
  batch 450 loss: 1.1673373293876648
  batch 500 loss: 1.1665449559688568
  batch 550 loss: 1.132721016407013
  batch 600 loss: 1.1468279922008515
  batch 650 loss: 1.1203519737720489
  batch 700 loss: 1.1758452880382537
  batch 750 loss: 1.1665532934665679
  batch 800 loss: 1.127669632434845
  batch 850 loss: 1.1517456924915315
  batch 900 loss: 1.1392060565948485
LOSS train 1.13921 valid 1.10249, valid PER 36.44%
EPOCH 8:
  batch 50 loss: 1.1287588691711425
  batch 100 loss: 1.096752427816391
  batch 150 loss: 1.1446634685993196
  batch 200 loss: 1.184224624633789
  batch 250 loss: 1.127332626581192
  batch 300 loss: 1.122629166841507
  batch 350 loss: 1.1519688224792481
  batch 400 loss: 1.1478658533096313
  batch 450 loss: 1.2112974941730499
  batch 500 loss: 1.1774071156978607
  batch 550 loss: 1.1549393773078918
  batch 600 loss: 1.1080682599544525
  batch 650 loss: 1.1154340660572053
  batch 700 loss: 1.1374421179294587
  batch 750 loss: 1.1523134732246398
  batch 800 loss: 1.1593508768081664
  batch 850 loss: 1.1045217430591583
  batch 900 loss: 1.1357311344146728
LOSS train 1.13573 valid 1.07117, valid PER 34.89%
EPOCH 9:
  batch 50 loss: 1.1188812673091888
  batch 100 loss: 1.082742145061493
  batch 150 loss: 1.0927278804779053
  batch 200 loss: 1.0883062613010406
  batch 250 loss: 1.085672287940979
  batch 300 loss: 1.1084221851825715
  batch 350 loss: 1.100771347284317
  batch 400 loss: 1.1170462715625762
  batch 450 loss: 1.1238582146167755
  batch 500 loss: 1.0897640240192414
  batch 550 loss: 1.1085670399665832
  batch 600 loss: 1.148787215948105
  batch 650 loss: 1.1148262584209443
  batch 700 loss: 1.1094443440437316
  batch 750 loss: 1.1286232912540435
  batch 800 loss: 1.1237615418434144
  batch 850 loss: 1.1366638767719268
  batch 900 loss: 1.1051350021362305
LOSS train 1.10514 valid 1.03842, valid PER 34.11%
EPOCH 10:
  batch 50 loss: 1.071649215221405
  batch 100 loss: 1.0909306275844575
  batch 150 loss: 1.149966914653778
  batch 200 loss: 1.067747197151184
  batch 250 loss: 1.0485702538490296
  batch 300 loss: 1.0591422629356384
  batch 350 loss: 1.0815050852298738
  batch 400 loss: 1.0398710262775421
  batch 450 loss: 1.0382776522636414
  batch 500 loss: 1.0589563179016113
  batch 550 loss: 1.0735820937156677
  batch 600 loss: 1.0411848711967469
  batch 650 loss: 1.0882823753356934
  batch 700 loss: 1.1346285724639893
  batch 750 loss: 1.1298401725292206
  batch 800 loss: 1.0885665476322175
  batch 850 loss: 1.066841061115265
  batch 900 loss: 1.0542283535003663
LOSS train 1.05423 valid 1.06167, valid PER 34.99%
EPOCH 11:
  batch 50 loss: 1.1008762693405152
  batch 100 loss: 1.028004034757614
  batch 150 loss: 1.0507490563392639
  batch 200 loss: 1.036594841480255
  batch 250 loss: 1.024060230255127
  batch 300 loss: 1.0292814135551454
  batch 350 loss: 1.081945720911026
  batch 400 loss: 1.0415250885486602
  batch 450 loss: 1.0698495376110078
  batch 500 loss: 1.0687904238700867
  batch 550 loss: 1.0708664286136627
  batch 600 loss: 1.048808319568634
  batch 650 loss: 1.0550505805015564
  batch 700 loss: 1.1443313312530519
  batch 750 loss: 1.059768133163452
  batch 800 loss: 1.0701580023765564
  batch 850 loss: 1.104666918516159
  batch 900 loss: 1.1097085094451904
LOSS train 1.10971 valid 1.03049, valid PER 32.59%
EPOCH 12:
  batch 50 loss: 1.0091376292705536
  batch 100 loss: 1.0212901508808137
  batch 150 loss: 1.047525109052658
  batch 200 loss: 1.0561104202270508
  batch 250 loss: 1.0336030662059783
  batch 300 loss: 1.0694388842582703
  batch 350 loss: 1.0504280853271484
  batch 400 loss: 1.0938026094436646
  batch 450 loss: 1.0790941691398621
  batch 500 loss: 1.0937271130084991
  batch 550 loss: 1.0754599070549011
  batch 600 loss: 1.0781818664073943
  batch 650 loss: 1.065687848329544
  batch 700 loss: 1.051849045753479
  batch 750 loss: 1.0790586829185487
  batch 800 loss: 1.0096902334690094
  batch 850 loss: 1.0433327996730803
  batch 900 loss: 1.0436768293380738
LOSS train 1.04368 valid 1.03374, valid PER 33.68%
EPOCH 13:
  batch 50 loss: 0.9949916529655457
  batch 100 loss: 1.0018100070953369
  batch 150 loss: 1.0362295019626617
  batch 200 loss: 0.994003586769104
  batch 250 loss: 1.0158863973617553
  batch 300 loss: 1.0651632511615754
  batch 350 loss: 1.0402240586280822
  batch 400 loss: 1.0385736203193665
  batch 450 loss: 1.0680403864383698
  batch 500 loss: 1.0272871577739715
  batch 550 loss: 1.0925559675693512
  batch 600 loss: 1.042606760263443
  batch 650 loss: 1.0322315382957459
  batch 700 loss: 1.0884434533119203
  batch 750 loss: 1.0367106080055237
  batch 800 loss: 1.0451893389225007
  batch 850 loss: 1.0317103517055513
  batch 900 loss: 1.0557612478733063
LOSS train 1.05576 valid 0.99300, valid PER 31.90%
EPOCH 14:
  batch 50 loss: 1.002420995235443
  batch 100 loss: 0.9959794473648071
  batch 150 loss: 1.0095255625247956
  batch 200 loss: 1.00223091006279
  batch 250 loss: 0.9969851493835449
  batch 300 loss: 0.9910829603672028
  batch 350 loss: 0.9930094623565674
  batch 400 loss: 1.0576781034469604
  batch 450 loss: 1.0330712270736695
  batch 500 loss: 1.0233454024791717
  batch 550 loss: 1.0320863282680512
  batch 600 loss: 0.9996340596675872
  batch 650 loss: 1.0309988725185395
  batch 700 loss: 1.0958004522323608
  batch 750 loss: 1.0739728009700775
  batch 800 loss: 1.0671464204788208
  batch 850 loss: 1.0554559743404388
  batch 900 loss: 1.0427136051654815
LOSS train 1.04271 valid 1.00570, valid PER 33.35%
EPOCH 15:
  batch 50 loss: 0.9538852560520172
  batch 100 loss: 0.9973388743400574
  batch 150 loss: 0.9734111964702606
  batch 200 loss: 1.0316150414943694
  batch 250 loss: 1.033403537273407
  batch 300 loss: 1.0108851408958435
  batch 350 loss: 0.9877612853050232
  batch 400 loss: 1.0126989674568176
  batch 450 loss: 1.0098291504383088
  batch 500 loss: 0.9989365553855896
  batch 550 loss: 1.0501124453544617
  batch 600 loss: 1.038569349050522
  batch 650 loss: 0.9736881387233735
  batch 700 loss: 0.9830382704734802
  batch 750 loss: 1.0152224445343017
  batch 800 loss: 1.0115038025379182
  batch 850 loss: 0.9825008368492126
  batch 900 loss: 0.9790216374397278
LOSS train 0.97902 valid 1.01629, valid PER 33.23%
EPOCH 16:
  batch 50 loss: 0.966798894405365
  batch 100 loss: 0.987147421836853
  batch 150 loss: 0.9922314596176147
  batch 200 loss: 0.986201844215393
  batch 250 loss: 1.0138460862636567
  batch 300 loss: 1.0651725077629088
  batch 350 loss: 1.022965143918991
  batch 400 loss: 1.000496780872345
  batch 450 loss: 1.0170773720741273
  batch 500 loss: 0.9856949460506439
  batch 550 loss: 0.9806881999969482
  batch 600 loss: 0.9914247286319733
  batch 650 loss: 1.0144880568981172
  batch 700 loss: 0.9829098653793334
  batch 750 loss: 0.9983592796325683
  batch 800 loss: 0.9899110984802246
  batch 850 loss: 0.9872316133975982
  batch 900 loss: 1.1130199491977693
LOSS train 1.11302 valid 1.14246, valid PER 35.98%
EPOCH 17:
  batch 50 loss: 1.098557130098343
  batch 100 loss: 1.0338442742824554
  batch 150 loss: 1.0769608438014984
  batch 200 loss: 1.014662710428238
  batch 250 loss: 1.018972270488739
  batch 300 loss: 1.0340891253948212
  batch 350 loss: 1.054963676929474
  batch 400 loss: 1.6170680129528046
  batch 450 loss: 1.744390013217926
  batch 500 loss: 1.4901953887939454
  batch 550 loss: 1.358308160305023
  batch 600 loss: 1.3585417699813842
  batch 650 loss: 1.2867056584358216
  batch 700 loss: 1.310414274930954
  batch 750 loss: 1.2143043673038483
  batch 800 loss: 1.2792525434494018
  batch 850 loss: 1.231882827281952
  batch 900 loss: 1.2532481729984284
LOSS train 1.25325 valid 1.18738, valid PER 37.36%
EPOCH 18:
  batch 50 loss: 1.2131154441833496
  batch 100 loss: 1.1783634543418884
  batch 150 loss: 1.2066129207611085
  batch 200 loss: 1.1816325271129609
  batch 250 loss: 1.142494764328003
  batch 300 loss: 1.1332143366336822
  batch 350 loss: 1.126253981590271
  batch 400 loss: 1.2063721644878387
  batch 450 loss: 1.2158577060699463
  batch 500 loss: 1.1834960317611694
  batch 550 loss: 1.2022583949565888
  batch 600 loss: 1.1838266885280608
  batch 650 loss: 1.1281079733371735
  batch 700 loss: 1.1277913665771484
  batch 750 loss: 1.1685646760463715
  batch 800 loss: 1.1506712675094604
  batch 850 loss: 1.1287948608398437
  batch 900 loss: 1.123551470041275
LOSS train 1.12355 valid 1.09227, valid PER 34.83%
EPOCH 19:
  batch 50 loss: 1.1189311850070953
  batch 100 loss: 1.1252521407604217
  batch 150 loss: 1.1072345674037933
  batch 200 loss: 1.1087545824050904
  batch 250 loss: 1.1710559844970703
  batch 300 loss: 1.1644559717178344
  batch 350 loss: 1.1747922015190124
  batch 400 loss: 1.118343127965927
  batch 450 loss: 1.094633811712265
  batch 500 loss: 1.0866735517978667
  batch 550 loss: 1.1445782041549684
  batch 600 loss: 1.128815118074417
  batch 650 loss: 1.1232654082775115
  batch 700 loss: 1.1504021167755127
  batch 750 loss: 1.0871249341964722
  batch 800 loss: 1.0654863440990447
  batch 850 loss: 1.1353340423107148
  batch 900 loss: 1.0809159314632415
LOSS train 1.08092 valid 1.09862, valid PER 35.24%
EPOCH 20:
  batch 50 loss: 1.030715615749359
  batch 100 loss: 1.098197032213211
  batch 150 loss: 1.1076557087898253
  batch 200 loss: 1.0971074545383452
  batch 250 loss: 1.1043598175048828
  batch 300 loss: 1.0953089201450348
  batch 350 loss: 1.0737368655204773
  batch 400 loss: 1.0650785338878632
  batch 450 loss: 1.1024949657917023
  batch 500 loss: 1.109099758863449
  batch 550 loss: 1.1054434633255006
  batch 600 loss: 1.0611763453483583
  batch 650 loss: 1.129898647069931
  batch 700 loss: 1.1029260540008545
  batch 750 loss: 1.088381608724594
  batch 800 loss: 1.1131312441825867
  batch 850 loss: 1.1214882349967956
  batch 900 loss: 1.0846579730510713
LOSS train 1.08466 valid 1.08848, valid PER 34.74%
Training finished in 3.0 minutes.
Model saved to checkpoints/20230115_205448/model_13
Loading model from checkpoints/20230115_205448/model_13
SUB: 15.38%, DEL: 14.28%, INS: 1.64%, COR: 70.35%, PER: 31.29%
