Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.7)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.202065420150757
  batch 100 loss: 3.134902377128601
  batch 150 loss: 3.041718044281006
  batch 200 loss: 2.946983118057251
  batch 250 loss: 2.900897421836853
  batch 300 loss: 2.7300866794586183
  batch 350 loss: 2.6108944416046143
  batch 400 loss: 2.54733446598053
  batch 450 loss: 2.5224734354019165
  batch 500 loss: 2.450710802078247
  batch 550 loss: 2.448089325428009
  batch 600 loss: 2.3102384638786315
  batch 650 loss: 2.2024088764190672
  batch 700 loss: 2.179801754951477
  batch 750 loss: 2.1392629194259642
  batch 800 loss: 2.1123566961288454
  batch 850 loss: 2.067237980365753
  batch 900 loss: 2.0436397790908813
LOSS train 2.04364 valid 2.01649, valid PER 77.53%
EPOCH 2:
  batch 50 loss: 2.04054048538208
  batch 100 loss: 2.0041730213165283
  batch 150 loss: 1.916764223575592
  batch 200 loss: 1.9441699242591859
  batch 250 loss: 1.9339731287956239
  batch 300 loss: 1.910191969871521
  batch 350 loss: 1.8891646122932435
  batch 400 loss: 1.8667146611213683
  batch 450 loss: 1.8768864583969116
  batch 500 loss: 1.863750891685486
  batch 550 loss: 1.843559832572937
  batch 600 loss: 1.8489419579505921
  batch 650 loss: 1.7819065618515015
  batch 700 loss: 1.8178369355201722
  batch 750 loss: 1.798687334060669
  batch 800 loss: 1.7438431262969971
  batch 850 loss: 1.7809225177764894
  batch 900 loss: 1.7415988183021545
LOSS train 1.74160 valid 1.66269, valid PER 66.25%
EPOCH 3:
  batch 50 loss: 1.6879013872146607
  batch 100 loss: 1.7523803520202637
  batch 150 loss: 1.7327115869522094
  batch 200 loss: 1.67927104473114
  batch 250 loss: 1.7270482754707337
  batch 300 loss: 1.7121957802772523
  batch 350 loss: 1.7155549311637879
  batch 400 loss: 1.6812189149856567
  batch 450 loss: 1.667584059238434
  batch 500 loss: 1.651660532951355
  batch 550 loss: 1.6626148176193238
  batch 600 loss: 1.6123982167243958
  batch 650 loss: 1.63015629529953
  batch 700 loss: 1.617595031261444
  batch 750 loss: 1.6783087944984436
  batch 800 loss: 1.6458024311065673
  batch 850 loss: 1.6313840436935425
  batch 900 loss: 1.6222512531280517
LOSS train 1.62225 valid 1.53136, valid PER 59.89%
EPOCH 4:
  batch 50 loss: 1.6245786404609681
  batch 100 loss: 1.5564719390869142
  batch 150 loss: 1.5941827607154846
  batch 200 loss: 1.569653744697571
  batch 250 loss: 1.6243223524093628
  batch 300 loss: 1.5669944024085998
  batch 350 loss: 1.5695553183555604
  batch 400 loss: 1.5391936683654786
  batch 450 loss: 1.5649770879745484
  batch 500 loss: 1.5927042698860168
  batch 550 loss: 1.5463815999031068
  batch 600 loss: 1.5102382755279542
  batch 650 loss: 1.56654794216156
  batch 700 loss: 1.6043234157562256
  batch 750 loss: 1.5439068675041199
  batch 800 loss: 1.5482437920570373
  batch 850 loss: 1.505941288471222
  batch 900 loss: 1.5244928574562073
LOSS train 1.52449 valid 1.45521, valid PER 54.01%
EPOCH 5:
  batch 50 loss: 1.5230125570297242
  batch 100 loss: 1.5077217173576356
  batch 150 loss: 1.528002860546112
  batch 200 loss: 1.5416145300865174
  batch 250 loss: 1.4907690382003784
  batch 300 loss: 1.5106453132629394
  batch 350 loss: 1.4759115147590638
  batch 400 loss: 1.4670321202278138
  batch 450 loss: 1.457234034538269
  batch 500 loss: 1.4319714212417602
  batch 550 loss: 1.4927437114715576
  batch 600 loss: 1.4963715147972108
  batch 650 loss: 1.4990886092185973
  batch 700 loss: 1.466626307964325
  batch 750 loss: 1.4479684937000274
  batch 800 loss: 1.5149884605407715
  batch 850 loss: 1.471764736175537
  batch 900 loss: 1.4523185682296753
LOSS train 1.45232 valid 1.33633, valid PER 48.79%
EPOCH 6:
  batch 50 loss: 1.4333820128440857
  batch 100 loss: 1.4633868408203126
  batch 150 loss: 1.448605489730835
  batch 200 loss: 1.4005309391021727
  batch 250 loss: 1.4405804586410522
  batch 300 loss: 1.4504788303375244
  batch 350 loss: 1.4430308866500854
  batch 400 loss: 1.4100465440750123
  batch 450 loss: 1.4481234312057496
  batch 500 loss: 1.4020409440994264
  batch 550 loss: 1.4323670029640199
  batch 600 loss: 1.422381148338318
  batch 650 loss: 1.3921001386642455
  batch 700 loss: 1.3831227850914
  batch 750 loss: 1.4350650215148926
  batch 800 loss: 1.4132151317596435
  batch 850 loss: 1.4334985041618347
  batch 900 loss: 1.4318499493598937
LOSS train 1.43185 valid 1.25862, valid PER 43.05%
EPOCH 7:
  batch 50 loss: 1.3793281364440917
  batch 100 loss: 1.4297913360595702
  batch 150 loss: 1.3685899090766906
  batch 200 loss: 1.367645869255066
  batch 250 loss: 1.4127331471443176
  batch 300 loss: 1.4018024134635925
  batch 350 loss: 1.435963180065155
  batch 400 loss: 1.3735040307044983
  batch 450 loss: 1.3747233283519744
  batch 500 loss: 1.3829631638526916
  batch 550 loss: 1.34768816947937
  batch 600 loss: 1.3586115503311158
  batch 650 loss: 1.3444991660118104
  batch 700 loss: 1.4079085755348206
  batch 750 loss: 1.3785389614105226
  batch 800 loss: 1.3468357586860658
  batch 850 loss: 1.3837020206451416
  batch 900 loss: 1.340954728126526
LOSS train 1.34095 valid 1.23527, valid PER 41.61%
EPOCH 8:
  batch 50 loss: 1.378696117401123
  batch 100 loss: 1.3163141703605652
  batch 150 loss: 1.357032492160797
  batch 200 loss: 1.364859138727188
  batch 250 loss: 1.3151241779327392
  batch 300 loss: 1.3060481429100037
  batch 350 loss: 1.3148205518722533
  batch 400 loss: 1.3537552738189698
  batch 450 loss: 1.375143973827362
  batch 500 loss: 1.3573335003852844
  batch 550 loss: 1.3350069189071656
  batch 600 loss: 1.3030123353004455
  batch 650 loss: 1.3258000111579895
  batch 700 loss: 1.3451624917984009
  batch 750 loss: 1.3494478273391723
  batch 800 loss: 1.3624711871147155
  batch 850 loss: 1.3337828874588014
  batch 900 loss: 1.349775311946869
LOSS train 1.34978 valid 1.20542, valid PER 39.99%
EPOCH 9:
  batch 50 loss: 1.3343400406837462
  batch 100 loss: 1.3312028908729554
  batch 150 loss: 1.3699687838554382
  batch 200 loss: 1.3105359554290772
  batch 250 loss: 1.278963862657547
  batch 300 loss: 1.3610315442085266
  batch 350 loss: 1.3050756692886352
  batch 400 loss: 1.3319602417945862
  batch 450 loss: 1.3148845875263213
  batch 500 loss: 1.2979396533966066
  batch 550 loss: 1.3293307733535766
  batch 600 loss: 1.3837284469604492
  batch 650 loss: 1.3456364321708678
  batch 700 loss: 1.3118040192127227
  batch 750 loss: 1.2965159559249877
  batch 800 loss: 1.3350034415721894
  batch 850 loss: 1.3611656999588013
  batch 900 loss: 1.3018131375312805
LOSS train 1.30181 valid 1.15023, valid PER 38.57%
EPOCH 10:
  batch 50 loss: 1.2836561512947082
  batch 100 loss: 1.2994911241531373
  batch 150 loss: 1.3290875363349914
  batch 200 loss: 1.3571656441688538
  batch 250 loss: 1.314736955165863
  batch 300 loss: 1.29664790391922
  batch 350 loss: 1.292788861989975
  batch 400 loss: 1.2715373730659485
  batch 450 loss: 1.2891218221187593
  batch 500 loss: 1.2897620129585265
  batch 550 loss: 1.2958399331569672
  batch 600 loss: 1.2746547174453735
  batch 650 loss: 1.2963731503486633
  batch 700 loss: 1.324163646697998
  batch 750 loss: 1.3314225029945375
  batch 800 loss: 1.3334787726402282
  batch 850 loss: 1.3161012852191925
  batch 900 loss: 1.3243064975738525
LOSS train 1.32431 valid 1.17485, valid PER 39.37%
EPOCH 11:
  batch 50 loss: 1.2821837067604065
  batch 100 loss: 1.2558843207359314
  batch 150 loss: 1.253319926261902
  batch 200 loss: 1.2314434325695038
  batch 250 loss: 1.2447234976291657
  batch 300 loss: 1.30590003490448
  batch 350 loss: 1.287211034297943
  batch 400 loss: 1.282222706079483
  batch 450 loss: 1.2949768233299255
  batch 500 loss: 1.3031763279438018
  batch 550 loss: 1.2900476229190827
  batch 600 loss: 1.3043563199043273
  batch 650 loss: 1.3070768189430237
  batch 700 loss: 1.3858916997909545
  batch 750 loss: 1.2370839667320253
  batch 800 loss: 1.2810855555534362
  batch 850 loss: 1.270893658399582
  batch 900 loss: 1.3002760076522828
LOSS train 1.30028 valid 1.13275, valid PER 38.21%
EPOCH 12:
  batch 50 loss: 1.2363323736190797
  batch 100 loss: 1.219729127883911
  batch 150 loss: 1.25187344789505
  batch 200 loss: 1.2816946744918822
  batch 250 loss: 1.2814261329174041
  batch 300 loss: 1.3078860592842103
  batch 350 loss: 1.2548013019561768
  batch 400 loss: 1.257365083694458
  batch 450 loss: 1.234174121618271
  batch 500 loss: 1.252155417203903
  batch 550 loss: 1.282401566505432
  batch 600 loss: 1.2610437989234924
  batch 650 loss: 1.2570047855377198
  batch 700 loss: 1.2089102172851562
  batch 750 loss: 1.294447830915451
  batch 800 loss: 1.2345355665683746
  batch 850 loss: 1.2447781264781952
  batch 900 loss: 1.2572947883605956
LOSS train 1.25729 valid 1.16355, valid PER 37.73%
EPOCH 13:
  batch 50 loss: 1.2097917103767395
  batch 100 loss: 1.2371726369857787
  batch 150 loss: 1.2880366230010987
  batch 200 loss: 1.2109687697887421
  batch 250 loss: 1.213153018951416
  batch 300 loss: 1.263809598684311
  batch 350 loss: 1.2154154992103576
  batch 400 loss: 1.2659687566757203
  batch 450 loss: 1.260906536579132
  batch 500 loss: 1.2293069088459014
  batch 550 loss: 1.2875296068191528
  batch 600 loss: 1.2519724583625793
  batch 650 loss: 1.2551729953289033
  batch 700 loss: 1.2895932626724242
  batch 750 loss: 1.2300426709651946
  batch 800 loss: 1.2447810006141662
  batch 850 loss: 1.2508720338344574
  batch 900 loss: 1.2105908393859863
LOSS train 1.21059 valid 1.08151, valid PER 36.28%
EPOCH 14:
  batch 50 loss: 1.21442378282547
  batch 100 loss: 1.1982849109172822
  batch 150 loss: 1.213403843641281
  batch 200 loss: 1.2399997663497926
  batch 250 loss: 1.2030768024921417
  batch 300 loss: 1.1859191048145294
  batch 350 loss: 1.21609601020813
  batch 400 loss: 1.2530415344238282
  batch 450 loss: 1.2024568819999695
  batch 500 loss: 1.206697564125061
  batch 550 loss: 1.2271777594089508
  batch 600 loss: 1.2115497064590455
  batch 650 loss: 1.2452658438682556
  batch 700 loss: 1.2294842576980591
  batch 750 loss: 1.2106004285812377
  batch 800 loss: 1.2371648681163787
  batch 850 loss: 1.2515412604808807
  batch 900 loss: 1.2178723764419557
LOSS train 1.21787 valid 1.11521, valid PER 37.00%
EPOCH 15:
  batch 50 loss: 1.1878622627258302
  batch 100 loss: 1.2101686573028565
  batch 150 loss: 1.185743293762207
  batch 200 loss: 1.2342908835411073
  batch 250 loss: 1.237453236579895
  batch 300 loss: 1.2066888332366943
  batch 350 loss: 1.2145245468616486
  batch 400 loss: 1.251589070558548
  batch 450 loss: 1.2176046586036682
  batch 500 loss: 1.1917549896240234
  batch 550 loss: 1.2403187239170075
  batch 600 loss: 1.2584126162528992
  batch 650 loss: 1.1894957423210144
  batch 700 loss: 1.2021481430530547
  batch 750 loss: 1.236143352985382
  batch 800 loss: 1.2092785739898682
  batch 850 loss: 1.189702558517456
  batch 900 loss: 1.1505111956596374
LOSS train 1.15051 valid 1.17865, valid PER 39.44%
EPOCH 16:
  batch 50 loss: 1.2336605179309845
  batch 100 loss: 1.1825315761566162
  batch 150 loss: 1.2124168539047242
  batch 200 loss: 1.229464304447174
  batch 250 loss: 1.1811880731582642
  batch 300 loss: 1.2357591927051543
  batch 350 loss: 1.226142580509186
  batch 400 loss: 1.1932357013225556
  batch 450 loss: 1.2066232025623322
  batch 500 loss: 1.2076469826698304
  batch 550 loss: 1.1837587881088256
  batch 600 loss: 1.236606878042221
  batch 650 loss: 1.2057962429523468
  batch 700 loss: 1.1702558946609498
  batch 750 loss: 1.1791066575050353
  batch 800 loss: 1.181034277677536
  batch 850 loss: 1.184516305923462
  batch 900 loss: 1.185066739320755
LOSS train 1.18507 valid 1.09683, valid PER 35.83%
EPOCH 17:
  batch 50 loss: 1.1844156849384309
  batch 100 loss: 1.1373558926582337
  batch 150 loss: 1.2088271462917328
  batch 200 loss: 1.1672628486156464
  batch 250 loss: 1.2217154204845428
  batch 300 loss: 1.223859544992447
  batch 350 loss: 1.2308770513534546
  batch 400 loss: 1.232091258764267
  batch 450 loss: 1.2144492661952973
  batch 500 loss: 1.1903749263286592
  batch 550 loss: 1.1943135166168213
  batch 600 loss: 1.2174021792411804
  batch 650 loss: 1.2030844330787658
  batch 700 loss: 1.2141573071479796
  batch 750 loss: 1.186181797981262
  batch 800 loss: 1.2604654598236085
  batch 850 loss: 1.1912503123283387
  batch 900 loss: 1.2027277874946594
LOSS train 1.20273 valid 1.05826, valid PER 35.06%
EPOCH 18:
  batch 50 loss: 1.169149500131607
  batch 100 loss: 1.2175215089321136
  batch 150 loss: 1.2774749493598938
  batch 200 loss: 1.2046458256244659
  batch 250 loss: 1.1632690346240997
  batch 300 loss: 1.2108019602298736
  batch 350 loss: 1.1880627107620239
  batch 400 loss: 1.1813955628871917
  batch 450 loss: 1.1861514592170714
  batch 500 loss: 1.1843323838710784
  batch 550 loss: 1.2306864309310912
  batch 600 loss: 1.2024882090091706
  batch 650 loss: 1.1574752736091614
  batch 700 loss: 1.1606894254684448
  batch 750 loss: 1.1596319425106048
  batch 800 loss: 1.2077694034576416
  batch 850 loss: 1.1769805836677552
  batch 900 loss: 1.170693119764328
LOSS train 1.17069 valid 1.07847, valid PER 36.30%
EPOCH 19:
  batch 50 loss: 1.1564074981212615
  batch 100 loss: 1.1679601216316222
  batch 150 loss: 1.1542961883544922
  batch 200 loss: 1.1333964562416077
  batch 250 loss: 1.2079818880558013
  batch 300 loss: 1.1713731479644776
  batch 350 loss: 1.1627592051029205
  batch 400 loss: 1.1705774366855621
  batch 450 loss: 1.1324744856357574
  batch 500 loss: 1.1472810423374176
  batch 550 loss: 1.244416069984436
  batch 600 loss: 1.2167632818222045
  batch 650 loss: 1.19873028755188
  batch 700 loss: 1.22175821185112
  batch 750 loss: 1.1791316843032837
  batch 800 loss: 1.1361543154716491
  batch 850 loss: 1.1877399635314942
  batch 900 loss: 1.1662228190898896
LOSS train 1.16622 valid 1.04861, valid PER 35.16%
EPOCH 20:
  batch 50 loss: 1.132396491765976
  batch 100 loss: 1.1610692381858825
  batch 150 loss: 1.143820163011551
  batch 200 loss: 1.1435759496688842
  batch 250 loss: 1.1738200736045838
  batch 300 loss: 1.1318954813480377
  batch 350 loss: 1.13986554980278
  batch 400 loss: 1.140009593963623
  batch 450 loss: 1.133518728017807
  batch 500 loss: 1.1504212236404419
  batch 550 loss: 1.1700192975997925
  batch 600 loss: 1.1254687786102295
  batch 650 loss: 1.1702222263813018
  batch 700 loss: 1.1354250764846803
  batch 750 loss: 1.1498524570465087
  batch 800 loss: 1.2069980120658874
  batch 850 loss: 1.2238810205459594
  batch 900 loss: 1.1970047104358672
LOSS train 1.19700 valid 1.10376, valid PER 36.83%
Training finished in 3.0 minutes.
Model saved to checkpoints/20230115_205521/model_19
Loading model from checkpoints/20230115_205521/model_19
SUB: 14.89%, DEL: 20.28%, INS: 0.76%, COR: 64.83%, PER: 35.94%
