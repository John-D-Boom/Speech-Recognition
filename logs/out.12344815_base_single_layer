Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20)
Total number of model parameters is 166952
EPOCH 1:
  batch 50 loss: 4.224976525306702
  batch 100 loss: 3.172708749771118
  batch 150 loss: 3.0564518690109255
  batch 200 loss: 2.9581443214416505
  batch 250 loss: 2.902849917411804
  batch 300 loss: 2.7074605083465575
  batch 350 loss: 2.5655258893966675
  batch 400 loss: 2.5313026666641236
  batch 450 loss: 2.4574816608428955
  batch 500 loss: 2.304477689266205
  batch 550 loss: 2.2261327791213987
  batch 600 loss: 2.1196058654785155
  batch 650 loss: 2.0002387285232546
  batch 700 loss: 2.016450731754303
  batch 750 loss: 1.9293442559242249
  batch 800 loss: 1.9012457728385925
  batch 850 loss: 1.8251132917404176
  batch 900 loss: 1.8268261861801147
LOSS train 1.82683 valid 1.75888, valid PER 65.92%
EPOCH 2:
  batch 50 loss: 1.7627282047271728
  batch 100 loss: 1.7012803435325623
  batch 150 loss: 1.6897327899932861
  batch 200 loss: 1.6725684738159179
  batch 250 loss: 1.6907594776153565
  batch 300 loss: 1.6307489991188049
  batch 350 loss: 1.5374812722206115
  batch 400 loss: 1.5522774529457093
  batch 450 loss: 1.503690550327301
  batch 500 loss: 1.5155977177619935
  batch 550 loss: 1.5313241147994996
  batch 600 loss: 1.4764313077926636
  batch 650 loss: 1.4845022797584533
  batch 700 loss: 1.4548959779739379
  batch 750 loss: 1.4126632928848266
  batch 800 loss: 1.3632104110717773
  batch 850 loss: 1.3760339283943177
  batch 900 loss: 1.3849403691291808
LOSS train 1.38494 valid 1.38333, valid PER 42.39%
EPOCH 3:
  batch 50 loss: 1.3408462810516357
  batch 100 loss: 1.3232884478569031
  batch 150 loss: 1.3227037262916566
  batch 200 loss: 1.276956732273102
  batch 250 loss: 1.259306375980377
  batch 300 loss: 1.2629731380939484
  batch 350 loss: 1.3022338461875915
  batch 400 loss: 1.3009687769412994
  batch 450 loss: 1.2765477836132049
  batch 500 loss: 1.263996354341507
  batch 550 loss: 1.2720719647407532
  batch 600 loss: 1.2284489059448243
  batch 650 loss: 1.2528347778320312
  batch 700 loss: 1.2676801955699921
  batch 750 loss: 1.355799332857132
  batch 800 loss: 1.2297047865390778
  batch 850 loss: 1.2656416869163514
  batch 900 loss: 1.2220037364959717
LOSS train 1.22200 valid 1.26860, valid PER 38.03%
EPOCH 4:
  batch 50 loss: 1.164448881149292
  batch 100 loss: 1.1973807871341706
  batch 150 loss: 1.1379851996898651
  batch 200 loss: 1.166702046394348
  batch 250 loss: 1.1795728302001953
  batch 300 loss: 1.183220136165619
  batch 350 loss: 1.1153332352638246
  batch 400 loss: 1.1625684595108032
  batch 450 loss: 1.146584805250168
  batch 500 loss: 1.1188552606105804
  batch 550 loss: 1.1547638261318207
  batch 600 loss: 1.1707267761230469
  batch 650 loss: 1.1477484464645387
  batch 700 loss: 1.1201611936092377
  batch 750 loss: 1.1069206476211548
  batch 800 loss: 1.0867343473434448
  batch 850 loss: 1.1198825788497926
  batch 900 loss: 1.1654821383953093
LOSS train 1.16548 valid 1.12574, valid PER 35.54%
EPOCH 5:
  batch 50 loss: 1.0715384376049042
  batch 100 loss: 1.08414852142334
  batch 150 loss: 1.1328472125530242
  batch 200 loss: 1.0670285522937775
  batch 250 loss: 1.0960818934440613
  batch 300 loss: 1.0831762742996216
  batch 350 loss: 1.074140499830246
  batch 400 loss: 1.0787732458114625
  batch 450 loss: 1.0681084501743316
  batch 500 loss: 1.07314408659935
  batch 550 loss: 1.0439300334453583
  batch 600 loss: 1.1111231243610382
  batch 650 loss: 1.0763973939418792
  batch 700 loss: 1.1165310692787171
  batch 750 loss: 1.0367046701908111
  batch 800 loss: 1.0689260959625244
  batch 850 loss: 1.0838852524757385
  batch 900 loss: 1.0826689493656159
LOSS train 1.08267 valid 1.12360, valid PER 34.26%
EPOCH 6:
  batch 50 loss: 1.0561170375347138
  batch 100 loss: 1.0127060723304748
  batch 150 loss: 0.9897660076618194
  batch 200 loss: 0.9952415466308594
  batch 250 loss: 1.0506485521793365
  batch 300 loss: 1.0241683292388917
  batch 350 loss: 1.0174635434150696
  batch 400 loss: 1.0484409511089325
  batch 450 loss: 1.0581549155712127
  batch 500 loss: 1.0409741961956025
  batch 550 loss: 1.0757814240455628
  batch 600 loss: 0.9946078372001648
  batch 650 loss: 1.0308473372459412
  batch 700 loss: 1.0163504445552827
  batch 750 loss: 1.009987658262253
  batch 800 loss: 1.0122442853450775
  batch 850 loss: 0.9896885561943054
  batch 900 loss: 1.0261906969547272
LOSS train 1.02619 valid 1.06975, valid PER 33.64%
EPOCH 7:
  batch 50 loss: 0.9803862571716309
  batch 100 loss: 1.000620014667511
  batch 150 loss: 0.9881802225112915
  batch 200 loss: 0.9645832085609436
  batch 250 loss: 0.9792102110385895
  batch 300 loss: 0.9630809283256531
  batch 350 loss: 0.9655527150630951
  batch 400 loss: 0.9641960442066193
  batch 450 loss: 0.9758342015743255
  batch 500 loss: 0.9519133961200714
  batch 550 loss: 0.9651764869689942
  batch 600 loss: 0.9825862145423889
  batch 650 loss: 0.9544039237499237
  batch 700 loss: 0.9950887846946717
  batch 750 loss: 0.963030595779419
  batch 800 loss: 0.9422463572025299
  batch 850 loss: 0.9767610108852387
  batch 900 loss: 1.0324197244644164
LOSS train 1.03242 valid 1.03956, valid PER 32.80%
EPOCH 8:
  batch 50 loss: 0.943362637758255
  batch 100 loss: 0.9218541634082794
  batch 150 loss: 0.9416480576992035
  batch 200 loss: 0.92132981300354
  batch 250 loss: 0.9419177627563476
  batch 300 loss: 0.8942352592945099
  batch 350 loss: 0.9560123145580292
  batch 400 loss: 0.9170620763301849
  batch 450 loss: 0.9467603611946106
  batch 500 loss: 0.9778893661499023
  batch 550 loss: 0.9154945766925812
  batch 600 loss: 0.9453739452362061
  batch 650 loss: 0.980288554430008
  batch 700 loss: 0.96692023396492
  batch 750 loss: 0.9627233695983887
  batch 800 loss: 0.9516629004478454
  batch 850 loss: 0.955905921459198
  batch 900 loss: 0.9322454512119294
LOSS train 0.93225 valid 1.00262, valid PER 31.64%
EPOCH 9:
  batch 50 loss: 0.8727202498912812
  batch 100 loss: 0.90524627327919
  batch 150 loss: 0.9124446237087249
  batch 200 loss: 0.8691643166542053
  batch 250 loss: 0.8949249136447907
  batch 300 loss: 0.8997120141983033
  batch 350 loss: 0.9303166854381562
  batch 400 loss: 0.9125413250923157
  batch 450 loss: 0.9139146602153778
  batch 500 loss: 0.8867939710617065
  batch 550 loss: 0.9259573352336884
  batch 600 loss: 0.9744472563266754
  batch 650 loss: 0.9603988039493561
  batch 700 loss: 0.930850168466568
  batch 750 loss: 0.9208873724937439
  batch 800 loss: 0.9462420630455017
  batch 850 loss: 0.9682981622219086
  batch 900 loss: 0.9106159532070159
LOSS train 0.91062 valid 1.01915, valid PER 32.05%
EPOCH 10:
  batch 50 loss: 0.8554001247882843
  batch 100 loss: 0.8744788837432861
  batch 150 loss: 0.9077026271820068
  batch 200 loss: 0.9606230783462525
  batch 250 loss: 0.993123539686203
  batch 300 loss: 0.91666184425354
  batch 350 loss: 0.9529539477825165
  batch 400 loss: 0.8920873773097991
  batch 450 loss: 0.8791540253162384
  batch 500 loss: 0.9305501103401184
  batch 550 loss: 0.9227210688591003
  batch 600 loss: 0.912433979511261
  batch 650 loss: 0.9065648376941681
  batch 700 loss: 0.918055739402771
  batch 750 loss: 0.9190739560127258
  batch 800 loss: 0.976547839641571
  batch 850 loss: 1.0482500839233397
  batch 900 loss: 0.9711185812950134
LOSS train 0.97112 valid 1.06194, valid PER 34.04%
EPOCH 11:
  batch 50 loss: 0.893223580121994
  batch 100 loss: 0.8927974998950958
  batch 150 loss: 0.8893166291713714
  batch 200 loss: 0.9081623101234436
  batch 250 loss: 0.9168921148777008
  batch 300 loss: 0.8961421501636505
  batch 350 loss: 0.8953955793380737
  batch 400 loss: 0.8961175680160522
  batch 450 loss: 0.9082023358345032
  batch 500 loss: 0.8815255689620972
  batch 550 loss: 0.9025715613365173
  batch 600 loss: 0.8821750497817993
  batch 650 loss: 0.9546135044097901
  batch 700 loss: 0.8504299366474152
  batch 750 loss: 0.886803857088089
  batch 800 loss: 0.9129173493385315
  batch 850 loss: 0.9490591931343079
  batch 900 loss: 0.9162445652484894
LOSS train 0.91624 valid 1.00718, valid PER 31.50%
EPOCH 12:
  batch 50 loss: 0.8755887031555176
  batch 100 loss: 0.8485264730453491
  batch 150 loss: 0.8437327349185944
  batch 200 loss: 0.8652423202991486
  batch 250 loss: 0.920669162273407
  batch 300 loss: 0.8742921423912048
  batch 350 loss: 0.8735778820514679
  batch 400 loss: 0.9014549171924591
  batch 450 loss: 0.9203291940689087
  batch 500 loss: 0.8911478304862976
  batch 550 loss: 0.8297863638401032
  batch 600 loss: 0.8819178855419159
  batch 650 loss: 0.929277447462082
  batch 700 loss: 0.9084400701522827
  batch 750 loss: 0.8700697255134583
  batch 800 loss: 0.8728219199180604
  batch 850 loss: 0.9057863175868988
  batch 900 loss: 0.947211264371872
LOSS train 0.94721 valid 1.00768, valid PER 31.36%
EPOCH 13:
  batch 50 loss: 0.8425291049480438
  batch 100 loss: 0.862649816274643
  batch 150 loss: 0.8386836206912994
  batch 200 loss: 0.8587390899658203
  batch 250 loss: 0.8620055866241455
  batch 300 loss: 0.8448356378078461
  batch 350 loss: 0.8490229237079621
  batch 400 loss: 0.8882165288925171
  batch 450 loss: 0.8657288789749146
  batch 500 loss: 0.830836079120636
  batch 550 loss: 0.8708048665523529
  batch 600 loss: 0.8844350636005401
  batch 650 loss: 0.8815606701374054
  batch 700 loss: 0.866954380273819
  batch 750 loss: 0.8153713846206665
  batch 800 loss: 0.8426801335811615
  batch 850 loss: 0.8726175630092621
  batch 900 loss: 0.8656143879890442
LOSS train 0.86561 valid 0.97616, valid PER 30.38%
EPOCH 14:
  batch 50 loss: 0.8097789525985718
  batch 100 loss: 0.8108543026447296
  batch 150 loss: 0.835228087902069
  batch 200 loss: 0.8232567405700684
  batch 250 loss: 0.8193867111206055
  batch 300 loss: 0.9034375262260437
  batch 350 loss: 0.8738331079483033
  batch 400 loss: 0.8517715454101562
  batch 450 loss: 0.8837136876583099
  batch 500 loss: 0.8737440192699433
  batch 550 loss: 0.8604102444648742
  batch 600 loss: 0.8399006140232086
  batch 650 loss: 0.8942526638507843
  batch 700 loss: 0.8846209537982941
  batch 750 loss: 0.8703553569316864
  batch 800 loss: 0.8097627902030945
  batch 850 loss: 0.8637564945220947
  batch 900 loss: 0.8831137096881867
LOSS train 0.88311 valid 0.97939, valid PER 30.25%
EPOCH 15:
  batch 50 loss: 0.8140871584415436
  batch 100 loss: 0.7977866703271865
  batch 150 loss: 0.8103534376621246
  batch 200 loss: 0.8606339621543885
  batch 250 loss: 0.8270242857933044
  batch 300 loss: 0.8278708350658417
  batch 350 loss: 0.8262724626064301
  batch 400 loss: 0.8034395158290863
  batch 450 loss: 0.8168967974185943
  batch 500 loss: 0.7990291357040405
  batch 550 loss: 0.8356598496437073
  batch 600 loss: 0.8590404915809632
  batch 650 loss: 0.9698604464530944
  batch 700 loss: 0.9545781767368317
  batch 750 loss: 0.9456242191791534
  batch 800 loss: 0.9000214886665344
  batch 850 loss: 0.8841489291191101
  batch 900 loss: 0.9371344494819641
LOSS train 0.93713 valid 1.05561, valid PER 32.37%
EPOCH 16:
  batch 50 loss: 0.8989107573032379
  batch 100 loss: 0.8974778485298157
  batch 150 loss: 0.9457435417175293
  batch 200 loss: 0.8786760807037354
  batch 250 loss: 0.8738506925106049
  batch 300 loss: 0.8656425440311432
  batch 350 loss: 0.8843043792247772
  batch 400 loss: 0.8606108093261718
  batch 450 loss: 0.8596271514892578
  batch 500 loss: 0.8172908079624176
  batch 550 loss: 0.8485497546195984
  batch 600 loss: 0.8459738373756409
  batch 650 loss: 0.8556981527805329
  batch 700 loss: 0.8162221121788025
  batch 750 loss: 0.8204004263877869
  batch 800 loss: 0.8697402739524841
  batch 850 loss: 0.9209980511665344
  batch 900 loss: 0.8565435934066773
LOSS train 0.85654 valid 0.99551, valid PER 30.71%
EPOCH 17:
  batch 50 loss: 0.8359032905101776
  batch 100 loss: 0.8288534843921661
  batch 150 loss: 0.8100061631202697
  batch 200 loss: 0.8186803567409515
  batch 250 loss: 0.9439640748500824
  batch 300 loss: 0.9848261380195618
  batch 350 loss: 0.8838878631591797
  batch 400 loss: 0.8910402286052704
  batch 450 loss: 0.875097416639328
  batch 500 loss: 0.821074845790863
  batch 550 loss: 0.8534855079650879
  batch 600 loss: 0.8909908556938171
  batch 650 loss: 0.8335792696475983
  batch 700 loss: 0.8486340820789338
  batch 750 loss: 0.8366391575336456
  batch 800 loss: 0.8605253350734711
  batch 850 loss: 0.8791067862510681
  batch 900 loss: 0.8408006000518798
LOSS train 0.84080 valid 0.98100, valid PER 30.38%
EPOCH 18:
  batch 50 loss: 0.8271978628635407
  batch 100 loss: 0.8553610789775848
  batch 150 loss: 0.9200734186172486
  batch 200 loss: 0.9175441539287568
  batch 250 loss: 0.8611458897590637
  batch 300 loss: 0.8232494115829467
  batch 350 loss: 1.0056421077251434
  batch 400 loss: 0.9613674294948578
  batch 450 loss: 0.950330296754837
  batch 500 loss: 0.9219382512569427
  batch 550 loss: 0.8762755131721497
  batch 600 loss: 0.8609924829006195
  batch 650 loss: 0.8784171199798584
  batch 700 loss: 0.9075390338897705
  batch 750 loss: 0.8637784886360168
  batch 800 loss: 0.871450012922287
  batch 850 loss: 0.8538142478466034
  batch 900 loss: 0.8797364461421967
LOSS train 0.87974 valid 0.99187, valid PER 30.97%
EPOCH 19:
  batch 50 loss: 0.8437509608268737
  batch 100 loss: 0.8301483714580535
  batch 150 loss: 0.8514833545684815
  batch 200 loss: 0.8509772861003876
  batch 250 loss: 0.8522807908058166
  batch 300 loss: 0.845288119316101
  batch 350 loss: 0.8176643753051758
  batch 400 loss: 0.8290052568912506
  batch 450 loss: 0.8321376276016236
  batch 500 loss: 0.8247459602355957
  batch 550 loss: 0.8293086326122284
  batch 600 loss: 0.8184467911720276
  batch 650 loss: 0.8928838884830474
  batch 700 loss: 0.8076291334629059
  batch 750 loss: 0.851296364068985
  batch 800 loss: 0.8684457123279572
  batch 850 loss: 0.8553389680385589
  batch 900 loss: 0.816402696967125
LOSS train 0.81640 valid 0.99342, valid PER 30.46%
EPOCH 20:
  batch 50 loss: 0.7597758042812347
  batch 100 loss: 0.7943409621715546
  batch 150 loss: 0.7798075342178344
  batch 200 loss: 0.8106323957443238
  batch 250 loss: 0.8324355804920196
  batch 300 loss: 0.850909696817398
  batch 350 loss: 0.7947514891624451
  batch 400 loss: 0.8117278409004212
  batch 450 loss: 0.8587716722488403
  batch 500 loss: 0.8396965432167053
  batch 550 loss: 0.9106293952465058
  batch 600 loss: 0.8335699427127838
  batch 650 loss: 0.8502899658679962
  batch 700 loss: 0.8459984695911408
  batch 750 loss: 0.8332372748851776
  batch 800 loss: 0.8689874649047852
  batch 850 loss: 0.8518994557857513
  batch 900 loss: 0.883180273771286
LOSS train 0.88318 valid 1.01177, valid PER 30.72%
Training finished in 4.0 minutes.
Model saved to checkpoints/20230114_204415/model_13
Loading model from checkpoints/20230114_204415/model_13
SUB: 17.68%, DEL: 12.16%, INS: 2.38%, COR: 70.15%, PER: 32.23%
