Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=2, fbank_dims=23, model_dims=256, concat=1, lr=0.002, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.2, beta1=0.9, beta2=0.999)
Total number of model parameters is 2172968
EPOCH 1:
  batch 50 loss: 4.721282291412353
  batch 100 loss: 3.1473801708221436
  batch 150 loss: 3.0164409112930297
  batch 200 loss: 2.784057731628418
  batch 250 loss: 2.609066948890686
  batch 300 loss: 2.398480410575867
  batch 350 loss: 2.1791731786727904
  batch 400 loss: 2.0759064292907716
  batch 450 loss: 1.9057906031608582
  batch 500 loss: 1.7396722674369811
  batch 550 loss: 1.7187380218505859
  batch 600 loss: 1.6542479228973388
  batch 650 loss: 1.5519056749343871
  batch 700 loss: 1.5561654043197632
  batch 750 loss: 1.4784521532058716
  batch 800 loss: 1.5071231937408447
  batch 850 loss: 1.4813268852233887
  batch 900 loss: 1.367982609272003
LOSS train 1.36798 valid 1.37655, valid PER 43.29%
EPOCH 2:
  batch 50 loss: 1.3359519624710083
  batch 100 loss: 1.3352654492855072
  batch 150 loss: 1.2675259959697724
  batch 200 loss: 1.2646728467941284
  batch 250 loss: 1.3208492612838745
  batch 300 loss: 1.2263514029979705
  batch 350 loss: 1.229197758436203
  batch 400 loss: 1.2155375945568085
  batch 450 loss: 1.2048937606811523
  batch 500 loss: 1.202189040184021
  batch 550 loss: 1.1818538093566895
  batch 600 loss: 1.1787824821472168
  batch 650 loss: 1.2379473495483397
  batch 700 loss: 1.1866992449760436
  batch 750 loss: 1.1603569996356964
  batch 800 loss: 1.130758775472641
  batch 850 loss: 1.127103021144867
  batch 900 loss: 1.1231896030902861
LOSS train 1.12319 valid 1.09268, valid PER 34.20%
EPOCH 3:
  batch 50 loss: 1.0183027470111847
  batch 100 loss: 1.1160419058799744
  batch 150 loss: 1.1200389134883881
  batch 200 loss: 1.0192830955982208
  batch 250 loss: 1.0221669232845307
  batch 300 loss: 1.0899704205989837
  batch 350 loss: 1.0656553626060485
  batch 400 loss: 1.0320557475090026
  batch 450 loss: 1.01028604388237
  batch 500 loss: 1.0252271258831025
  batch 550 loss: 1.0351045942306518
  batch 600 loss: 0.9946123552322388
  batch 650 loss: 1.0074070465564728
  batch 700 loss: 1.0401373052597045
  batch 750 loss: 1.0315880036354066
  batch 800 loss: 1.030610146522522
  batch 850 loss: 0.9942811584472656
  batch 900 loss: 0.989646772146225
LOSS train 0.98965 valid 0.99823, valid PER 31.54%
EPOCH 4:
  batch 50 loss: 0.9758411574363709
  batch 100 loss: 0.890498116016388
  batch 150 loss: 0.9165176165103912
  batch 200 loss: 0.8873287379741669
  batch 250 loss: 0.9010883438587188
  batch 300 loss: 0.9031368219852447
  batch 350 loss: 0.8980480635166168
  batch 400 loss: 0.8782651782035827
  batch 450 loss: 0.9003864645957946
  batch 500 loss: 0.9547391974925995
  batch 550 loss: 0.8691892087459564
  batch 600 loss: 0.9181582069396973
  batch 650 loss: 0.9391007316112518
  batch 700 loss: 0.9386252295970917
  batch 750 loss: 0.9099726712703705
  batch 800 loss: 0.9045409739017487
  batch 850 loss: 0.9044534766674042
  batch 900 loss: 0.9182071483135223
LOSS train 0.91821 valid 0.94412, valid PER 29.06%
EPOCH 5:
  batch 50 loss: 0.8454191625118256
  batch 100 loss: 0.8091577422618866
  batch 150 loss: 0.8278353297710419
  batch 200 loss: 0.8551832401752472
  batch 250 loss: 0.8344632995128631
  batch 300 loss: 0.8703208661079407
  batch 350 loss: 0.8975549805164337
  batch 400 loss: 0.8278437376022338
  batch 450 loss: 0.8406144773960114
  batch 500 loss: 0.8161978006362915
  batch 550 loss: 0.8578077310323715
  batch 600 loss: 0.8388643252849579
  batch 650 loss: 0.8547501718997955
  batch 700 loss: 0.8452440637350083
  batch 750 loss: 0.8202508926391602
  batch 800 loss: 0.8496159636974334
  batch 850 loss: 0.865844806432724
  batch 900 loss: 0.8218505883216858
LOSS train 0.82185 valid 0.90757, valid PER 28.10%
EPOCH 6:
  batch 50 loss: 0.761251357793808
  batch 100 loss: 0.7607881426811218
  batch 150 loss: 0.7665116214752197
  batch 200 loss: 0.748644061088562
  batch 250 loss: 0.775871307849884
  batch 300 loss: 0.818461502790451
  batch 350 loss: 0.8287178349494934
  batch 400 loss: 0.7874134588241577
  batch 450 loss: 0.8116293269395828
  batch 500 loss: 0.7417819744348526
  batch 550 loss: 0.8031661856174469
  batch 600 loss: 0.7718442165851593
  batch 650 loss: 0.7584405529499054
  batch 700 loss: 0.7777594155073166
  batch 750 loss: 0.7931709063053131
  batch 800 loss: 0.7722628331184387
  batch 850 loss: 0.813009866476059
  batch 900 loss: 0.827613559961319
LOSS train 0.82761 valid 0.86414, valid PER 27.31%
EPOCH 7:
  batch 50 loss: 0.7215054535865784
  batch 100 loss: 0.7644895935058593
  batch 150 loss: 0.687251483798027
  batch 200 loss: 0.7319515246152878
  batch 250 loss: 0.745453292131424
  batch 300 loss: 0.7210843408107758
  batch 350 loss: 0.7623324608802795
  batch 400 loss: 0.735307457447052
  batch 450 loss: 0.7203128683567047
  batch 500 loss: 0.732074464559555
  batch 550 loss: 0.7392257010936737
  batch 600 loss: 0.7575722050666809
  batch 650 loss: 0.7148111718893051
  batch 700 loss: 0.7427974140644074
  batch 750 loss: 0.751848635673523
  batch 800 loss: 0.7385224175453186
  batch 850 loss: 0.7367965054512023
  batch 900 loss: 0.7282247602939605
LOSS train 0.72822 valid 0.85984, valid PER 26.78%
EPOCH 8:
  batch 50 loss: 0.6605093085765839
  batch 100 loss: 0.6350256234407425
  batch 150 loss: 0.6904121828079224
  batch 200 loss: 0.6662467384338379
  batch 250 loss: 0.6637985014915466
  batch 300 loss: 0.6430670404434204
  batch 350 loss: 0.6669251579046249
  batch 400 loss: 0.6698429441452026
  batch 450 loss: 0.74398601770401
  batch 500 loss: 0.7549136972427368
  batch 550 loss: 0.7511712753772736
  batch 600 loss: 0.7204741322994233
  batch 650 loss: 0.711237701177597
  batch 700 loss: 0.730459303855896
  batch 750 loss: 0.7489052999019623
  batch 800 loss: 0.7593217802047729
  batch 850 loss: 0.7181470942497253
  batch 900 loss: 0.7068925297260285
LOSS train 0.70689 valid 0.83213, valid PER 25.72%
EPOCH 9:
  batch 50 loss: 0.6109678071737289
  batch 100 loss: 0.6256407916545867
  batch 150 loss: 0.6383923715353013
  batch 200 loss: 0.6364824378490448
  batch 250 loss: 0.6344338071346283
  batch 300 loss: 0.6521417933702469
  batch 350 loss: 0.6426608222723007
  batch 400 loss: 0.6790785437822342
  batch 450 loss: 0.6891962713003159
  batch 500 loss: 0.6576399463415146
  batch 550 loss: 0.6533751362562179
  batch 600 loss: 0.6612100499868393
  batch 650 loss: 0.6630736494064331
  batch 700 loss: 0.6563671159744263
  batch 750 loss: 0.6793532019853592
  batch 800 loss: 0.6995972847938537
  batch 850 loss: 0.6798224890232086
  batch 900 loss: 0.6644452899694443
LOSS train 0.66445 valid 0.81795, valid PER 24.78%
EPOCH 10:
  batch 50 loss: 0.5661322563886643
  batch 100 loss: 0.6715624564886093
  batch 150 loss: 0.6232433182001114
  batch 200 loss: 0.590836632847786
  batch 250 loss: 0.6012964558601379
  batch 300 loss: 0.6059690380096435
  batch 350 loss: 0.5975792080163955
  batch 400 loss: 0.5790767568349838
  batch 450 loss: 0.5923678547143936
  batch 500 loss: 0.6059522020816803
  batch 550 loss: 0.6194256097078323
  batch 600 loss: 0.644327871799469
  batch 650 loss: 0.632478728890419
  batch 700 loss: 0.6083414250612259
  batch 750 loss: 0.6291099387407303
  batch 800 loss: 0.6442043334245682
  batch 850 loss: 0.616911671757698
  batch 900 loss: 0.6007388591766357
LOSS train 0.60074 valid 0.81059, valid PER 24.46%
EPOCH 11:
  batch 50 loss: 0.5401781809329986
  batch 100 loss: 0.5341139489412308
  batch 150 loss: 0.562312479019165
  batch 200 loss: 0.5189743727445603
  batch 250 loss: 0.5426625883579255
  batch 300 loss: 0.5476165163516998
  batch 350 loss: 0.6464393198490143
  batch 400 loss: 0.6099595534801483
  batch 450 loss: 0.5970763021707535
  batch 500 loss: 0.5759989213943482
  batch 550 loss: 0.5978232723474503
  batch 600 loss: 0.5687768137454987
  batch 650 loss: 0.6270712733268737
  batch 700 loss: 0.6518176931142807
  batch 750 loss: 0.5813334423303604
  batch 800 loss: 0.5993974447250366
  batch 850 loss: 0.5944405108690262
  batch 900 loss: 0.6072050803899764
LOSS train 0.60721 valid 0.79316, valid PER 24.44%
EPOCH 12:
  batch 50 loss: 0.49580552518367765
  batch 100 loss: 0.4888139075040817
  batch 150 loss: 0.5182361871004104
  batch 200 loss: 0.5356026303768158
  batch 250 loss: 0.5518201923370362
  batch 300 loss: 0.5455040109157562
  batch 350 loss: 0.5178679901361466
  batch 400 loss: 0.5394142466783524
  batch 450 loss: 0.5278711122274399
  batch 500 loss: 0.6037792474031448
  batch 550 loss: 0.5773422890901565
  batch 600 loss: 0.569592398405075
  batch 650 loss: 0.5838387930393218
  batch 700 loss: 0.5711361819505691
  batch 750 loss: 0.5735321116447448
  batch 800 loss: 0.5353274077177048
  batch 850 loss: 0.5659924775362015
  batch 900 loss: 0.5833782923221588
LOSS train 0.58338 valid 0.80239, valid PER 23.66%
EPOCH 13:
  batch 50 loss: 0.47907855093479157
  batch 100 loss: 0.4693342304229736
  batch 150 loss: 0.5017670738697052
  batch 200 loss: 0.4685112798213959
  batch 250 loss: 0.48596377432346344
  batch 300 loss: 0.5330367708206176
  batch 350 loss: 0.5026575011014939
  batch 400 loss: 0.5350603145360947
  batch 450 loss: 0.5226728993654252
  batch 500 loss: 0.5431760329008103
  batch 550 loss: 0.5761396706104278
  batch 600 loss: 0.5479483729600907
  batch 650 loss: 0.5203554391860962
  batch 700 loss: 0.5605029225349426
  batch 750 loss: 0.5221351420879364
  batch 800 loss: 0.5410471904277802
  batch 850 loss: 0.5132112568616867
  batch 900 loss: 0.5584392046928406
LOSS train 0.55844 valid 0.83333, valid PER 24.38%
EPOCH 14:
  batch 50 loss: 0.4806749027967453
  batch 100 loss: 0.4628596019744873
  batch 150 loss: 0.4775374519824982
  batch 200 loss: 0.474372119307518
  batch 250 loss: 0.46918735802173617
  batch 300 loss: 0.46935251116752624
  batch 350 loss: 0.46912618637084963
  batch 400 loss: 0.5391956984996795
  batch 450 loss: 0.5005908435583115
  batch 500 loss: 0.5074697685241699
  batch 550 loss: 0.5398844572901725
  batch 600 loss: 0.4974513506889343
  batch 650 loss: 0.524904944896698
  batch 700 loss: 0.526832685470581
  batch 750 loss: 0.5210115849971771
  batch 800 loss: 0.5187132906913757
  batch 850 loss: 0.5996155345439911
  batch 900 loss: 0.5573351687192917
LOSS train 0.55734 valid 0.83990, valid PER 24.28%
EPOCH 15:
  batch 50 loss: 0.4527250075340271
  batch 100 loss: 0.4552525568008423
  batch 150 loss: 0.4548749887943268
  batch 200 loss: 0.4675076925754547
  batch 250 loss: 0.49392152070999146
  batch 300 loss: 0.47054576575756074
  batch 350 loss: 0.46227466225624086
  batch 400 loss: 0.4599748694896698
  batch 450 loss: 0.4535293760895729
  batch 500 loss: 0.4467395889759064
  batch 550 loss: 0.5224396497011184
  batch 600 loss: 0.5157364022731781
  batch 650 loss: 0.47878770291805267
  batch 700 loss: 0.4510525697469711
  batch 750 loss: 0.4770111399888992
  batch 800 loss: 0.45104057013988497
  batch 850 loss: 0.4726355487108231
  batch 900 loss: 0.4854330602288246
LOSS train 0.48543 valid 0.83483, valid PER 23.87%
EPOCH 16:
  batch 50 loss: 0.42862718164920804
  batch 100 loss: 0.423978243470192
  batch 150 loss: 0.4395252633094788
  batch 200 loss: 0.4219637423753738
  batch 250 loss: 0.4230520871281624
  batch 300 loss: 0.4098038503527641
  batch 350 loss: 0.42687694549560545
  batch 400 loss: 0.43682570099830625
  batch 450 loss: 0.4580077838897705
  batch 500 loss: 0.4454428607225418
  batch 550 loss: 0.43037074863910674
  batch 600 loss: 0.4764558047056198
  batch 650 loss: 0.4930593204498291
  batch 700 loss: 0.4491164356470108
  batch 750 loss: 0.480931293964386
  batch 800 loss: 0.48051904916763305
  batch 850 loss: 0.4751466345787048
  batch 900 loss: 0.4770309776067734
LOSS train 0.47703 valid 0.83149, valid PER 23.59%
EPOCH 17:
  batch 50 loss: 0.41248864829540255
  batch 100 loss: 0.3744292396306992
  batch 150 loss: 0.40122049808502197
  batch 200 loss: 0.37944106429815294
  batch 250 loss: 0.40409337610006335
  batch 300 loss: 0.3818400022387505
  batch 350 loss: 0.4147892627120018
  batch 400 loss: 0.43660046279430387
  batch 450 loss: 0.38445107340812684
  batch 500 loss: 0.4270035642385483
  batch 550 loss: 0.41785859227180483
  batch 600 loss: 0.45104075998067855
  batch 650 loss: 0.43067536771297454
  batch 700 loss: 0.4567268723249435
  batch 750 loss: 0.42140513509511945
  batch 800 loss: 0.46750701785087584
  batch 850 loss: 0.4573959082365036
  batch 900 loss: 0.43546162128448485
LOSS train 0.43546 valid 0.83004, valid PER 23.21%
EPOCH 18:
  batch 50 loss: 0.3532230454683304
  batch 100 loss: 0.3551563647389412
  batch 150 loss: 0.39709853649139404
  batch 200 loss: 0.366020023226738
  batch 250 loss: 0.35424346894025804
  batch 300 loss: 0.3733518624305725
  batch 350 loss: 0.35073023080825805
  batch 400 loss: 0.3757919743657112
  batch 450 loss: 0.39792469650506973
  batch 500 loss: 0.4165199959278107
  batch 550 loss: 0.47207980275154116
  batch 600 loss: 0.46624768555164336
  batch 650 loss: 0.4533799505233765
  batch 700 loss: 0.4257390236854553
  batch 750 loss: 0.43771638095378873
  batch 800 loss: 0.4560388284921646
  batch 850 loss: 0.4557763481140137
  batch 900 loss: 0.43043984681367875
LOSS train 0.43044 valid 0.86207, valid PER 24.02%
EPOCH 19:
  batch 50 loss: 0.3565803477168083
  batch 100 loss: 0.3614689978957176
  batch 150 loss: 0.36730811089277265
  batch 200 loss: 0.3732041680812836
  batch 250 loss: 0.38521695405244827
  batch 300 loss: 0.41249214977025983
  batch 350 loss: 0.38095891118049624
  batch 400 loss: 0.37513757407665255
  batch 450 loss: 0.3801593375205994
  batch 500 loss: 0.35706775188446044
  batch 550 loss: 0.40002756655216215
  batch 600 loss: 0.3965862354636192
  batch 650 loss: 0.4081463408470154
  batch 700 loss: 0.4272111541032791
  batch 750 loss: 0.3711014288663864
  batch 800 loss: 0.3767049342393875
  batch 850 loss: 0.4168564221262932
  batch 900 loss: 0.4142395758628845
LOSS train 0.41424 valid 0.87122, valid PER 23.64%
EPOCH 20:
  batch 50 loss: 0.3084655919671059
  batch 100 loss: 0.3526107174158096
  batch 150 loss: 0.3396502149105072
  batch 200 loss: 0.3573524942994118
  batch 250 loss: 0.3487434273958206
  batch 300 loss: 0.34992808192968367
  batch 350 loss: 0.3290357756614685
  batch 400 loss: 0.3541657555103302
  batch 450 loss: 0.4135669016838074
  batch 500 loss: 0.4253465896844864
  batch 550 loss: 0.3961548674106598
  batch 600 loss: 0.3764291748404503
  batch 650 loss: 0.39431177496910097
  batch 700 loss: 0.3787874594330788
  batch 750 loss: 0.3864921858906746
  batch 800 loss: 0.41198654770851134
  batch 850 loss: 0.3967009761929512
  batch 900 loss: 0.40593538314104083
LOSS train 0.40594 valid 0.85660, valid PER 23.37%
[1.367982609272003, 1.1231896030902861, 0.989646772146225, 0.9182071483135223, 0.8218505883216858, 0.827613559961319, 0.7282247602939605, 0.7068925297260285, 0.6644452899694443, 0.6007388591766357, 0.6072050803899764, 0.5833782923221588, 0.5584392046928406, 0.5573351687192917, 0.4854330602288246, 0.4770309776067734, 0.43546162128448485, 0.43043984681367875, 0.4142395758628845, 0.40593538314104083]
[1.3765536546707153, 1.0926847457885742, 0.9982308149337769, 0.9441192150115967, 0.907567024230957, 0.8641437888145447, 0.8598412871360779, 0.8321323394775391, 0.8179520964622498, 0.8105857372283936, 0.7931641936302185, 0.8023864030838013, 0.8333280682563782, 0.8399023413658142, 0.8348345756530762, 0.8314921855926514, 0.8300433158874512, 0.8620662689208984, 0.8712241649627686, 0.8565983176231384]
Training finished in 16.0 minutes.
Model saved to checkpoints/20230125_110309/model_11
Loading model from checkpoints/20230125_110309/model_11
