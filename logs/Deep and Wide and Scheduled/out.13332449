Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=4, fbank_dims=23, model_dims=256, concat=1, lr=0.002, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.4, beta1=0.9, beta2=0.999, exp=0.9)
Total number of model parameters is 5326888
EPOCH 1:
  batch 50 loss: 4.503149833679199
  batch 100 loss: 3.306166009902954
  batch 150 loss: 3.278663787841797
  batch 200 loss: 3.2655202198028563
  batch 250 loss: 3.1249037837982176
  batch 300 loss: 3.007829842567444
  batch 350 loss: 2.9268736839294434
  batch 400 loss: 2.9017409753799437
  batch 450 loss: 2.7933034610748293
  batch 500 loss: 2.689956560134888
  batch 550 loss: 2.5584956645965575
  batch 600 loss: 2.4474156522750854
  batch 650 loss: 2.3400082063674925
  batch 700 loss: 2.2679180669784547
  batch 750 loss: 2.147055459022522
  batch 800 loss: 2.136925148963928
  batch 850 loss: 2.04966739654541
  batch 900 loss: 1.9764248156547546
LOSS train 1.97642 valid 1.86972, valid PER 56.83%
loss updated
EPOCH 2:
  batch 50 loss: 1.8912580156326293
  batch 100 loss: 1.857617561817169
  batch 150 loss: 1.749217174053192
  batch 200 loss: 1.7682431960105895
  batch 250 loss: 1.736179118156433
  batch 300 loss: 1.693559446334839
  batch 350 loss: 1.6569334506988525
  batch 400 loss: 1.6238215732574464
  batch 450 loss: 1.5897343373298645
  batch 500 loss: 1.5794586420059205
  batch 550 loss: 1.557759075164795
  batch 600 loss: 1.5060478496551513
  batch 650 loss: 1.4399161624908448
  batch 700 loss: 1.4580177164077759
  batch 750 loss: 1.435321979522705
  batch 800 loss: 1.4074905943870544
  batch 850 loss: 1.3930726909637452
  batch 900 loss: 1.4027943801879883
LOSS train 1.40279 valid 1.28718, valid PER 39.76%
loss updated
EPOCH 3:
  batch 50 loss: 1.3169139552116393
  batch 100 loss: 1.3741113352775574
  batch 150 loss: 1.3518795704841613
  batch 200 loss: 1.2870002281665802
  batch 250 loss: 1.2597271120548248
  batch 300 loss: 1.6814236378669738
  batch 350 loss: 1.354556291103363
  batch 400 loss: 1.4469524097442628
  batch 450 loss: 1.5186269426345824
  batch 500 loss: 1.337173411846161
  batch 550 loss: 1.278070435523987
  batch 600 loss: 1.2250677394866942
  batch 650 loss: 1.2166385650634766
  batch 700 loss: 1.2171920824050904
  batch 750 loss: 1.250957384109497
  batch 800 loss: 1.263161473274231
  batch 850 loss: 1.1879394674301147
  batch 900 loss: 1.1840255856513977
LOSS train 1.18403 valid 1.14693, valid PER 36.00%
loss updated
EPOCH 4:
  batch 50 loss: 1.183562914133072
  batch 100 loss: 1.1367108821868896
  batch 150 loss: 1.1408945083618165
  batch 200 loss: 1.156613928079605
  batch 250 loss: 1.1573577070236205
  batch 300 loss: 1.1503600203990936
  batch 350 loss: 1.1185052347183229
  batch 400 loss: 1.105888183116913
  batch 450 loss: 1.0686403214931488
  batch 500 loss: 1.1426602721214294
  batch 550 loss: 1.0852332818508148
  batch 600 loss: 1.0758518409729003
  batch 650 loss: 1.1306616318225862
  batch 700 loss: 1.1404436719417572
  batch 750 loss: 1.1020133078098298
  batch 800 loss: 1.0990929436683654
  batch 850 loss: 1.0656043231487273
  batch 900 loss: 1.070181189775467
LOSS train 1.07018 valid 1.05609, valid PER 32.96%
loss updated
EPOCH 5:
  batch 50 loss: 1.0359479677677155
  batch 100 loss: 1.0167060911655426
  batch 150 loss: 1.0506973278522491
  batch 200 loss: 1.1117295908927918
  batch 250 loss: 1.015914294719696
  batch 300 loss: 1.0412410581111908
  batch 350 loss: 0.9936323809623718
  batch 400 loss: 0.9609126615524292
  batch 450 loss: 1.0218150615692139
  batch 500 loss: 0.9965639364719391
  batch 550 loss: 1.0712184536457061
  batch 600 loss: 1.0204506278038026
  batch 650 loss: 1.017370684146881
  batch 700 loss: 1.0026720583438873
  batch 750 loss: 0.9816054737567902
  batch 800 loss: 1.0548515617847443
  batch 850 loss: 1.0124482786655427
  batch 900 loss: 0.9719023299217224
LOSS train 0.97190 valid 1.04639, valid PER 32.32%
loss updated
EPOCH 6:
  batch 50 loss: 1.0051750981807708
  batch 100 loss: 0.994044531583786
  batch 150 loss: 0.9728436851501465
  batch 200 loss: 0.9404215085506439
  batch 250 loss: 0.9852397572994233
  batch 300 loss: 0.9928787052631378
  batch 350 loss: 0.9994102036952972
  batch 400 loss: 0.9534112429618835
  batch 450 loss: 0.9766774082183838
  batch 500 loss: 0.9463289701938629
  batch 550 loss: 0.9552921760082245
  batch 600 loss: 0.9275698697566986
  batch 650 loss: 0.890141988992691
  batch 700 loss: 0.9157734751701355
  batch 750 loss: 0.969668253660202
  batch 800 loss: 0.9763447785377503
  batch 850 loss: 0.9776918470859528
  batch 900 loss: 0.9790810739994049
LOSS train 0.97908 valid 0.91871, valid PER 28.60%
loss updated
EPOCH 7:
  batch 50 loss: 0.8915327179431916
  batch 100 loss: 0.9332281577587128
  batch 150 loss: 0.8823507189750671
  batch 200 loss: 0.9333616971969605
  batch 250 loss: 0.9722909641265869
  batch 300 loss: 0.9092041110992432
  batch 350 loss: 0.9325911629199982
  batch 400 loss: 0.9068691575527191
  batch 450 loss: 0.9118656790256501
  batch 500 loss: 0.9223816108703613
  batch 550 loss: 0.8883162474632263
  batch 600 loss: 0.9155436563491821
  batch 650 loss: 0.9010113084316254
  batch 700 loss: 0.909335230588913
  batch 750 loss: 0.8988264191150666
  batch 800 loss: 0.8965961289405823
  batch 850 loss: 0.8656704330444336
  batch 900 loss: 0.8896722745895386
LOSS train 0.88967 valid 0.88770, valid PER 28.00%
loss updated
EPOCH 8:
  batch 50 loss: 0.8614869737625122
  batch 100 loss: 0.8227755427360535
  batch 150 loss: 0.884362839460373
  batch 200 loss: 0.8862541782855987
  batch 250 loss: 0.8524042427539825
  batch 300 loss: 0.8359785568714142
  batch 350 loss: 0.8589181995391846
  batch 400 loss: 0.837332340478897
  batch 450 loss: 0.9195451533794403
  batch 500 loss: 0.8887967073917389
  batch 550 loss: 0.8672901248931885
  batch 600 loss: 0.8472307801246644
  batch 650 loss: 0.8549678087234497
  batch 700 loss: 0.8656199991703033
  batch 750 loss: 0.8688712477684021
  batch 800 loss: 0.8933944725990295
  batch 850 loss: 0.8490028810501099
  batch 900 loss: 0.8366115772724152
LOSS train 0.83661 valid 0.86731, valid PER 27.09%
loss updated
EPOCH 9:
  batch 50 loss: 0.7994987571239471
  batch 100 loss: 0.8013250255584716
  batch 150 loss: 0.8099775874614715
  batch 200 loss: 0.8296829783916473
  batch 250 loss: 0.8108772921562195
  batch 300 loss: 0.8256648266315461
  batch 350 loss: 0.8329437243938446
  batch 400 loss: 0.8327713376283645
  batch 450 loss: 0.8672894728183747
  batch 500 loss: 0.8034207558631897
  batch 550 loss: 0.8052038311958313
  batch 600 loss: 0.8408198368549347
  batch 650 loss: 0.8426790726184845
  batch 700 loss: 0.8010123074054718
  batch 750 loss: 0.8179834634065628
  batch 800 loss: 0.8417034018039703
  batch 850 loss: 0.8227699887752533
  batch 900 loss: 0.7906043040752411
LOSS train 0.79060 valid 0.83534, valid PER 26.20%
loss updated
EPOCH 10:
  batch 50 loss: 0.7690324187278748
  batch 100 loss: 0.7699336206912994
  batch 150 loss: 0.7847667384147644
  batch 200 loss: 0.7820926904678345
  batch 250 loss: 0.7696221053600312
  batch 300 loss: 0.7760228037834167
  batch 350 loss: 0.7606721693277358
  batch 400 loss: 0.7690459728240967
  batch 450 loss: 0.7810116028785705
  batch 500 loss: 0.7549753570556641
  batch 550 loss: 0.7856758534908295
  batch 600 loss: 0.7647598707675933
  batch 650 loss: 0.804963983297348
  batch 700 loss: 0.7802561151981354
  batch 750 loss: 0.7991379630565644
  batch 800 loss: 0.7760150742530822
  batch 850 loss: 0.7907141804695129
  batch 900 loss: 0.8019756376743317
LOSS train 0.80198 valid 0.84752, valid PER 26.19%
EPOCH 11:
  batch 50 loss: 0.7794737052917481
  batch 100 loss: 0.7593111670017243
  batch 150 loss: 0.769102674126625
  batch 200 loss: 0.7296324419975281
  batch 250 loss: 0.7485226547718048
  batch 300 loss: 0.7423147886991501
  batch 350 loss: 0.7877253466844558
  batch 400 loss: 0.7305650645494461
  batch 450 loss: 0.743135906457901
  batch 500 loss: 0.7367645418643951
  batch 550 loss: 0.7645610797405243
  batch 600 loss: 0.7612083643674851
  batch 650 loss: 0.7538247048854828
  batch 700 loss: 0.799044793844223
  batch 750 loss: 0.7617207479476928
  batch 800 loss: 0.7589555418491364
  batch 850 loss: 0.7469236552715302
  batch 900 loss: 0.7832426464557648
LOSS train 0.78324 valid 0.82705, valid PER 25.94%
loss updated
EPOCH 12:
  batch 50 loss: 0.7128940272331238
  batch 100 loss: 0.670069529414177
  batch 150 loss: 0.7152703130245208
  batch 200 loss: 0.7262834846973419
  batch 250 loss: 0.7290502238273621
  batch 300 loss: 0.7655026876926422
  batch 350 loss: 0.7085614967346191
  batch 400 loss: 0.7635643529891968
  batch 450 loss: 0.7241031414270401
  batch 500 loss: 0.7304357302188873
  batch 550 loss: 0.7476779901981354
  batch 600 loss: 0.7601659071445465
  batch 650 loss: 0.7449668848514557
  batch 700 loss: 0.7574180603027344
  batch 750 loss: 0.7578849983215332
  batch 800 loss: 0.7365004336833954
  batch 850 loss: 0.7450576043128967
  batch 900 loss: 0.7536594432592392
LOSS train 0.75366 valid 0.80948, valid PER 25.18%
loss updated
EPOCH 13:
  batch 50 loss: 0.6833622753620148
  batch 100 loss: 0.6904116767644882
  batch 150 loss: 0.7195128166675567
  batch 200 loss: 0.6815126740932465
  batch 250 loss: 0.7033571088314057
  batch 300 loss: 0.7152663224935532
  batch 350 loss: 0.7094448846578598
  batch 400 loss: 0.7251066589355468
  batch 450 loss: 0.7381272685527801
  batch 500 loss: 0.7145713651180268
  batch 550 loss: 0.7646607840061188
  batch 600 loss: 0.7120893973112107
  batch 650 loss: 0.721350051164627
  batch 700 loss: 0.7253978157043457
  batch 750 loss: 0.6987191295623779
  batch 800 loss: 0.7028923255205154
  batch 850 loss: 0.7073889291286468
  batch 900 loss: 0.702440534234047
LOSS train 0.70244 valid 0.76832, valid PER 23.66%
loss updated
EPOCH 14:
  batch 50 loss: 0.648160879611969
  batch 100 loss: 0.6282492995262146
  batch 150 loss: 0.6471097016334534
  batch 200 loss: 0.6636693221330643
  batch 250 loss: 0.6752156454324723
  batch 300 loss: 0.666792219877243
  batch 350 loss: 0.6658506214618682
  batch 400 loss: 0.6828446018695832
  batch 450 loss: 0.6744119411706925
  batch 500 loss: 0.6911781531572342
  batch 550 loss: 0.6870748960971832
  batch 600 loss: 0.6993193429708481
  batch 650 loss: 0.6897441899776459
  batch 700 loss: 0.712081880569458
  batch 750 loss: 0.7147050702571869
  batch 800 loss: 0.6764019238948822
  batch 850 loss: 0.721399478316307
  batch 900 loss: 0.6960808467864991
LOSS train 0.69608 valid 0.78504, valid PER 24.12%
EPOCH 15:
  batch 50 loss: 0.637091298699379
  batch 100 loss: 0.6523848068714142
  batch 150 loss: 0.6540133261680603
  batch 200 loss: 0.6918895757198333
  batch 250 loss: 0.6768758183717728
  batch 300 loss: 0.6786817914247513
  batch 350 loss: 0.6455317854881286
  batch 400 loss: 0.6785686212778091
  batch 450 loss: 0.6676998221874237
  batch 500 loss: 0.6510077768564224
  batch 550 loss: 0.7384051096439361
  batch 600 loss: 0.7168863666057587
  batch 650 loss: 0.6706887525320053
  batch 700 loss: 0.6195385038852692
  batch 750 loss: 0.6716270065307617
  batch 800 loss: 0.6456152409315109
  batch 850 loss: 0.665604378581047
  batch 900 loss: 0.6290761798620224
LOSS train 0.62908 valid 0.76704, valid PER 23.33%
loss updated
EPOCH 16:
  batch 50 loss: 0.6137056213617325
  batch 100 loss: 0.6019014596939087
  batch 150 loss: 0.616729406118393
  batch 200 loss: 0.6636766195297241
  batch 250 loss: 0.6328573042154312
  batch 300 loss: 0.6365051746368409
  batch 350 loss: 0.6467740976810455
  batch 400 loss: 0.6234554994106293
  batch 450 loss: 0.6467741119861603
  batch 500 loss: 0.6412478482723236
  batch 550 loss: 0.6109039402008056
  batch 600 loss: 0.658908777832985
  batch 650 loss: 0.6665134984254837
  batch 700 loss: 0.6478920823335648
  batch 750 loss: 0.6546628111600876
  batch 800 loss: 0.6552469485998154
  batch 850 loss: 0.6419697827100754
  batch 900 loss: 0.6436340415477753
LOSS train 0.64363 valid 0.75493, valid PER 23.36%
loss updated
EPOCH 17:
  batch 50 loss: 0.6029090601205825
  batch 100 loss: 0.5620955300331115
  batch 150 loss: 0.6477568000555038
  batch 200 loss: 0.6111148828268052
  batch 250 loss: 0.6280987453460694
  batch 300 loss: 0.6075920897722245
  batch 350 loss: 0.6377267342805862
  batch 400 loss: 0.6498310500383377
  batch 450 loss: 0.6016797047853469
  batch 500 loss: 0.633835443854332
  batch 550 loss: 0.6296941608190536
  batch 600 loss: 0.6777442044019699
  batch 650 loss: 0.6174694156646728
  batch 700 loss: 0.6393039125204086
  batch 750 loss: 0.5996002823114395
  batch 800 loss: 0.6391963404417038
  batch 850 loss: 0.6388206350803375
  batch 900 loss: 0.6194588863849639
LOSS train 0.61946 valid 0.75748, valid PER 22.88%
EPOCH 18:
  batch 50 loss: 0.5682494801282882
  batch 100 loss: 0.5781035816669464
  batch 150 loss: 0.6183788675069809
  batch 200 loss: 0.5961644792556763
  batch 250 loss: 0.5846795761585235
  batch 300 loss: 0.5872495871782303
  batch 350 loss: 0.5766986924409866
  batch 400 loss: 0.5958094269037246
  batch 450 loss: 0.595858919620514
  batch 500 loss: 0.6058430314064026
  batch 550 loss: 0.6593590927124023
  batch 600 loss: 0.633555873632431
  batch 650 loss: 0.6033189237117768
  batch 700 loss: 0.60764384329319
  batch 750 loss: 0.6304232597351074
  batch 800 loss: 0.6504653024673462
  batch 850 loss: 0.6285745424032211
  batch 900 loss: 0.6204884159564972
LOSS train 0.62049 valid 0.75172, valid PER 22.66%
loss updated
EPOCH 19:
  batch 50 loss: 0.5866732954978943
  batch 100 loss: 0.563968391418457
  batch 150 loss: 0.5598124426603317
  batch 200 loss: 0.5546258026361466
  batch 250 loss: 0.6082221657037735
  batch 300 loss: 0.6283994799852372
  batch 350 loss: 0.6153338199853897
  batch 400 loss: 0.5939365607500077
  batch 450 loss: 0.561452249288559
  batch 500 loss: 0.5783599185943603
  batch 550 loss: 0.6103058463335037
  batch 600 loss: 0.6077225828170776
  batch 650 loss: 0.6219831883907319
  batch 700 loss: 0.6374908602237701
  batch 750 loss: 0.6001513850688934
  batch 800 loss: 0.6046656388044357
  batch 850 loss: 0.6054718899726867
  batch 900 loss: 0.5955227112770081
LOSS train 0.59552 valid 0.74497, valid PER 22.50%
loss updated
EPOCH 20:
  batch 50 loss: 0.5343929553031921
  batch 100 loss: 0.5718417972326278
  batch 150 loss: 0.5695639485120774
  batch 200 loss: 0.5638688510656357
  batch 250 loss: 0.5754105967283248
  batch 300 loss: 0.5594622975587845
  batch 350 loss: 0.5349102532863617
  batch 400 loss: 0.571078149676323
  batch 450 loss: 0.565032486319542
  batch 500 loss: 0.5881658393144608
  batch 550 loss: 0.5986556774377823
  batch 600 loss: 0.582160302400589
  batch 650 loss: 0.6303660100698472
  batch 700 loss: 0.6673539584875107
  batch 750 loss: 0.6355789464712143
  batch 800 loss: 0.631450988650322
  batch 850 loss: 0.6088191103935242
  batch 900 loss: 0.5911183536052704
LOSS train 0.59112 valid 0.77232, valid PER 23.10%
[1.9764248156547546, 1.4027943801879883, 1.1840255856513977, 1.070181189775467, 0.9719023299217224, 0.9790810739994049, 0.8896722745895386, 0.8366115772724152, 0.7906043040752411, 0.8019756376743317, 0.7832426464557648, 0.7536594432592392, 0.702440534234047, 0.6960808467864991, 0.6290761798620224, 0.6436340415477753, 0.6194588863849639, 0.6204884159564972, 0.5955227112770081, 0.5911183536052704]
[1.869718074798584, 1.2871787548065186, 1.146929144859314, 1.0560858249664307, 1.046385645866394, 0.918708086013794, 0.887704074382782, 0.8673064708709717, 0.8353356719017029, 0.8475174307823181, 0.8270468711853027, 0.809479296207428, 0.7683196067810059, 0.7850376963615417, 0.7670368552207947, 0.7549348473548889, 0.7574772238731384, 0.7517173886299133, 0.7449673414230347, 0.7723206281661987]
Training finished in 27.0 minutes.
Model saved to checkpoints/20230125_221344/model_19
Loading model from checkpoints/20230125_221344/model_19
SUB: 15.92%, DEL: 6.31%, INS: 2.22%, COR: 77.77%, PER: 24.45%
