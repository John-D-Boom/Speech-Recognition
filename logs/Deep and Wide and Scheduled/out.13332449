Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=4, fbank_dims=23, model_dims=256, concat=1, lr=0.002, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.4, beta1=0.9, beta2=0.999, exp=0.9)
Total number of model parameters is 5326888
EPOCH 1:
  batch 50 loss: 4.503149833679199
  batch 100 loss: 3.306166009902954
  batch 150 loss: 3.278663787841797
  batch 200 loss: 3.2655202198028563
  batch 250 loss: 3.1249037837982176
  batch 300 loss: 3.007829842567444
  batch 350 loss: 2.9268736839294434
  batch 400 loss: 2.9017409753799437
  batch 450 loss: 2.7933034610748293
  batch 500 loss: 2.689956560134888
  batch 550 loss: 2.5584956645965575
  batch 600 loss: 2.4474156522750854
  batch 650 loss: 2.3400082063674925
  batch 700 loss: 2.2679180669784547
  batch 750 loss: 2.147055459022522
  batch 800 loss: 2.136925148963928
  batch 850 loss: 2.04966739654541
  batch 900 loss: 1.9764248156547546
LOSS train 1.97642 valid 1.86972, valid PER 56.83%
loss updated
EPOCH 2:
  batch 50 loss: 1.8912580156326293
  batch 100 loss: 1.857617561817169
  batch 150 loss: 1.749217174053192
  batch 200 loss: 1.7682431960105895
  batch 250 loss: 1.736179118156433
  batch 300 loss: 1.693559446334839
  batch 350 loss: 1.6569334506988525
  batch 400 loss: 1.6238215732574464
  batch 450 loss: 1.5897343373298645
  batch 500 loss: 1.5794586420059205
  batch 550 loss: 1.557759075164795
  batch 600 loss: 1.5060478496551513
  batch 650 loss: 1.4399161624908448
  batch 700 loss: 1.4580177164077759
  batch 750 loss: 1.435321979522705
  batch 800 loss: 1.4074905943870544
  batch 850 loss: 1.3930726909637452
  batch 900 loss: 1.4027943801879883
LOSS train 1.40279 valid 1.28718, valid PER 39.76%
loss updated
EPOCH 3:
  batch 50 loss: 1.3169139552116393
  batch 100 loss: 1.3741113352775574
  batch 150 loss: 1.3518795704841613
  batch 200 loss: 1.2870002281665802
  batch 250 loss: 1.2597271120548248
  batch 300 loss: 1.6814236378669738
  batch 350 loss: 1.354556291103363
  batch 400 loss: 1.4469524097442628
  batch 450 loss: 1.5186269426345824
  batch 500 loss: 1.337173411846161
  batch 550 loss: 1.278070435523987
  batch 600 loss: 1.2250677394866942
  batch 650 loss: 1.2166385650634766
  batch 700 loss: 1.2171920824050904
  batch 750 loss: 1.250957384109497
  batch 800 loss: 1.263161473274231
  batch 850 loss: 1.1879394674301147
  batch 900 loss: 1.1840255856513977
LOSS train 1.18403 valid 1.14693, valid PER 36.00%
loss updated
EPOCH 4:
  batch 50 loss: 1.183562914133072
  batch 100 loss: 1.1367108821868896
  batch 150 loss: 1.1408945083618165
  batch 200 loss: 1.156613928079605
  batch 250 loss: 1.1573577070236205
  batch 300 loss: 1.1503600203990936
  batch 350 loss: 1.1185052347183229
  batch 400 loss: 1.105888183116913
  batch 450 loss: 1.0686403214931488
  batch 500 loss: 1.1426602721214294
  batch 550 loss: 1.0852332818508148
  batch 600 loss: 1.0758518409729003
  batch 650 loss: 1.1306616318225862
  batch 700 loss: 1.1404436719417572
  batch 750 loss: 1.1020133078098298
  batch 800 loss: 1.0990929436683654
  batch 850 loss: 1.0656043231487273
  batch 900 loss: 1.070181189775467
LOSS train 1.07018 valid 1.05609, valid PER 32.96%
loss updated
EPOCH 5:
  batch 50 loss: 1.0359479677677155
  batch 100 loss: 1.0167060911655426
  batch 150 loss: 1.0506973278522491
  batch 200 loss: 1.1117295908927918
  batch 250 loss: 1.015914294719696
  batch 300 loss: 1.0412410581111908
  batch 350 loss: 0.9936323809623718
  batch 400 loss: 0.9609126615524292
  batch 450 loss: 1.0218150615692139
  batch 500 loss: 0.9965639364719391
  batch 550 loss: 1.0712184536457061
  batch 600 loss: 1.0204506278038026
  batch 650 loss: 1.017370684146881
  batch 700 loss: 1.0026720583438873
  batch 750 loss: 0.9816054737567902
