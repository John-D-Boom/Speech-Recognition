Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=4, fbank_dims=23, model_dims=256, concat=1, lr=0.004, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.4, beta1=0.9, beta2=0.999, exp=0.5)
Total number of model parameters is 5326888
EPOCH 1:
  batch 50 loss: 4.204145884513855
  batch 100 loss: 3.297043228149414
  batch 150 loss: 3.1964517402648926
  batch 200 loss: 2.9705053091049196
  batch 250 loss: 2.8886210441589357
  batch 300 loss: 2.791964569091797
  batch 350 loss: 2.7131243896484376
  batch 400 loss: 2.637701253890991
  batch 450 loss: 2.5723326444625854
  batch 500 loss: 2.4756913566589356
  batch 550 loss: 2.429895353317261
  batch 600 loss: 2.4043053436279296
  batch 650 loss: 2.3906161546707154
  batch 700 loss: 2.406859107017517
  batch 750 loss: 2.2937706232070925
  batch 800 loss: 2.2968828201293947
  batch 850 loss: 2.241346864700317
  batch 900 loss: 2.182820453643799
LOSS train 2.18282 valid 2.23580, valid PER 67.38%
loss updated
EPOCH 2:
  batch 50 loss: 2.2004493737220763
  batch 100 loss: 2.1222409057617186
  batch 150 loss: 2.02777104139328
  batch 200 loss: 2.065408742427826
  batch 250 loss: 2.0382771587371824
  batch 300 loss: 1.9679645681381226
  batch 350 loss: 1.9526966333389282
  batch 400 loss: 1.91867849111557
  batch 450 loss: 1.9542258763313294
  batch 500 loss: 1.906784210205078
  batch 550 loss: 1.8496237468719483
  batch 600 loss: 1.819807140827179
  batch 650 loss: 1.7900785064697267
  batch 700 loss: 1.7911459016799927
  batch 750 loss: 1.8027598357200623
  batch 800 loss: 1.7576831269264221
  batch 850 loss: 1.7557465505599976
  batch 900 loss: 1.752558183670044
LOSS train 1.75256 valid 1.69524, valid PER 50.52%
loss updated
EPOCH 3:
  batch 50 loss: 1.6670384478569031
  batch 100 loss: 1.7134174466133119
  batch 150 loss: 1.7246507430076599
  batch 200 loss: 1.7015438842773438
  batch 250 loss: 1.689057993888855
  batch 300 loss: 1.6409843730926514
  batch 350 loss: 1.6799223041534423
  batch 400 loss: 1.6134088826179505
  batch 450 loss: 1.5854144883155823
  batch 500 loss: 1.628701651096344
  batch 550 loss: 1.5947088813781738
  batch 600 loss: 1.5646174812316895
  batch 650 loss: 1.5596351432800293
  batch 700 loss: 1.5255263090133666
  batch 750 loss: 1.6003640031814574
  batch 800 loss: 1.5667473268508911
  batch 850 loss: 1.5335848474502562
  batch 900 loss: 1.5068454146385193
LOSS train 1.50685 valid 1.43244, valid PER 44.65%
loss updated
EPOCH 4:
  batch 50 loss: 1.5039933586120606
  batch 100 loss: 1.4775111341476441
  batch 150 loss: 1.4738122773170472
  batch 200 loss: 1.4717387986183166
  batch 250 loss: 1.4647049307823181
  batch 300 loss: 1.4725181198120116
  batch 350 loss: 1.4273306035995483
  batch 400 loss: 1.4467882585525513
  batch 450 loss: 1.445597608089447
  batch 500 loss: 1.4443476581573487
  batch 550 loss: 1.3843983626365661
  batch 600 loss: 1.4062395763397217
  batch 650 loss: 1.409502055644989
  batch 700 loss: 1.3996231317520142
  batch 750 loss: 1.369556155204773
  batch 800 loss: 1.3929017615318298
  batch 850 loss: 1.3727447414398193
  batch 900 loss: 1.389290063381195
LOSS train 1.38929 valid 1.34940, valid PER 40.88%
loss updated
EPOCH 5:
  batch 50 loss: 1.381171736717224
  batch 100 loss: 1.3369075560569763
  batch 150 loss: 1.3891204190254212
  batch 200 loss: 1.391995964050293
  batch 250 loss: 1.309097065925598
  batch 300 loss: 1.333964388370514
  batch 350 loss: 1.320895572900772
  batch 400 loss: 1.259700393676758
  batch 450 loss: 1.322080798149109
  batch 500 loss: 1.2857008755207062
  batch 550 loss: 1.345327377319336
  batch 600 loss: 1.3344131708145142
  batch 650 loss: 1.3103882110118865
  batch 700 loss: 1.2952803564071655
  batch 750 loss: 1.2864025342464447
  batch 800 loss: 1.3736186265945434
  batch 850 loss: 1.3669281911849975
  batch 900 loss: 1.292737762928009
LOSS train 1.29274 valid 1.24711, valid PER 39.05%
loss updated
EPOCH 6:
  batch 50 loss: 1.2783671069145202
  batch 100 loss: 1.2837443840503693
  batch 150 loss: 1.2534488940238953
  batch 200 loss: 1.250620619058609
  batch 250 loss: 1.300562596321106
  batch 300 loss: 1.2903404712677002
  batch 350 loss: 1.2660518956184388
  batch 400 loss: 1.2558737540245055
  batch 450 loss: 1.2891683197021484
  batch 500 loss: 1.29455176115036
  batch 550 loss: 1.2817237174510956
  batch 600 loss: 1.2481323027610778
  batch 650 loss: 1.229565601348877
  batch 700 loss: 1.2542944526672364
  batch 750 loss: 1.2749570965766908
  batch 800 loss: 1.2751096558570862
  batch 850 loss: 1.2622781813144683
  batch 900 loss: 1.2764813339710235
LOSS train 1.27648 valid 1.18642, valid PER 36.86%
loss updated
EPOCH 7:
  batch 50 loss: 1.2531586349010468
  batch 100 loss: 1.2637854957580565
  batch 150 loss: 1.2042213594913482
  batch 200 loss: 1.2111296701431273
  batch 250 loss: 1.267068235874176
  batch 300 loss: 1.2180742502212525
  batch 350 loss: 1.2438018310070038
  batch 400 loss: 1.2536772966384888
  batch 450 loss: 1.2343548035621643
  batch 500 loss: 1.207985303401947
  batch 550 loss: 1.1641742086410523
  batch 600 loss: 1.1846662807464599
  batch 650 loss: 1.173862226009369
  batch 700 loss: 1.2363608884811401
  batch 750 loss: 1.1985897254943847
  batch 800 loss: 1.181649317741394
  batch 850 loss: 1.204018805027008
  batch 900 loss: 1.2204669618606567
LOSS train 1.22047 valid 1.17790, valid PER 36.12%
loss updated
EPOCH 8:
  batch 50 loss: 1.2519859158992768
  batch 100 loss: 1.1618631422519683
  batch 150 loss: 1.2414927744865418
  batch 200 loss: 1.245728509426117
  batch 250 loss: 1.236820743083954
  batch 300 loss: 1.2212414050102234
  batch 350 loss: 1.235425546169281
  batch 400 loss: 1.191332800388336
  batch 450 loss: 1.2580338275432588
  batch 500 loss: 1.2046068334579467
  batch 550 loss: 1.1927931606769562
  batch 600 loss: 1.1679714345932006
  batch 650 loss: 1.16250705242157
  batch 700 loss: 1.1709567403793335
  batch 750 loss: 1.2336165988445282
  batch 800 loss: 1.1950519168376923
  batch 850 loss: 1.1455549025535583
  batch 900 loss: 1.1778687381744384
LOSS train 1.17787 valid 1.19471, valid PER 36.76%
EPOCH 9:
  batch 50 loss: 1.213368685245514
  batch 100 loss: 1.1561226725578309
  batch 150 loss: 1.1788770103454589
  batch 200 loss: 1.1724373090267182
  batch 250 loss: 1.1380249667167663
  batch 300 loss: 1.175939507484436
  batch 350 loss: 1.1600196063518524
  batch 400 loss: 1.17837096452713
  batch 450 loss: 1.1717489755153656
  batch 500 loss: 1.1397455382347106
  batch 550 loss: 1.1741697871685028
  batch 600 loss: 1.1727657687664033
  batch 650 loss: 1.164876730442047
  batch 700 loss: 1.1419890069961547
  batch 750 loss: 1.153712729215622
  batch 800 loss: 1.2176888573169709
  batch 850 loss: 1.1777423012256623
  batch 900 loss: 1.1194996678829192
LOSS train 1.11950 valid 1.13305, valid PER 34.48%
loss updated
EPOCH 10:
  batch 50 loss: 1.1575418841838836
  batch 100 loss: 1.1712070333957671
  batch 150 loss: 1.1870573449134827
  batch 200 loss: 1.1601711666584016
  batch 250 loss: 1.1430545103549958
  batch 300 loss: 1.1410995125770569
  batch 350 loss: 1.141950181722641
  batch 400 loss: 1.1262090551853179
  batch 450 loss: 1.1510530817508697
  batch 500 loss: 1.1709175932407379
  batch 550 loss: 1.1651314544677733
  batch 600 loss: 1.1406498456001282
  batch 650 loss: 1.1470862662792205
  batch 700 loss: 1.1562840056419372
  batch 750 loss: 1.1643553137779237
  batch 800 loss: 1.1610724437236786
  batch 850 loss: 1.1567814064025879
  batch 900 loss: 1.1870692324638368
LOSS train 1.18707 valid 1.17516, valid PER 36.26%
EPOCH 11:
  batch 50 loss: 1.1830929124355316
  batch 100 loss: 1.161012246608734
  batch 150 loss: 1.1995138871669768
  batch 200 loss: 1.1419705414772034
  batch 250 loss: 1.1728560245037079
  batch 300 loss: 1.196138916015625
  batch 350 loss: 1.21823983669281
  batch 400 loss: 1.1719449543952942
  batch 450 loss: 1.163839921951294
  batch 500 loss: 1.133133727312088
  batch 550 loss: 1.1462435615062714
  batch 600 loss: 1.156976102590561
  batch 650 loss: 1.1502225625514984
  batch 700 loss: 1.2321391010284424
  batch 750 loss: 1.1049187505245208
  batch 800 loss: 1.126795072555542
  batch 850 loss: 1.1365315735340118
  batch 900 loss: 1.1859743320941925
LOSS train 1.18597 valid 1.13809, valid PER 34.31%
EPOCH 12:
  batch 50 loss: 1.1375613009929657
  batch 100 loss: 1.1274498450756072
  batch 150 loss: 1.1128371024131776
  batch 200 loss: 1.131050626039505
  batch 250 loss: 1.108716231584549
  batch 300 loss: 1.165142798423767
  batch 350 loss: 1.0898482382297516
  batch 400 loss: 1.1228546953201295
  batch 450 loss: 1.1061661159992218
  batch 500 loss: 1.1407247912883758
  batch 550 loss: 1.126696914434433
  batch 600 loss: 1.1145757269859313
  batch 650 loss: 1.1870248770713807
  batch 700 loss: 1.141558051109314
  batch 750 loss: 1.1393429625034333
  batch 800 loss: 1.1101437544822692
  batch 850 loss: 1.1518277299404145
  batch 900 loss: 1.081936627626419
