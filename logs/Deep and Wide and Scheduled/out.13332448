Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=4, fbank_dims=23, model_dims=256, concat=1, lr=0.002, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.4, beta1=0.9, beta2=0.999, exp=0.3)
Total number of model parameters is 5326888
EPOCH 1:
  batch 50 loss: 4.440292553901672
  batch 100 loss: 3.3346250867843628
  batch 150 loss: 3.2966888761520385
  batch 200 loss: 3.2816705465316773
  batch 250 loss: 3.234452428817749
  batch 300 loss: 3.1113258743286134
  batch 350 loss: 3.0711409854888916
  batch 400 loss: 3.0053793001174927
  batch 450 loss: 2.929987554550171
  batch 500 loss: 2.861620059013367
  batch 550 loss: 2.7840499448776246
  batch 600 loss: 2.7290078687667845
  batch 650 loss: 2.6697671937942506
  batch 700 loss: 2.6240099763870237
  batch 750 loss: 2.5225405645370484
  batch 800 loss: 2.4557949686050415
  batch 850 loss: 2.416235876083374
  batch 900 loss: 2.3458636808395386
LOSS train 2.34586 valid 2.26823, valid PER 74.08%
loss updated
EPOCH 2:
  batch 50 loss: 2.268106768131256
  batch 100 loss: 2.220746786594391
  batch 150 loss: 2.096440713405609
  batch 200 loss: 2.059043664932251
  batch 250 loss: 2.0414809131622316
  batch 300 loss: 1.9493135166168214
  batch 350 loss: 1.939879870414734
  batch 400 loss: 1.883611171245575
  batch 450 loss: 1.8474284744262695
  batch 500 loss: 1.8796854305267334
  batch 550 loss: 1.7957151103019715
  batch 600 loss: 1.7464521503448487
  batch 650 loss: 1.7326849913597107
  batch 700 loss: 1.6972418522834778
  batch 750 loss: 1.664581744670868
  batch 800 loss: 1.6407605481147767
  batch 850 loss: 1.6258830666542052
  batch 900 loss: 1.5649600553512573
LOSS train 1.56496 valid 1.51834, valid PER 49.74%
loss updated
EPOCH 3:
  batch 50 loss: 1.5163078784942627
  batch 100 loss: 1.5852329516410828
  batch 150 loss: 1.5319795632362365
  batch 200 loss: 1.4867420744895936
  batch 250 loss: 1.4625953936576843
  batch 300 loss: 1.4606836390495301
  batch 350 loss: 1.490396044254303
  batch 400 loss: 1.4134324812889099
  batch 450 loss: 1.4034588646888733
  batch 500 loss: 1.366479868888855
  batch 550 loss: 1.3663457798957825
  batch 600 loss: 1.337451000213623
  batch 650 loss: 1.362077763080597
  batch 700 loss: 1.312005910873413
  batch 750 loss: 1.342837824821472
  batch 800 loss: 1.338640398979187
  batch 850 loss: 1.3208228313922883
  batch 900 loss: 1.3178552746772767
LOSS train 1.31786 valid 1.26364, valid PER 38.87%
loss updated
EPOCH 4:
  batch 50 loss: 1.2848874807357789
  batch 100 loss: 1.2503481268882752
  batch 150 loss: 1.2684276723861694
  batch 200 loss: 1.2519600915908813
  batch 250 loss: 1.248711326122284
  batch 300 loss: 1.2269079411029815
  batch 350 loss: 1.2179922199249267
  batch 400 loss: 1.1904472529888153
  batch 450 loss: 1.1877649676799775
  batch 500 loss: 1.22130682349205
  batch 550 loss: 1.1649811100959777
  batch 600 loss: 1.1637372601032256
  batch 650 loss: 1.2145335698127746
  batch 700 loss: 1.204391816854477
  batch 750 loss: 1.176897703409195
  batch 800 loss: 1.162836607694626
  batch 850 loss: 1.1370398831367492
  batch 900 loss: 1.1474076688289643
LOSS train 1.14741 valid 1.13170, valid PER 35.00%
loss updated
EPOCH 5:
  batch 50 loss: 1.1242909502983094
  batch 100 loss: 1.1267730057239533
  batch 150 loss: 1.144534627199173
  batch 200 loss: 1.162736072540283
  batch 250 loss: 1.0797873175144195
  batch 300 loss: 1.10720445394516
  batch 350 loss: 1.0578879284858704
  batch 400 loss: 1.0484096097946167
  batch 450 loss: 1.0761728656291962
  batch 500 loss: 1.0453870344161986
  batch 550 loss: 1.097724620103836
