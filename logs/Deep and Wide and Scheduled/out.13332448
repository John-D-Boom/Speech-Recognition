Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=4, fbank_dims=23, model_dims=256, concat=1, lr=0.002, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.4, beta1=0.9, beta2=0.999, exp=0.3)
Total number of model parameters is 5326888
EPOCH 1:
  batch 50 loss: 4.440292553901672
  batch 100 loss: 3.3346250867843628
  batch 150 loss: 3.2966888761520385
  batch 200 loss: 3.2816705465316773
  batch 250 loss: 3.234452428817749
  batch 300 loss: 3.1113258743286134
  batch 350 loss: 3.0711409854888916
  batch 400 loss: 3.0053793001174927
  batch 450 loss: 2.929987554550171
  batch 500 loss: 2.861620059013367
  batch 550 loss: 2.7840499448776246
  batch 600 loss: 2.7290078687667845
  batch 650 loss: 2.6697671937942506
  batch 700 loss: 2.6240099763870237
  batch 750 loss: 2.5225405645370484
  batch 800 loss: 2.4557949686050415
  batch 850 loss: 2.416235876083374
  batch 900 loss: 2.3458636808395386
LOSS train 2.34586 valid 2.26823, valid PER 74.08%
loss updated
EPOCH 2:
  batch 50 loss: 2.268106768131256
  batch 100 loss: 2.220746786594391
  batch 150 loss: 2.096440713405609
  batch 200 loss: 2.059043664932251
  batch 250 loss: 2.0414809131622316
  batch 300 loss: 1.9493135166168214
  batch 350 loss: 1.939879870414734
  batch 400 loss: 1.883611171245575
  batch 450 loss: 1.8474284744262695
  batch 500 loss: 1.8796854305267334
  batch 550 loss: 1.7957151103019715
  batch 600 loss: 1.7464521503448487
  batch 650 loss: 1.7326849913597107
  batch 700 loss: 1.6972418522834778
  batch 750 loss: 1.664581744670868
  batch 800 loss: 1.6407605481147767
  batch 850 loss: 1.6258830666542052
  batch 900 loss: 1.5649600553512573
LOSS train 1.56496 valid 1.51834, valid PER 49.74%
loss updated
EPOCH 3:
  batch 50 loss: 1.5163078784942627
  batch 100 loss: 1.5852329516410828
  batch 150 loss: 1.5319795632362365
  batch 200 loss: 1.4867420744895936
  batch 250 loss: 1.4625953936576843
  batch 300 loss: 1.4606836390495301
  batch 350 loss: 1.490396044254303
  batch 400 loss: 1.4134324812889099
  batch 450 loss: 1.4034588646888733
  batch 500 loss: 1.366479868888855
  batch 550 loss: 1.3663457798957825
  batch 600 loss: 1.337451000213623
  batch 650 loss: 1.362077763080597
  batch 700 loss: 1.312005910873413
  batch 750 loss: 1.342837824821472
  batch 800 loss: 1.338640398979187
  batch 850 loss: 1.3208228313922883
  batch 900 loss: 1.3178552746772767
LOSS train 1.31786 valid 1.26364, valid PER 38.87%
loss updated
EPOCH 4:
  batch 50 loss: 1.2848874807357789
  batch 100 loss: 1.2503481268882752
  batch 150 loss: 1.2684276723861694
  batch 200 loss: 1.2519600915908813
  batch 250 loss: 1.248711326122284
  batch 300 loss: 1.2269079411029815
  batch 350 loss: 1.2179922199249267
  batch 400 loss: 1.1904472529888153
  batch 450 loss: 1.1877649676799775
  batch 500 loss: 1.22130682349205
  batch 550 loss: 1.1649811100959777
  batch 600 loss: 1.1637372601032256
  batch 650 loss: 1.2145335698127746
  batch 700 loss: 1.204391816854477
  batch 750 loss: 1.176897703409195
  batch 800 loss: 1.162836607694626
  batch 850 loss: 1.1370398831367492
  batch 900 loss: 1.1474076688289643
LOSS train 1.14741 valid 1.13170, valid PER 35.00%
loss updated
EPOCH 5:
  batch 50 loss: 1.1242909502983094
  batch 100 loss: 1.1267730057239533
  batch 150 loss: 1.144534627199173
  batch 200 loss: 1.162736072540283
  batch 250 loss: 1.0797873175144195
  batch 300 loss: 1.10720445394516
  batch 350 loss: 1.0578879284858704
  batch 400 loss: 1.0484096097946167
  batch 450 loss: 1.0761728656291962
  batch 500 loss: 1.0453870344161986
  batch 550 loss: 1.097724620103836
  batch 600 loss: 1.0842312574386597
  batch 650 loss: 1.0786688685417176
  batch 700 loss: 1.0770091450214385
  batch 750 loss: 1.0468560576438903
  batch 800 loss: 1.1126952850818634
  batch 850 loss: 1.0989112889766692
  batch 900 loss: 1.053299527168274
LOSS train 1.05330 valid 1.04303, valid PER 31.77%
loss updated
EPOCH 6:
  batch 50 loss: 1.031971263885498
  batch 100 loss: 1.0620396077632903
  batch 150 loss: 1.0178749358654022
  batch 200 loss: 0.9802368819713593
  batch 250 loss: 1.029651668071747
  batch 300 loss: 1.0431318819522857
  batch 350 loss: 1.0321662986278535
  batch 400 loss: 1.0061259806156158
  batch 450 loss: 1.027820200920105
  batch 500 loss: 1.004974194765091
  batch 550 loss: 1.0362319540977478
  batch 600 loss: 0.9862345325946807
  batch 650 loss: 0.9578194451332093
  batch 700 loss: 0.9623027276992798
  batch 750 loss: 1.0067783081531525
  batch 800 loss: 0.9829847776889801
  batch 850 loss: 0.9945630562305451
  batch 900 loss: 1.0256158792972565
LOSS train 1.02562 valid 0.98842, valid PER 31.04%
loss updated
EPOCH 7:
  batch 50 loss: 0.9619266140460968
  batch 100 loss: 0.9776090002059936
  batch 150 loss: 0.9414395272731781
  batch 200 loss: 0.936461614370346
  batch 250 loss: 0.992248250246048
  batch 300 loss: 0.9365054333209991
  batch 350 loss: 1.012945830821991
  batch 400 loss: 0.9469014751911163
  batch 450 loss: 0.9407553803920746
  batch 500 loss: 0.9554140257835388
  batch 550 loss: 0.9448227119445801
  batch 600 loss: 0.9376170241832733
  batch 650 loss: 0.9371590733528137
  batch 700 loss: 1.0662509000301361
  batch 750 loss: 0.9583113813400268
  batch 800 loss: 0.9241075253486634
  batch 850 loss: 0.9310260903835297
  batch 900 loss: 0.9535402071475982
LOSS train 0.95354 valid 0.94955, valid PER 29.70%
loss updated
EPOCH 8:
  batch 50 loss: 0.913300850391388
  batch 100 loss: 0.8860041880607605
  batch 150 loss: 0.9246676337718963
  batch 200 loss: 0.9057228732109069
  batch 250 loss: 0.8609396374225616
  batch 300 loss: 0.8763820421695709
  batch 350 loss: 0.8816751515865326
  batch 400 loss: 0.8789974117279052
  batch 450 loss: 0.9193971943855286
  batch 500 loss: 0.9174120545387268
  batch 550 loss: 0.8844322752952576
  batch 600 loss: 0.8614771914482117
  batch 650 loss: 0.8966222262382507
  batch 700 loss: 0.899668949842453
  batch 750 loss: 0.9008737945556641
  batch 800 loss: 0.8928946745395661
  batch 850 loss: 0.8644825768470764
  batch 900 loss: 0.8685851645469665
LOSS train 0.86859 valid 0.88383, valid PER 27.83%
loss updated
EPOCH 9:
  batch 50 loss: 0.8368453466892243
  batch 100 loss: 0.8389035415649414
  batch 150 loss: 0.858610007762909
  batch 200 loss: 0.8294056749343872
  batch 250 loss: 0.8175023865699768
  batch 300 loss: 0.8725723195075988
  batch 350 loss: 0.8890250635147094
  batch 400 loss: 0.8664488458633423
  batch 450 loss: 0.8925318825244903
  batch 500 loss: 0.8570957183837891
  batch 550 loss: 0.8657256424427032
  batch 600 loss: 0.8859774672985077
  batch 650 loss: 0.8623857772350312
  batch 700 loss: 0.8538164556026459
  batch 750 loss: 0.8456677484512329
  batch 800 loss: 0.8743518519401551
  batch 850 loss: 0.8638166415691376
  batch 900 loss: 0.8306733536720275
LOSS train 0.83067 valid 0.86095, valid PER 27.23%
loss updated
EPOCH 10:
  batch 50 loss: 0.8122567844390869
  batch 100 loss: 0.8415604358911515
  batch 150 loss: 0.8365407419204712
  batch 200 loss: 0.8243027365207672
  batch 250 loss: 0.8140001678466797
  batch 300 loss: 0.8022092032432556
  batch 350 loss: 0.796693139076233
  batch 400 loss: 0.7861298143863678
  batch 450 loss: 0.8662170624732971
  batch 500 loss: 0.8781414568424225
  batch 550 loss: 0.858293138742447
  batch 600 loss: 0.828655812740326
  batch 650 loss: 0.8452469313144684
  batch 700 loss: 0.8548797011375427
  batch 750 loss: 0.8748263251781464
  batch 800 loss: 0.856409957408905
  batch 850 loss: 0.7991998600959778
  batch 900 loss: 0.8104117441177369
LOSS train 0.81041 valid 0.90602, valid PER 27.32%
EPOCH 11:
  batch 50 loss: 0.8238059675693512
  batch 100 loss: 0.7630959475040435
  batch 150 loss: 0.8033505219221115
  batch 200 loss: 0.7515405929088592
  batch 250 loss: 0.7691206628084183
  batch 300 loss: 0.7675475120544434
  batch 350 loss: 0.820246341228485
  batch 400 loss: 0.7658155870437622
  batch 450 loss: 0.7841220438480377
  batch 500 loss: 0.7666771733760833
  batch 550 loss: 0.7962308704853058
  batch 600 loss: 0.796979068517685
  batch 650 loss: 0.8230432951450348
  batch 700 loss: 0.8780588483810425
  batch 750 loss: 0.8458455443382263
  batch 800 loss: 0.8212337672710419
  batch 850 loss: 0.8165824675559997
  batch 900 loss: 0.840620619058609
LOSS train 0.84062 valid 0.87189, valid PER 26.82%
EPOCH 12:
  batch 50 loss: 0.8152684414386749
  batch 100 loss: 0.7803338265419006
  batch 150 loss: 0.8086732578277588
  batch 200 loss: 0.8726833057403565
  batch 250 loss: 0.8161685299873352
  batch 300 loss: 0.8235620188713074
  batch 350 loss: 0.794416937828064
  batch 400 loss: 0.8327756464481354
  batch 450 loss: 0.7904560899734497
  batch 500 loss: 0.8179910886287689
  batch 550 loss: 0.8048884499073029
  batch 600 loss: 0.8105570089817047
  batch 650 loss: 0.7810785782337188
  batch 700 loss: 0.7855410635471344
  batch 750 loss: 0.8273787605762482
  batch 800 loss: 0.7715877425670624
  batch 850 loss: 0.8064161264896392
  batch 900 loss: 0.7938832640647888
LOSS train 0.79388 valid 0.83582, valid PER 26.08%
loss updated
EPOCH 13:
  batch 50 loss: 0.753155847787857
  batch 100 loss: 0.7584535926580429
  batch 150 loss: 0.7458254063129425
  batch 200 loss: 0.7134538543224335
  batch 250 loss: 0.7314160418510437
  batch 300 loss: 0.7669255620241165
  batch 350 loss: 0.7266312050819397
  batch 400 loss: 0.7574200838804245
  batch 450 loss: 0.7781144964694977
  batch 500 loss: 0.7584514939785003
  batch 550 loss: 0.7953681182861329
  batch 600 loss: 0.7600633656978607
  batch 650 loss: 0.7483770900964737
  batch 700 loss: 0.7584434378147126
  batch 750 loss: 0.7266103196144104
  batch 800 loss: 0.7468783748149872
  batch 850 loss: 0.702149623632431
  batch 900 loss: 0.7490113085508346
LOSS train 0.74901 valid 0.83104, valid PER 25.48%
loss updated
EPOCH 14:
  batch 50 loss: 0.7127095592021943
  batch 100 loss: 0.6880359131097794
  batch 150 loss: 0.7128226238489151
  batch 200 loss: 0.7114702028036117
  batch 250 loss: 0.7208980160951615
  batch 300 loss: 0.7122038328647613
  batch 350 loss: 0.7118586719036102
  batch 400 loss: 0.7278890490531922
  batch 450 loss: 0.7095660054683686
  batch 500 loss: 0.7098070019483567
  batch 550 loss: 0.7275856500864029
  batch 600 loss: 0.7293945211172104
  batch 650 loss: 0.7877358341217041
  batch 700 loss: 0.7540197479724884
  batch 750 loss: 0.747487758398056
  batch 800 loss: 0.7533154827356339
  batch 850 loss: 0.7797243499755859
  batch 900 loss: 0.7359136313199997
LOSS train 0.73591 valid 0.79468, valid PER 24.76%
loss updated
EPOCH 15:
  batch 50 loss: 0.687760591506958
  batch 100 loss: 0.7104441905021668
  batch 150 loss: 0.69464575111866
  batch 200 loss: 0.7278346526622772
  batch 250 loss: 0.7507460087537765
  batch 300 loss: 0.7354868173599243
  batch 350 loss: 0.7159886920452118
  batch 400 loss: 0.7123953157663345
  batch 450 loss: 0.700423218011856
  batch 500 loss: 0.6869415539503098
  batch 550 loss: 0.774417655467987
  batch 600 loss: 0.742582905292511
  batch 650 loss: 0.704966539144516
  batch 700 loss: 0.7015357059240341
  batch 750 loss: 0.7099804520606995
  batch 800 loss: 0.6997438883781433
  batch 850 loss: 0.6882883948087692
  batch 900 loss: 0.6749390161037445
LOSS train 0.67494 valid 0.80895, valid PER 24.92%
EPOCH 16:
  batch 50 loss: 0.6890890908241272
  batch 100 loss: 0.6722293388843537
  batch 150 loss: 0.6805100095272064
  batch 200 loss: 0.7071894264221191
  batch 250 loss: 0.6903886222839355
  batch 300 loss: 0.7233513367176055
  batch 350 loss: 0.6950337153673172
  batch 400 loss: 0.7235143280029297
  batch 450 loss: 0.7165474474430085
  batch 500 loss: 0.6973131000995636
  batch 550 loss: 0.6758283722400665
  batch 600 loss: 0.702157906293869
  batch 650 loss: 0.7065976482629776
  batch 700 loss: 0.6616336101293564
  batch 750 loss: 0.6928585356473923
  batch 800 loss: 0.6814709103107452
  batch 850 loss: 0.6999623781442642
  batch 900 loss: 0.7025515067577363
LOSS train 0.70255 valid 0.81124, valid PER 24.88%
EPOCH 17:
  batch 50 loss: 0.7020914208889008
  batch 100 loss: 0.6554243689775467
  batch 150 loss: 0.7325061857700348
  batch 200 loss: 0.6899534809589386
  batch 250 loss: 0.7278061759471893
  batch 300 loss: 0.6953882849216462
  batch 350 loss: 0.7093067061901093
  batch 400 loss: 0.7206607627868652
  batch 450 loss: 0.689563376903534
  batch 500 loss: 0.7103841173648834
  batch 550 loss: 0.7038685858249665
  batch 600 loss: 0.8209137010574341
  batch 650 loss: 0.7440324819087982
  batch 700 loss: 0.7259906899929046
  batch 750 loss: 0.650193738937378
  batch 800 loss: 0.706115130186081
  batch 850 loss: 0.6846861821413041
  batch 900 loss: 0.6879783624410629
LOSS train 0.68798 valid 0.76577, valid PER 23.69%
loss updated
EPOCH 18:
  batch 50 loss: 0.6420367443561554
  batch 100 loss: 0.6641420358419419
  batch 150 loss: 0.7053895771503449
  batch 200 loss: 0.6798914819955826
  batch 250 loss: 0.6815084499120713
  batch 300 loss: 0.6754749995470047
  batch 350 loss: 0.6272893863916397
  batch 400 loss: 0.6556839722394944
  batch 450 loss: 0.6624433529376984
  batch 500 loss: 0.6545665520429611
  batch 550 loss: 0.680196807384491
  batch 600 loss: 0.686137136220932
  batch 650 loss: 0.6414572387933731
  batch 700 loss: 0.6839177721738815
  batch 750 loss: 0.6583031928539276
  batch 800 loss: 0.7048649424314499
  batch 850 loss: 0.7023509347438812
  batch 900 loss: 0.6828464293479919
LOSS train 0.68285 valid 0.78383, valid PER 24.48%
EPOCH 19:
  batch 50 loss: 0.6450481867790222
  batch 100 loss: 0.6467721134424209
  batch 150 loss: 0.6237473815679551
  batch 200 loss: 0.626201793551445
  batch 250 loss: 0.6945341372489929
  batch 300 loss: 0.7067395782470703
  batch 350 loss: 0.6827302479743957
  batch 400 loss: 0.6731451570987701
  batch 450 loss: 0.6482014721632003
  batch 500 loss: 0.6368506073951721
  batch 550 loss: 0.659643931388855
  batch 600 loss: 0.6762495160102844
  batch 650 loss: 0.7052853667736053
  batch 700 loss: 0.7425512039661407
  batch 750 loss: 0.672009973526001
  batch 800 loss: 0.6270460468530655
  batch 850 loss: 0.6592819571495057
  batch 900 loss: 0.6387766349315643
LOSS train 0.63878 valid 0.77364, valid PER 23.38%
EPOCH 20:
  batch 50 loss: 0.6032522219419479
  batch 100 loss: 0.6297682410478592
  batch 150 loss: 0.6362554967403412
  batch 200 loss: 0.6250197738409042
  batch 250 loss: 0.660934329032898
  batch 300 loss: 0.6533144998550415
  batch 350 loss: 0.6329824447631835
  batch 400 loss: 0.652598368525505
  batch 450 loss: 0.6357619374990463
  batch 500 loss: 0.6584740400314331
  batch 550 loss: 0.6913860183954239
  batch 600 loss: 0.632435238957405
  batch 650 loss: 0.6626286596059799
  batch 700 loss: 0.6440555101633072
  batch 750 loss: 0.6497866576910019
  batch 800 loss: 0.662131301164627
  batch 850 loss: 0.6528965991735458
  batch 900 loss: 0.6405783730745316
LOSS train 0.64058 valid 0.77554, valid PER 23.35%
[2.3458636808395386, 1.5649600553512573, 1.3178552746772767, 1.1474076688289643, 1.053299527168274, 1.0256158792972565, 0.9535402071475982, 0.8685851645469665, 0.8306733536720275, 0.8104117441177369, 0.840620619058609, 0.7938832640647888, 0.7490113085508346, 0.7359136313199997, 0.6749390161037445, 0.7025515067577363, 0.6879783624410629, 0.6828464293479919, 0.6387766349315643, 0.6405783730745316]
[2.2682275772094727, 1.5183401107788086, 1.2636399269104004, 1.1317023038864136, 1.043032169342041, 0.9884155988693237, 0.9495545029640198, 0.8838332891464233, 0.8609480857849121, 0.9060195088386536, 0.8718869686126709, 0.8358249664306641, 0.8310351967811584, 0.7946816682815552, 0.8089450001716614, 0.8112425208091736, 0.7657710909843445, 0.7838318347930908, 0.7736392021179199, 0.7755430936813354]
Training finished in 28.0 minutes.
Model saved to checkpoints/20230125_221344/model_17
Loading model from checkpoints/20230125_221344/model_17
SUB: 15.77%, DEL: 6.84%, INS: 2.42%, COR: 77.39%, PER: 25.02%
