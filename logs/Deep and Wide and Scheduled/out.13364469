Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=4, fbank_dims=23, model_dims=256, concat=1, lr=0.00025, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.4, beta1=0.9, beta2=0.999, exp=0.5)
Total number of model parameters is 5326888
EPOCH 1:
  batch 50 loss: 8.176148562431335
  batch 100 loss: 3.315859360694885
  batch 150 loss: 3.3072510528564454
  batch 200 loss: 3.2850173568725585
  batch 250 loss: 3.259261703491211
  batch 300 loss: 3.232748312950134
  batch 350 loss: 3.21552743434906
  batch 400 loss: 3.1791069507598877
  batch 450 loss: 3.038005895614624
  batch 500 loss: 2.870350551605225
  batch 550 loss: 2.752784013748169
  batch 600 loss: 2.6850853776931762
  batch 650 loss: 2.5794028425216675
  batch 700 loss: 2.5172103452682495
  batch 750 loss: 2.412771215438843
  batch 800 loss: 2.341560082435608
  batch 850 loss: 2.237090139389038
  batch 900 loss: 2.1515001249313355
LOSS train 2.15150 valid 2.04477, valid PER 71.86%
loss updated
EPOCH 2:
  batch 50 loss: 2.0369052410125734
  batch 100 loss: 1.958099799156189
  batch 150 loss: 1.8325645709037781
  batch 200 loss: 1.792456247806549
  batch 250 loss: 1.754241120815277
  batch 300 loss: 1.6601469349861144
  batch 350 loss: 1.6477009320259095
  batch 400 loss: 1.601105351448059
  batch 450 loss: 1.5408763957023621
  batch 500 loss: 1.5247275447845459
  batch 550 loss: 1.4654763269424438
  batch 600 loss: 1.4126466393470765
  batch 650 loss: 1.373306200504303
  batch 700 loss: 1.3672129535675048
  batch 750 loss: 1.3502649092674255
  batch 800 loss: 1.292468798160553
  batch 850 loss: 1.2816424667835236
  batch 900 loss: 1.2660025906562806
LOSS train 1.26600 valid 1.20412, valid PER 39.37%
loss updated
EPOCH 3:
  batch 50 loss: 1.184024442434311
  batch 100 loss: 1.2334143400192261
  batch 150 loss: 1.2239573335647582
  batch 200 loss: 1.161519123315811
  batch 250 loss: 1.1551940310001374
  batch 300 loss: 1.1408415889739991
  batch 350 loss: 1.1603338813781738
  batch 400 loss: 1.1210127317905425
  batch 450 loss: 1.0977110433578492
  batch 500 loss: 1.1065698707103728
  batch 550 loss: 1.0960647225379945
  batch 600 loss: 1.0347771322727204
  batch 650 loss: 1.0758575105667114
  batch 700 loss: 1.0581156480312348
  batch 750 loss: 1.0466388845443726
  batch 800 loss: 1.1051708436012269
  batch 850 loss: 1.0368439328670502
  batch 900 loss: 1.0556594622135163
LOSS train 1.05566 valid 1.01398, valid PER 31.64%
loss updated
EPOCH 4:
  batch 50 loss: 1.0272324895858764
  batch 100 loss: 0.96231618642807
  batch 150 loss: 0.9795436000823975
  batch 200 loss: 0.9678631198406219
  batch 250 loss: 0.9824112594127655
  batch 300 loss: 0.9840815448760987
  batch 350 loss: 0.9629186427593232
  batch 400 loss: 0.9240986478328704
  batch 450 loss: 0.9268768548965454
  batch 500 loss: 0.9977803432941437
  batch 550 loss: 0.9208561265468598
  batch 600 loss: 0.9290451383590699
  batch 650 loss: 0.9685221886634827
  batch 700 loss: 0.943891339302063
  batch 750 loss: 0.9468367636203766
  batch 800 loss: 0.9241825556755066
  batch 850 loss: 0.9149751830101013
  batch 900 loss: 0.9233909738063812
LOSS train 0.92339 valid 0.93464, valid PER 28.68%
loss updated
EPOCH 5:
  batch 50 loss: 0.864403269290924
  batch 100 loss: 0.8484999227523804
  batch 150 loss: 0.8740248000621795
  batch 200 loss: 0.8850631380081176
  batch 250 loss: 0.8385819125175477
  batch 300 loss: 0.879221168756485
  batch 350 loss: 0.8311571383476257
  batch 400 loss: 0.807484689950943
  batch 450 loss: 0.8350182521343231
  batch 500 loss: 0.8185031890869141
  batch 550 loss: 0.8759360790252686
  batch 600 loss: 0.856440327167511
  batch 650 loss: 0.8787600040435791
  batch 700 loss: 0.845193909406662
  batch 750 loss: 0.8316190314292907
  batch 800 loss: 0.8684749352931976
  batch 850 loss: 0.8588569939136506
  batch 900 loss: 0.8200245296955109
LOSS train 0.82002 valid 0.88421, valid PER 27.32%
loss updated
EPOCH 6:
  batch 50 loss: 0.7913597047328949
  batch 100 loss: 0.7864129030704499
  batch 150 loss: 0.7762969088554382
  batch 200 loss: 0.7560282456874847
  batch 250 loss: 0.7485624420642852
  batch 300 loss: 0.7995266234874725
  batch 350 loss: 0.794394987821579
  batch 400 loss: 0.7728770112991333
  batch 450 loss: 0.7988349044322968
  batch 500 loss: 0.7448418986797333
  batch 550 loss: 0.786284276843071
  batch 600 loss: 0.7744205957651138
  batch 650 loss: 0.7181835842132568
  batch 700 loss: 0.7616341841220856
  batch 750 loss: 0.779670774936676
  batch 800 loss: 0.7513606756925583
  batch 850 loss: 0.7971025788784027
  batch 900 loss: 0.7987588649988174
LOSS train 0.79876 valid 0.82081, valid PER 25.18%
loss updated
EPOCH 7:
  batch 50 loss: 0.7040347892045975
  batch 100 loss: 0.7550350975990295
  batch 150 loss: 0.7087929975986481
  batch 200 loss: 0.6959106129407883
  batch 250 loss: 0.7486223328113556
  batch 300 loss: 0.7166288197040558
  batch 350 loss: 0.7327252995967865
  batch 400 loss: 0.6991439181566238
  batch 450 loss: 0.6994118028879166
  batch 500 loss: 0.707067790031433
  batch 550 loss: 0.7147842109203338
  batch 600 loss: 0.7046471285820007
  batch 650 loss: 0.6919116109609604
  batch 700 loss: 0.709550171494484
  batch 750 loss: 0.7299239772558213
  batch 800 loss: 0.6965265202522278
  batch 850 loss: 0.6891656643152237
  batch 900 loss: 0.708275921344757
LOSS train 0.70828 valid 0.77660, valid PER 24.52%
loss updated
EPOCH 8:
  batch 50 loss: 0.6676687014102936
  batch 100 loss: 0.6251195961236954
  batch 150 loss: 0.6732590276002884
  batch 200 loss: 0.657882056236267
  batch 250 loss: 0.6507586520910263
  batch 300 loss: 0.628611575961113
  batch 350 loss: 0.645244008898735
  batch 400 loss: 0.6341371327638626
  batch 450 loss: 0.6903970897197723
  batch 500 loss: 0.6556297504901886
  batch 550 loss: 0.6752728581428528
  batch 600 loss: 0.6392607396841049
  batch 650 loss: 0.6418498980998993
  batch 700 loss: 0.6701647835969925
  batch 750 loss: 0.6643959164619446
  batch 800 loss: 0.6729165291786194
  batch 850 loss: 0.6548561841249466
  batch 900 loss: 0.666279503107071
LOSS train 0.66628 valid 0.75519, valid PER 23.08%
loss updated
EPOCH 9:
  batch 50 loss: 0.5883377540111542
  batch 100 loss: 0.5859565168619156
  batch 150 loss: 0.6031214594841003
  batch 200 loss: 0.5975137037038804
  batch 250 loss: 0.5734345781803131
  batch 300 loss: 0.6203586316108703
  batch 350 loss: 0.5881945610046386
  batch 400 loss: 0.6218972104787827
  batch 450 loss: 0.6285327988862991
  batch 500 loss: 0.606412160396576
  batch 550 loss: 0.6098181265592575
  batch 600 loss: 0.6382931530475616
  batch 650 loss: 0.6361132025718689
  batch 700 loss: 0.5934155666828156
  batch 750 loss: 0.62714708507061
  batch 800 loss: 0.6408719807863236
  batch 850 loss: 0.646306083202362
  batch 900 loss: 0.5908118486404419
LOSS train 0.59081 valid 0.74354, valid PER 22.71%
loss updated
EPOCH 10:
  batch 50 loss: 0.5430250716209412
  batch 100 loss: 0.5584729504585266
  batch 150 loss: 0.5643061882257462
  batch 200 loss: 0.5454043608903885
  batch 250 loss: 0.5525410747528077
  batch 300 loss: 0.5730363917350769
  batch 350 loss: 0.5551225817203522
  batch 400 loss: 0.5448624283075333
  batch 450 loss: 0.5514195102453232
  batch 500 loss: 0.5506781148910522
  batch 550 loss: 0.5699412989616394
  batch 600 loss: 0.574399710893631
  batch 650 loss: 0.5789387047290802
  batch 700 loss: 0.5651973760128022
  batch 750 loss: 0.5995993077754974
  batch 800 loss: 0.5917353737354278
  batch 850 loss: 0.5582265198230744
  batch 900 loss: 0.581338894367218
LOSS train 0.58134 valid 0.74839, valid PER 22.67%
EPOCH 11:
  batch 50 loss: 0.5252145040035248
  batch 100 loss: 0.49951962411403655
  batch 150 loss: 0.5099026554822922
  batch 200 loss: 0.474778328537941
  batch 250 loss: 0.5084795874357223
  batch 300 loss: 0.5126077663898468
  batch 350 loss: 0.5472343695163727
  batch 400 loss: 0.521020747423172
  batch 450 loss: 0.5108375698328018
  batch 500 loss: 0.5214825528860092
  batch 550 loss: 0.5410926675796509
  batch 600 loss: 0.5255432826280594
  batch 650 loss: 0.5314702254533767
  batch 700 loss: 0.565099949836731
  batch 750 loss: 0.5141495883464813
  batch 800 loss: 0.5481700032949448
  batch 850 loss: 0.5355114996433258
  batch 900 loss: 0.5460163187980652
LOSS train 0.54602 valid 0.71211, valid PER 21.42%
loss updated
EPOCH 12:
  batch 50 loss: 0.4668834620714188
  batch 100 loss: 0.4474216878414154
  batch 150 loss: 0.4769099199771881
  batch 200 loss: 0.4808726471662521
  batch 250 loss: 0.48719782710075377
  batch 300 loss: 0.4939450389146805
  batch 350 loss: 0.46577832341194153
  batch 400 loss: 0.5059492146968841
  batch 450 loss: 0.4780126529932022
  batch 500 loss: 0.49710742831230165
  batch 550 loss: 0.48569712579250335
  batch 600 loss: 0.493886690735817
  batch 650 loss: 0.5046959209442139
  batch 700 loss: 0.5002315801382065
  batch 750 loss: 0.4904084247350693
  batch 800 loss: 0.4710737371444702
  batch 850 loss: 0.4926238423585892
  batch 900 loss: 0.5140617340803146
LOSS train 0.51406 valid 0.71152, valid PER 21.10%
loss updated
EPOCH 13:
  batch 50 loss: 0.431917924284935
  batch 100 loss: 0.4328400644659996
  batch 150 loss: 0.43309751510620115
  batch 200 loss: 0.41550157725811004
  batch 250 loss: 0.431579584479332
  batch 300 loss: 0.4667560091614723
  batch 350 loss: 0.41634453684091566
  batch 400 loss: 0.4448744511604309
  batch 450 loss: 0.4561796373128891
  batch 500 loss: 0.43996107697486875
  batch 550 loss: 0.4714632698893547
  batch 600 loss: 0.4577676716446877
  batch 650 loss: 0.46652921438217165
  batch 700 loss: 0.48552908360958097
  batch 750 loss: 0.4434551140666008
  batch 800 loss: 0.46140736758708956
  batch 850 loss: 0.4586261489987373
  batch 900 loss: 0.47692919850349424
LOSS train 0.47693 valid 0.71438, valid PER 21.12%
EPOCH 14:
  batch 50 loss: 0.40188138246536254
  batch 100 loss: 0.39778551042079924
  batch 150 loss: 0.4043009123206139
  batch 200 loss: 0.41750380873680115
  batch 250 loss: 0.39513166546821593
  batch 300 loss: 0.42490643471479417
  batch 350 loss: 0.42183222353458405
  batch 400 loss: 0.4358305591344833
  batch 450 loss: 0.4017787000536919
  batch 500 loss: 0.4257101774215698
  batch 550 loss: 0.4328363734483719
  batch 600 loss: 0.4152321571111679
  batch 650 loss: 0.4362223821878433
  batch 700 loss: 0.4547476780414581
  batch 750 loss: 0.4467534023523331
  batch 800 loss: 0.42927263855934145
  batch 850 loss: 0.45038821280002594
  batch 900 loss: 0.4372579142451286
LOSS train 0.43726 valid 0.73141, valid PER 21.16%
EPOCH 15:
  batch 50 loss: 0.3667449426651001
  batch 100 loss: 0.37297160655260087
  batch 150 loss: 0.39776379346847535
  batch 200 loss: 0.39258884340524675
  batch 250 loss: 0.3890078270435333
  batch 300 loss: 0.39887186110019685
  batch 350 loss: 0.38819412887096405
  batch 400 loss: 0.3728617352247238
  batch 450 loss: 0.38195530116558074
  batch 500 loss: 0.38075878262519836
  batch 550 loss: 0.4165400773286819
  batch 600 loss: 0.44318084567785265
  batch 650 loss: 0.4232784539461136
  batch 700 loss: 0.39689601451158524
  batch 750 loss: 0.40467564582824705
  batch 800 loss: 0.37689292907714844
  batch 850 loss: 0.38414983451366425
  batch 900 loss: 0.3820368456840515
LOSS train 0.38204 valid 0.70216, valid PER 20.28%
loss updated
EPOCH 16:
  batch 50 loss: 0.34924048364162447
  batch 100 loss: 0.33423298805952073
  batch 150 loss: 0.3508515805006027
  batch 200 loss: 0.3718207985162735
  batch 250 loss: 0.35016758561134337
  batch 300 loss: 0.36422750651836394
  batch 350 loss: 0.3631940659880638
  batch 400 loss: 0.36920177906751633
  batch 450 loss: 0.38077605456113817
  batch 500 loss: 0.3493205976486206
  batch 550 loss: 0.3409692174196243
  batch 600 loss: 0.38625291496515274
  batch 650 loss: 0.38895161390304567
  batch 700 loss: 0.3635468739271164
  batch 750 loss: 0.377059628367424
  batch 800 loss: 0.36963427037000657
  batch 850 loss: 0.38447679072618485
  batch 900 loss: 0.37076465398073194
LOSS train 0.37076 valid 0.71485, valid PER 20.14%
EPOCH 17:
  batch 50 loss: 0.33657381772994993
  batch 100 loss: 0.29040361374616624
  batch 150 loss: 0.3394534096121788
  batch 200 loss: 0.32295623555779457
  batch 250 loss: 0.33551146715879443
  batch 300 loss: 0.31269378304481504
  batch 350 loss: 0.35868083596229555
  batch 400 loss: 0.3389528328180313
  batch 450 loss: 0.33391607433557513
  batch 500 loss: 0.3498194074630737
  batch 550 loss: 0.35333178281784056
  batch 600 loss: 0.3766709154844284
  batch 650 loss: 0.33353355526924133
  batch 700 loss: 0.36597256183624266
  batch 750 loss: 0.32733027279376986
  batch 800 loss: 0.346647142469883
  batch 850 loss: 0.36246629476547243
  batch 900 loss: 0.3521921044588089
LOSS train 0.35219 valid 0.72488, valid PER 19.90%
EPOCH 18:
  batch 50 loss: 0.27870228737592695
  batch 100 loss: 0.2903424060344696
  batch 150 loss: 0.3329436585307121
  batch 200 loss: 0.29571456626057624
  batch 250 loss: 0.2946029832959175
  batch 300 loss: 0.2921740025281906
  batch 350 loss: 0.28667340725660323
  batch 400 loss: 0.2888089555501938
  batch 450 loss: 0.3251441362500191
  batch 500 loss: 0.31959920465946196
  batch 550 loss: 0.33231322437524796
  batch 600 loss: 0.31623953074216843
  batch 650 loss: 0.3135726135969162
  batch 700 loss: 0.3281753128767014
  batch 750 loss: 0.3248465013504028
  batch 800 loss: 0.34288831532001496
  batch 850 loss: 0.33219686537981036
  batch 900 loss: 0.3127069133520126
LOSS train 0.31271 valid 0.73602, valid PER 20.18%
EPOCH 19:
  batch 50 loss: 0.28365630328655245
  batch 100 loss: 0.28315404564142227
  batch 150 loss: 0.28410690009593964
  batch 200 loss: 0.2640327787399292
  batch 250 loss: 0.29543441444635393
  batch 300 loss: 0.30474090605974197
  batch 350 loss: 0.2954631194472313
  batch 400 loss: 0.2849903044104576
  batch 450 loss: 0.26678253829479215
  batch 500 loss: 0.2930678150057793
  batch 550 loss: 0.30645854949951173
  batch 600 loss: 0.3078090983629227
  batch 650 loss: 0.3242448687553406
  batch 700 loss: 0.3306882384419441
  batch 750 loss: 0.29688006281852725
  batch 800 loss: 0.3025127094984055
  batch 850 loss: 0.3252007731795311
  batch 900 loss: 0.3088488492369652
LOSS train 0.30885 valid 0.75050, valid PER 20.23%
[2.1515001249313355, 1.2660025906562806, 1.0556594622135163, 0.9233909738063812, 0.8200245296955109, 0.7987588649988174, 0.708275921344757, 0.666279503107071, 0.5908118486404419, 0.581338894367218, 0.5460163187980652, 0.5140617340803146, 0.47692919850349424, 0.4372579142451286, 0.3820368456840515, 0.37076465398073194, 0.3521921044588089, 0.3127069133520126, 0.3088488492369652]
[2.0447709560394287, 1.204124093055725, 1.0139755010604858, 0.9346422553062439, 0.8842095136642456, 0.8208120465278625, 0.7766042947769165, 0.7551911473274231, 0.7435371279716492, 0.7483921051025391, 0.7121105790138245, 0.7115241289138794, 0.7143787145614624, 0.7314075231552124, 0.7021574378013611, 0.7148465514183044, 0.7248796820640564, 0.7360247373580933, 0.7504990696907043]
Training finished in 21.0 minutes.
Model saved to checkpoints/20230125_233754/model_15
Loading model from checkpoints/20230125_233754/model_15
SUB: 14.03%, DEL: 5.91%, INS: 2.52%, COR: 80.06%, PER: 22.45%
