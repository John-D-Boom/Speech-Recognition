Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=4, fbank_dims=23, model_dims=256, concat=1, lr=0.0005, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.4, beta1=0.9, beta2=0.999, exp=0.5)
Total number of model parameters is 5326888
EPOCH 1:
  batch 50 loss: 6.197421236038208
  batch 100 loss: 3.311358766555786
  batch 150 loss: 3.2904750394821165
  batch 200 loss: 3.2685946655273437
  batch 250 loss: 3.2335411643981935
  batch 300 loss: 3.1523394536972047
  batch 350 loss: 3.007188868522644
  batch 400 loss: 2.8758276224136354
  batch 450 loss: 2.8096540069580076
  batch 500 loss: 2.649870433807373
  batch 550 loss: 2.5437899255752563
  batch 600 loss: 2.46619900226593
  batch 650 loss: 2.3270573472976683
  batch 700 loss: 2.2587202692031862
  batch 750 loss: 2.1456760478019716
  batch 800 loss: 2.0789857721328735
  batch 850 loss: 1.952044539451599
  batch 900 loss: 1.8684845876693725
LOSS train 1.86848 valid 1.78037, valid PER 65.16%
loss updated
EPOCH 2:
  batch 50 loss: 1.7890923976898194
  batch 100 loss: 1.7172256422042846
  batch 150 loss: 1.6201803946495057
  batch 200 loss: 1.56962899684906
  batch 250 loss: 1.5584267640113831
  batch 300 loss: 1.492244439125061
  batch 350 loss: 1.4650167560577392
  batch 400 loss: 1.4401275634765625
  batch 450 loss: 1.3948501825332642
  batch 500 loss: 1.3632728743553162
  batch 550 loss: 1.3371676361560823
  batch 600 loss: 1.2964635014533996
  batch 650 loss: 1.2522656941413879
  batch 700 loss: 1.278703887462616
  batch 750 loss: 1.2410805201530457
  batch 800 loss: 1.225130376815796
  batch 850 loss: 1.1850837743282319
  batch 900 loss: 1.172775011062622
LOSS train 1.17278 valid 1.14070, valid PER 36.05%
loss updated
EPOCH 3:
  batch 50 loss: 1.0742891597747803
  batch 100 loss: 1.1668288838863372
  batch 150 loss: 1.1620353496074676
  batch 200 loss: 1.1016271591186524
  batch 250 loss: 1.1025315141677856
  batch 300 loss: 1.0814762008190155
  batch 350 loss: 1.0843561136722564
  batch 400 loss: 1.0617947888374328
  batch 450 loss: 1.0520997190475463
  batch 500 loss: 1.0521132409572602
  batch 550 loss: 1.0525872814655304
  batch 600 loss: 0.997663596868515
  batch 650 loss: 1.0543705701828003
  batch 700 loss: 1.0318691849708557
  batch 750 loss: 1.0234781539440154
  batch 800 loss: 1.0570302891731262
  batch 850 loss: 0.9875575184822083
  batch 900 loss: 1.0172749114036561
LOSS train 1.01727 valid 0.98575, valid PER 30.94%
loss updated
EPOCH 4:
  batch 50 loss: 1.0005924904346466
  batch 100 loss: 0.9192819666862487
  batch 150 loss: 0.9556961727142333
  batch 200 loss: 0.93512570977211
  batch 250 loss: 0.9442008125782013
  batch 300 loss: 0.9623365378379822
  batch 350 loss: 0.9090073680877686
  batch 400 loss: 0.876554970741272
  batch 450 loss: 0.8930286836624145
  batch 500 loss: 0.99744624376297
  batch 550 loss: 0.8871398735046386
  batch 600 loss: 0.8751964950561524
  batch 650 loss: 0.9155085396766662
  batch 700 loss: 0.9235872185230255
  batch 750 loss: 0.9295758652687073
  batch 800 loss: 0.8998658633232117
  batch 850 loss: 0.8949547016620636
  batch 900 loss: 0.9300707805156708
LOSS train 0.93007 valid 0.89796, valid PER 27.68%
loss updated
EPOCH 5:
  batch 50 loss: 0.8584387183189393
  batch 100 loss: 0.8551224732398987
  batch 150 loss: 0.8636829102039337
  batch 200 loss: 0.8893438613414765
  batch 250 loss: 0.8330391299724579
  batch 300 loss: 0.8708072078227996
  batch 350 loss: 0.8380686485767365
  batch 400 loss: 0.8293675827980042
  batch 450 loss: 0.8277449703216553
  batch 500 loss: 0.7961467444896698
  batch 550 loss: 0.8767845106124877
  batch 600 loss: 0.835423663854599
  batch 650 loss: 0.8540261745452881
  batch 700 loss: 0.8071729922294617
  batch 750 loss: 0.80562260389328
  batch 800 loss: 0.8543548655509948
  batch 850 loss: 0.8446592080593109
  batch 900 loss: 0.7958855259418488
LOSS train 0.79589 valid 0.84935, valid PER 26.16%
loss updated
EPOCH 6:
  batch 50 loss: 0.7819478392601014
  batch 100 loss: 0.7939249289035797
  batch 150 loss: 0.7668665754795074
  batch 200 loss: 0.7645639073848725
  batch 250 loss: 0.7692936420440674
  batch 300 loss: 0.7969531702995301
  batch 350 loss: 0.7845739829540253
  batch 400 loss: 0.766634134054184
  batch 450 loss: 0.7974668252468109
  batch 500 loss: 0.7353703171014786
  batch 550 loss: 0.7527937060594558
  batch 600 loss: 0.7423419684171677
  batch 650 loss: 0.7103791558742523
  batch 700 loss: 0.7465991371870041
  batch 750 loss: 0.7640289700031281
  batch 800 loss: 0.7423055171966553
  batch 850 loss: 0.7806254196166992
  batch 900 loss: 0.7912274515628814
LOSS train 0.79123 valid 0.83856, valid PER 25.85%
loss updated
EPOCH 7:
  batch 50 loss: 0.741247341632843
  batch 100 loss: 0.7717908954620362
  batch 150 loss: 0.7041164606809616
  batch 200 loss: 0.6828818666934967
  batch 250 loss: 0.7661532759666443
  batch 300 loss: 0.7232134473323822
  batch 350 loss: 0.7518389868736267
  batch 400 loss: 0.6995139348506928
  batch 450 loss: 0.7013881897926331
  batch 500 loss: 0.688472672700882
  batch 550 loss: 0.7180842661857605
  batch 600 loss: 0.7005543410778046
  batch 650 loss: 0.6972402584552765
  batch 700 loss: 0.7282550519704819
  batch 750 loss: 0.729821858406067
  batch 800 loss: 0.7115623176097869
  batch 850 loss: 0.6979992163181304
  batch 900 loss: 0.7131641763448715
LOSS train 0.71316 valid 0.78935, valid PER 24.74%
loss updated
EPOCH 8:
  batch 50 loss: 0.6566346299648285
  batch 100 loss: 0.6337084954977036
  batch 150 loss: 0.6768138039112092
  batch 200 loss: 0.6687566137313843
  batch 250 loss: 0.6810846322774887
  batch 300 loss: 0.6506383669376373
  batch 350 loss: 0.6510197603702546
  batch 400 loss: 0.6673761177062988
  batch 450 loss: 0.7222875213623047
  batch 500 loss: 0.6808669185638427
  batch 550 loss: 0.6910158634185791
  batch 600 loss: 0.6606664007902145
  batch 650 loss: 0.6502606081962585
  batch 700 loss: 0.6697486174106598
  batch 750 loss: 0.6793442630767822
  batch 800 loss: 0.6903089171648026
  batch 850 loss: 0.6793777912855148
  batch 900 loss: 0.6674501514434814
LOSS train 0.66745 valid 0.79007, valid PER 24.64%
EPOCH 9:
  batch 50 loss: 0.6433678072690964
  batch 100 loss: 0.5897644048929215
  batch 150 loss: 0.622968841791153
  batch 200 loss: 0.6210473555326462
  batch 250 loss: 0.5838523018360138
  batch 300 loss: 0.6387600666284561
  batch 350 loss: 0.604535955786705
  batch 400 loss: 0.6224757850170135
  batch 450 loss: 0.6299678146839142
  batch 500 loss: 0.6190170693397522
  batch 550 loss: 0.6227255129814148
  batch 600 loss: 0.6514061105251312
  batch 650 loss: 0.6683742040395737
  batch 700 loss: 0.6329138451814651
  batch 750 loss: 0.6255449217557907
  batch 800 loss: 0.6477119648456573
  batch 850 loss: 0.6306312710046769
  batch 900 loss: 0.6049419665336608
LOSS train 0.60494 valid 0.73695, valid PER 23.05%
loss updated
EPOCH 10:
  batch 50 loss: 0.5699529159069061
  batch 100 loss: 0.5774674713611603
  batch 150 loss: 0.5883723133802414
  batch 200 loss: 0.5581678640842438
  batch 250 loss: 0.6112867695093155
  batch 300 loss: 0.6105398947000503
  batch 350 loss: 0.5940867710113525
  batch 400 loss: 0.5901664465665817
  batch 450 loss: 0.5785963714122773
  batch 500 loss: 0.5624748915433884
  batch 550 loss: 0.5933297663927078
  batch 600 loss: 0.5911746495962142
  batch 650 loss: 0.6209876346588135
  batch 700 loss: 0.6000355732440948
  batch 750 loss: 0.6212966883182526
  batch 800 loss: 0.5984419256448745
  batch 850 loss: 0.5759365254640579
  batch 900 loss: 0.5900156706571579
LOSS train 0.59002 valid 0.71040, valid PER 21.88%
loss updated
EPOCH 11:
  batch 50 loss: 0.5575416541099548
  batch 100 loss: 0.5439172959327698
  batch 150 loss: 0.5470248401165009
  batch 200 loss: 0.4811324155330658
  batch 250 loss: 0.5344967079162598
  batch 300 loss: 0.5220905637741089
  batch 350 loss: 0.5678882908821106
  batch 400 loss: 0.5555420470237732
  batch 450 loss: 0.589711702466011
  batch 500 loss: 0.5704436701536179
  batch 550 loss: 0.5831772589683533
  batch 600 loss: 0.554469456076622
  batch 650 loss: 0.6002744793891907
  batch 700 loss: 0.6176973432302475
  batch 750 loss: 0.5584001940488815
  batch 800 loss: 0.5988677895069122
  batch 850 loss: 0.5906041878461837
  batch 900 loss: 0.6262450361251831
LOSS train 0.62625 valid 0.72801, valid PER 22.55%
EPOCH 12:
  batch 50 loss: 0.5364553320407868
  batch 100 loss: 0.5002094745635987
  batch 150 loss: 0.5069502156972885
  batch 200 loss: 0.5162203013896942
  batch 250 loss: 0.5118524938821792
  batch 300 loss: 0.5434316563606262
  batch 350 loss: 0.5193197780847549
  batch 400 loss: 0.5732890433073043
  batch 450 loss: 0.5430145275592804
  batch 500 loss: 0.5420794218778611
  batch 550 loss: 0.5404715615510941
  batch 600 loss: 0.5268281942605972
  batch 650 loss: 0.5037066632509232
  batch 700 loss: 0.5330497026443481
  batch 750 loss: 0.5321040695905686
  batch 800 loss: 0.5340840512514115
  batch 850 loss: 0.5363617676496506
  batch 900 loss: 0.5598942971229554
LOSS train 0.55989 valid 0.71791, valid PER 21.38%
EPOCH 13:
  batch 50 loss: 0.46993153274059296
  batch 100 loss: 0.48630465418100355
  batch 150 loss: 0.4794176596403122
  batch 200 loss: 0.47324433565139773
  batch 250 loss: 0.48348522424697876
  batch 300 loss: 0.5022528213262558
  batch 350 loss: 0.4755768465995789
  batch 400 loss: 0.4978391182422638
  batch 450 loss: 0.5094133532047271
  batch 500 loss: 0.5030080497264862
  batch 550 loss: 0.5455249351263046
  batch 600 loss: 0.5265596568584442
  batch 650 loss: 0.514507954120636
  batch 700 loss: 0.5448653888702393
  batch 750 loss: 0.5125765830278397
  batch 800 loss: 0.5037576323747635
  batch 850 loss: 0.47830682069063185
  batch 900 loss: 0.5208769503235817
LOSS train 0.52088 valid 0.66671, valid PER 20.28%
loss updated
EPOCH 14:
  batch 50 loss: 0.4670820373296738
  batch 100 loss: 0.44455630838871
  batch 150 loss: 0.4639683532714844
  batch 200 loss: 0.45896956115961074
  batch 250 loss: 0.4296108719706535
  batch 300 loss: 0.4371927946805954
  batch 350 loss: 0.4584654426574707
  batch 400 loss: 0.4771735745668411
  batch 450 loss: 0.43632995426654814
  batch 500 loss: 0.4601207119226456
  batch 550 loss: 0.46928091764450075
  batch 600 loss: 0.47358394145965577
  batch 650 loss: 0.46880489885807036
  batch 700 loss: 0.46177723944187166
  batch 750 loss: 0.47945886313915254
  batch 800 loss: 0.4650488388538361
  batch 850 loss: 0.4947284382581711
  batch 900 loss: 0.48286264061927797
LOSS train 0.48286 valid 0.69622, valid PER 20.67%
EPOCH 15:
  batch 50 loss: 0.41186471104621886
  batch 100 loss: 0.42963600367307664
  batch 150 loss: 0.44361538767814634
  batch 200 loss: 0.4405120986700058
  batch 250 loss: 0.4613400399684906
  batch 300 loss: 0.43464768171310425
  batch 350 loss: 0.4432506850361824
  batch 400 loss: 0.4230720090866089
  batch 450 loss: 0.4126478016376495
  batch 500 loss: 0.41913362622261047
  batch 550 loss: 0.46328611552715304
  batch 600 loss: 0.4837796014547348
  batch 650 loss: 0.4547894749045372
  batch 700 loss: 0.4820871025323868
  batch 750 loss: 0.4677514272928238
  batch 800 loss: 0.44796050488948824
  batch 850 loss: 0.46479267954826353
  batch 900 loss: 0.4439482817053795
LOSS train 0.44395 valid 0.69597, valid PER 20.90%
EPOCH 16:
  batch 50 loss: 0.39518552243709565
  batch 100 loss: 0.3864180302619934
  batch 150 loss: 0.424348663687706
  batch 200 loss: 0.4310104686021805
  batch 250 loss: 0.4136164590716362
  batch 300 loss: 0.398909872174263
  batch 350 loss: 0.4100689536333084
  batch 400 loss: 0.42179569602012634
  batch 450 loss: 0.4358168062567711
  batch 500 loss: 0.44200272858142853
  batch 550 loss: 0.4131707614660263
  batch 600 loss: 0.4317001622915268
  batch 650 loss: 0.44680469036102294
  batch 700 loss: 0.4027545082569122
  batch 750 loss: 0.43192516058683394
  batch 800 loss: 0.40737224400043487
  batch 850 loss: 0.4355204272270203
  batch 900 loss: 0.4527261710166931
LOSS train 0.45273 valid 0.70775, valid PER 20.77%
EPOCH 17:
  batch 50 loss: 0.3981673434376717
  batch 100 loss: 0.33330642223358153
  batch 150 loss: 0.39095381200313567
  batch 200 loss: 0.3668813282251358
  batch 250 loss: 0.3746703064441681
  batch 300 loss: 0.37296431422233584
  batch 350 loss: 0.4247890466451645
  batch 400 loss: 0.4046264839172363
  batch 450 loss: 0.3786825576424599
  batch 500 loss: 0.3949602711200714
  batch 550 loss: 0.37722003012895583
  batch 600 loss: 0.4090038365125656
  batch 650 loss: 0.3924292457103729
  batch 700 loss: 0.41333951741456987
  batch 750 loss: 0.38971230387687683
  batch 800 loss: 0.4408022141456604
  batch 850 loss: 0.40594338059425356
  batch 900 loss: 0.3997323140501976
LOSS train 0.39973 valid 0.68984, valid PER 19.80%
[1.8684845876693725, 1.172775011062622, 1.0172749114036561, 0.9300707805156708, 0.7958855259418488, 0.7912274515628814, 0.7131641763448715, 0.6674501514434814, 0.6049419665336608, 0.5900156706571579, 0.6262450361251831, 0.5598942971229554, 0.5208769503235817, 0.48286264061927797, 0.4439482817053795, 0.4527261710166931, 0.3997323140501976]
[1.7803688049316406, 1.1407023668289185, 0.985749363899231, 0.8979626893997192, 0.8493524789810181, 0.8385611772537231, 0.7893476486206055, 0.7900745272636414, 0.7369489073753357, 0.7104011178016663, 0.7280080914497375, 0.7179139256477356, 0.666707456111908, 0.6962200999259949, 0.695969820022583, 0.7077515125274658, 0.6898447871208191]
Training finished in 19.0 minutes.
Model saved to checkpoints/20230125_230853/model_13
Loading model from checkpoints/20230125_230853/model_13
SUB: 14.11%, DEL: 5.55%, INS: 2.67%, COR: 80.34%, PER: 22.33%
