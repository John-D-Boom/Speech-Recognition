Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=4, fbank_dims=23, model_dims=256, concat=1, lr=0.002, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.4, beta1=0.9, beta2=0.999, exp=0.5)
Total number of model parameters is 5326888
EPOCH 1:
  batch 50 loss: 4.4634112358093265
  batch 100 loss: 3.3123093032836914
  batch 150 loss: 3.2815749549865725
  batch 200 loss: 3.2624835109710695
  batch 250 loss: 3.135741419792175
  batch 300 loss: 2.980585618019104
  batch 350 loss: 2.904597840309143
  batch 400 loss: 2.8759979391098023
  batch 450 loss: 2.7973055696487426
  batch 500 loss: 2.6828461599349978
  batch 550 loss: 2.5424467182159423
  batch 600 loss: 2.4616933965682986
  batch 650 loss: 2.3911238384246825
  batch 700 loss: 2.3382125663757325
  batch 750 loss: 2.2474042057991026
  batch 800 loss: 2.224852936267853
  batch 850 loss: 2.1201064658164976
  batch 900 loss: 2.059456140995026
LOSS train 2.05946 valid 2.01941, valid PER 60.91%
loss updated
EPOCH 2:
  batch 50 loss: 2.0256245732307434
  batch 100 loss: 1.9797846508026122
  batch 150 loss: 1.8600908088684083
  batch 200 loss: 1.8172768998146056
  batch 250 loss: 1.8190398740768432
  batch 300 loss: 1.7546808338165283
  batch 350 loss: 1.8130312371253967
  batch 400 loss: 1.766972975730896
  batch 450 loss: 1.6876304411888123
  batch 500 loss: 1.6835988116264344
  batch 550 loss: 1.6770400047302245
  batch 600 loss: 1.6593322563171387
  batch 650 loss: 1.596819031238556
  batch 700 loss: 1.5918206572532654
  batch 750 loss: 1.5376015853881837
  batch 800 loss: 1.4819780850410462
  batch 850 loss: 1.4941302132606507
  batch 900 loss: 1.437880470752716
LOSS train 1.43788 valid 1.38517, valid PER 42.73%
loss updated
EPOCH 3:
  batch 50 loss: 1.3756142020225526
  batch 100 loss: 1.4513788318634033
  batch 150 loss: 1.4438893604278564
  batch 200 loss: 1.3432354569435119
  batch 250 loss: 1.3389457035064698
  batch 300 loss: 1.3386902379989625
  batch 350 loss: 1.361235065460205
  batch 400 loss: 1.3154634618759156
  batch 450 loss: 1.3334216284751892
  batch 500 loss: 1.2592094683647155
  batch 550 loss: 1.2880548655986785
  batch 600 loss: 1.233929215669632
  batch 650 loss: 1.2521380019187927
  batch 700 loss: 1.253987113237381
  batch 750 loss: 1.2223351645469664
  batch 800 loss: 1.2465773820877075
  batch 850 loss: 1.226875557899475
  batch 900 loss: 1.2339453494548798
LOSS train 1.23395 valid 1.19231, valid PER 37.79%
loss updated
EPOCH 4:
  batch 50 loss: 1.2129522502422332
  batch 100 loss: 1.1625739705562592
  batch 150 loss: 1.1714851236343384
  batch 200 loss: 1.1941937470436097
  batch 250 loss: 1.1777607274055482
  batch 300 loss: 1.1779825472831726
  batch 350 loss: 1.149869486093521
  batch 400 loss: 1.099885982275009
  batch 450 loss: 1.095916359424591
  batch 500 loss: 1.1612825727462768
  batch 550 loss: 1.090301194190979
  batch 600 loss: 1.1193414700031281
  batch 650 loss: 1.1625578117370605
  batch 700 loss: 1.1527865386009217
  batch 750 loss: 1.1189180159568786
  batch 800 loss: 1.1260083794593811
  batch 850 loss: 1.1099256205558776
  batch 900 loss: 1.132095409631729
LOSS train 1.13210 valid 1.07392, valid PER 34.26%
loss updated
EPOCH 5:
  batch 50 loss: 1.06729345202446
  batch 100 loss: 1.0684541749954224
  batch 150 loss: 1.0931952118873596
  batch 200 loss: 1.1425400757789612
  batch 250 loss: 1.0747110986709594
  batch 300 loss: 1.096289553642273
  batch 350 loss: 1.0236003959178925
  batch 400 loss: 1.008695969581604
  batch 450 loss: 1.0304013836383819
  batch 500 loss: 1.0247291648387908
  batch 550 loss: 1.0551605582237245
