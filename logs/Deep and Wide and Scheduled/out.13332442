Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=4, fbank_dims=23, model_dims=256, concat=1, lr=0.002, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.4, beta1=0.9, beta2=0.999, exp=0.5)
Total number of model parameters is 5326888
EPOCH 1:
  batch 50 loss: 4.4634112358093265
  batch 100 loss: 3.3123093032836914
  batch 150 loss: 3.2815749549865725
  batch 200 loss: 3.2624835109710695
  batch 250 loss: 3.135741419792175
  batch 300 loss: 2.980585618019104
  batch 350 loss: 2.904597840309143
  batch 400 loss: 2.8759979391098023
  batch 450 loss: 2.7973055696487426
  batch 500 loss: 2.6828461599349978
  batch 550 loss: 2.5424467182159423
  batch 600 loss: 2.4616933965682986
  batch 650 loss: 2.3911238384246825
  batch 700 loss: 2.3382125663757325
  batch 750 loss: 2.2474042057991026
  batch 800 loss: 2.224852936267853
  batch 850 loss: 2.1201064658164976
  batch 900 loss: 2.059456140995026
LOSS train 2.05946 valid 2.01941, valid PER 60.91%
loss updated
EPOCH 2:
  batch 50 loss: 2.0256245732307434
  batch 100 loss: 1.9797846508026122
  batch 150 loss: 1.8600908088684083
  batch 200 loss: 1.8172768998146056
  batch 250 loss: 1.8190398740768432
  batch 300 loss: 1.7546808338165283
  batch 350 loss: 1.8130312371253967
  batch 400 loss: 1.766972975730896
  batch 450 loss: 1.6876304411888123
  batch 500 loss: 1.6835988116264344
  batch 550 loss: 1.6770400047302245
  batch 600 loss: 1.6593322563171387
  batch 650 loss: 1.596819031238556
  batch 700 loss: 1.5918206572532654
  batch 750 loss: 1.5376015853881837
  batch 800 loss: 1.4819780850410462
  batch 850 loss: 1.4941302132606507
  batch 900 loss: 1.437880470752716
LOSS train 1.43788 valid 1.38517, valid PER 42.73%
loss updated
EPOCH 3:
  batch 50 loss: 1.3756142020225526
  batch 100 loss: 1.4513788318634033
  batch 150 loss: 1.4438893604278564
  batch 200 loss: 1.3432354569435119
  batch 250 loss: 1.3389457035064698
  batch 300 loss: 1.3386902379989625
  batch 350 loss: 1.361235065460205
  batch 400 loss: 1.3154634618759156
  batch 450 loss: 1.3334216284751892
  batch 500 loss: 1.2592094683647155
  batch 550 loss: 1.2880548655986785
  batch 600 loss: 1.233929215669632
  batch 650 loss: 1.2521380019187927
  batch 700 loss: 1.253987113237381
  batch 750 loss: 1.2223351645469664
  batch 800 loss: 1.2465773820877075
  batch 850 loss: 1.226875557899475
  batch 900 loss: 1.2339453494548798
LOSS train 1.23395 valid 1.19231, valid PER 37.79%
loss updated
EPOCH 4:
  batch 50 loss: 1.2129522502422332
  batch 100 loss: 1.1625739705562592
  batch 150 loss: 1.1714851236343384
  batch 200 loss: 1.1941937470436097
  batch 250 loss: 1.1777607274055482
  batch 300 loss: 1.1779825472831726
  batch 350 loss: 1.149869486093521
  batch 400 loss: 1.099885982275009
  batch 450 loss: 1.095916359424591
  batch 500 loss: 1.1612825727462768
  batch 550 loss: 1.090301194190979
  batch 600 loss: 1.1193414700031281
  batch 650 loss: 1.1625578117370605
  batch 700 loss: 1.1527865386009217
  batch 750 loss: 1.1189180159568786
  batch 800 loss: 1.1260083794593811
  batch 850 loss: 1.1099256205558776
  batch 900 loss: 1.132095409631729
LOSS train 1.13210 valid 1.07392, valid PER 34.26%
loss updated
EPOCH 5:
  batch 50 loss: 1.06729345202446
  batch 100 loss: 1.0684541749954224
  batch 150 loss: 1.0931952118873596
  batch 200 loss: 1.1425400757789612
  batch 250 loss: 1.0747110986709594
  batch 300 loss: 1.096289553642273
  batch 350 loss: 1.0236003959178925
  batch 400 loss: 1.008695969581604
  batch 450 loss: 1.0304013836383819
  batch 500 loss: 1.0247291648387908
  batch 550 loss: 1.0551605582237245
  batch 600 loss: 1.039924337863922
  batch 650 loss: 1.054389284849167
  batch 700 loss: 1.0401579678058623
  batch 750 loss: 0.9867699909210205
  batch 800 loss: 1.0564110958576203
  batch 850 loss: 1.0596908223628998
  batch 900 loss: 1.0230845367908479
LOSS train 1.02308 valid 1.01953, valid PER 32.09%
loss updated
EPOCH 6:
  batch 50 loss: 1.0001654720306397
  batch 100 loss: 1.0080322229862213
  batch 150 loss: 0.9978234696388245
  batch 200 loss: 0.9777446043491363
  batch 250 loss: 0.9952656161785126
  batch 300 loss: 1.0004536056518554
  batch 350 loss: 1.00102441906929
  batch 400 loss: 0.9586640703678131
  batch 450 loss: 1.0114455902576447
  batch 500 loss: 0.9678126895427703
  batch 550 loss: 0.9766991126537323
  batch 600 loss: 0.952099837064743
  batch 650 loss: 0.9573876118659973
  batch 700 loss: 0.9655581784248352
  batch 750 loss: 0.9792478656768799
  batch 800 loss: 0.9732289648056031
  batch 850 loss: 1.0383636593818664
  batch 900 loss: 1.0034867465496062
LOSS train 1.00349 valid 0.93522, valid PER 29.64%
loss updated
EPOCH 7:
  batch 50 loss: 0.9160926318168641
  batch 100 loss: 0.9989521598815918
  batch 150 loss: 0.9404675710201263
  batch 200 loss: 0.9484014976024627
  batch 250 loss: 0.9896495747566223
  batch 300 loss: 0.9315816414356232
  batch 350 loss: 0.9776668071746826
  batch 400 loss: 0.9281291115283966
  batch 450 loss: 0.909955369234085
  batch 500 loss: 0.9431333100795746
  batch 550 loss: 0.921270740032196
  batch 600 loss: 0.9215600752830505
  batch 650 loss: 0.888147702217102
  batch 700 loss: 0.9522351825237274
  batch 750 loss: 0.9324199783802033
  batch 800 loss: 0.908628214597702
  batch 850 loss: 0.9032242476940155
  batch 900 loss: 0.9317586874961853
LOSS train 0.93176 valid 0.93192, valid PER 29.06%
loss updated
EPOCH 8:
  batch 50 loss: 0.9191999053955078
  batch 100 loss: 0.857213340997696
  batch 150 loss: 0.9021368145942688
  batch 200 loss: 0.8900241601467133
  batch 250 loss: 0.872964049577713
  batch 300 loss: 0.8610714042186737
  batch 350 loss: 0.8789815866947174
  batch 400 loss: 0.8598634803295135
  batch 450 loss: 0.9356284546852112
  batch 500 loss: 0.9208280694484711
  batch 550 loss: 0.8748862791061401
  batch 600 loss: 0.8322208178043365
  batch 650 loss: 0.8844806706905365
  batch 700 loss: 0.9054641985893249
  batch 750 loss: 0.8899003446102143
  batch 800 loss: 0.8950043118000031
  batch 850 loss: 0.8591813135147095
  batch 900 loss: 0.8806695342063904
LOSS train 0.88067 valid 0.87085, valid PER 27.92%
loss updated
EPOCH 9:
  batch 50 loss: 0.833288311958313
  batch 100 loss: 0.8168455791473389
  batch 150 loss: 0.8499596869945526
  batch 200 loss: 0.8327022540569305
  batch 250 loss: 0.7917437422275543
  batch 300 loss: 0.8120993494987487
  batch 350 loss: 0.8219896113872528
  batch 400 loss: 0.889111555814743
  batch 450 loss: 0.8821405100822449
  batch 500 loss: 0.8505289018154144
  batch 550 loss: 0.8359993004798889
  batch 600 loss: 0.8549298465251922
  batch 650 loss: 0.8414810812473297
  batch 700 loss: 0.8231340479850769
  batch 750 loss: 0.8289469587802887
  batch 800 loss: 0.8703527307510376
  batch 850 loss: 0.8507612931728363
  batch 900 loss: 0.8130955177545548
LOSS train 0.81310 valid 0.83544, valid PER 26.11%
loss updated
EPOCH 10:
  batch 50 loss: 0.7768327581882477
  batch 100 loss: 0.7956870293617249
  batch 150 loss: 0.8299337375164032
  batch 200 loss: 0.7950769758224487
  batch 250 loss: 0.7950426268577576
  batch 300 loss: 0.8143570959568024
  batch 350 loss: 0.7995462638139724
  batch 400 loss: 0.7644880497455597
  batch 450 loss: 0.8061900329589844
  batch 500 loss: 0.7906977963447571
  batch 550 loss: 0.8034957039356232
  batch 600 loss: 0.8075902724266052
  batch 650 loss: 0.8236436951160431
  batch 700 loss: 0.8060286593437195
  batch 750 loss: 0.8548308527469635
  batch 800 loss: 0.8375050497055053
  batch 850 loss: 0.7989009630680084
  batch 900 loss: 0.8003015732765197
LOSS train 0.80030 valid 0.85201, valid PER 26.30%
EPOCH 11:
  batch 50 loss: 0.7705243158340455
  batch 100 loss: 0.7487078011035919
  batch 150 loss: 0.7640528815984726
  batch 200 loss: 0.7283873403072357
  batch 250 loss: 0.7445397341251373
  batch 300 loss: 0.7699863004684449
  batch 350 loss: 0.8014128434658051
  batch 400 loss: 0.7703321993350982
  batch 450 loss: 0.7826235544681549
  batch 500 loss: 0.766151658296585
  batch 550 loss: 0.8027231550216675
  batch 600 loss: 0.7901931738853455
  batch 650 loss: 0.7897299373149872
  batch 700 loss: 0.8427915763854981
  batch 750 loss: 0.7769926834106445
  batch 800 loss: 0.7937189364433288
  batch 850 loss: 0.7914061486721039
  batch 900 loss: 0.8052548301219941
LOSS train 0.80525 valid 0.81762, valid PER 25.69%
loss updated
EPOCH 12:
  batch 50 loss: 0.7384167444705964
  batch 100 loss: 0.7256128090620041
  batch 150 loss: 0.7580253827571869
  batch 200 loss: 0.7244528472423554
  batch 250 loss: 0.7305329203605652
  batch 300 loss: 0.785073413848877
  batch 350 loss: 0.7496531534194947
  batch 400 loss: 0.7688925731182098
  batch 450 loss: 0.7502029210329055
  batch 500 loss: 0.7719144582748413
  batch 550 loss: 0.7622309929132461
  batch 600 loss: 0.778550773859024
  batch 650 loss: 0.788433198928833
  batch 700 loss: 0.7790920150279999
  batch 750 loss: 0.7958402633666992
  batch 800 loss: 0.7564840829372406
  batch 850 loss: 0.7812152063846588
  batch 900 loss: 0.7749762666225434
LOSS train 0.77498 valid 0.81819, valid PER 25.13%
EPOCH 13:
  batch 50 loss: 0.7170836639404297
  batch 100 loss: 0.7250917345285416
  batch 150 loss: 0.728657306432724
  batch 200 loss: 0.6778636646270751
  batch 250 loss: 0.7037592458724976
  batch 300 loss: 0.7487989640235901
  batch 350 loss: 0.710102242231369
  batch 400 loss: 0.7269903528690338
  batch 450 loss: 0.7229532116651535
  batch 500 loss: 0.7352247077226639
  batch 550 loss: 0.7951464384794236
  batch 600 loss: 0.7378372502326965
  batch 650 loss: 0.719614754319191
  batch 700 loss: 0.7520584201812744
  batch 750 loss: 0.7040913623571395
  batch 800 loss: 0.7065524339675904
  batch 850 loss: 0.7103057968616485
  batch 900 loss: 0.7308588421344757
LOSS train 0.73086 valid 0.77923, valid PER 24.14%
loss updated
EPOCH 14:
  batch 50 loss: 0.6738507223129272
  batch 100 loss: 0.6711512649059296
  batch 150 loss: 0.6775861322879791
  batch 200 loss: 0.7000254935026169
  batch 250 loss: 0.694791864156723
  batch 300 loss: 0.6820856893062591
  batch 350 loss: 0.6908478975296021
  batch 400 loss: 0.7049791520833969
  batch 450 loss: 0.6701368820667267
  batch 500 loss: 0.6714274716377259
  batch 550 loss: 0.7073985505104065
  batch 600 loss: 0.6943619638681412
  batch 650 loss: 0.7160717785358429
  batch 700 loss: 0.7178532272577286
  batch 750 loss: 0.6935692977905273
  batch 800 loss: 0.6911194628477096
  batch 850 loss: 0.7454285186529159
  batch 900 loss: 0.7195013725757599
LOSS train 0.71950 valid 0.79156, valid PER 24.40%
EPOCH 15:
  batch 50 loss: 0.6593512749671936
  batch 100 loss: 0.670761171579361
  batch 150 loss: 0.6789209985733032
  batch 200 loss: 0.698943098783493
  batch 250 loss: 0.6949908745288849
  batch 300 loss: 0.6777410393953324
  batch 350 loss: 0.6585307335853576
  batch 400 loss: 0.6734244847297668
  batch 450 loss: 0.6553915882110596
  batch 500 loss: 0.6353093707561492
  batch 550 loss: 0.7103894835710526
  batch 600 loss: 0.7074608564376831
  batch 650 loss: 0.6928959429264069
  batch 700 loss: 0.6611857295036316
  batch 750 loss: 0.6954378724098206
  batch 800 loss: 0.696852530837059
  batch 850 loss: 0.6893601059913635
  batch 900 loss: 0.6488336533308029
LOSS train 0.64883 valid 0.77743, valid PER 23.76%
loss updated
EPOCH 16:
  batch 50 loss: 0.659865778684616
  batch 100 loss: 0.61906503200531
  batch 150 loss: 0.6822475445270538
  batch 200 loss: 0.6835093921422959
  batch 250 loss: 0.6902685296535492
  batch 300 loss: 0.6873876696825028
  batch 350 loss: 0.6801828730106354
  batch 400 loss: 0.6747701990604401
  batch 450 loss: 0.6642818677425385
  batch 500 loss: 0.6479056471586228
  batch 550 loss: 0.6244212454557418
  batch 600 loss: 0.6767769593000412
  batch 650 loss: 0.6607545489072799
  batch 700 loss: 0.6530900341272354
  batch 750 loss: 0.6498886930942536
  batch 800 loss: 0.666579464673996
  batch 850 loss: 0.6518685519695282
  batch 900 loss: 0.6915787947177887
LOSS train 0.69158 valid 0.74181, valid PER 23.08%
loss updated
EPOCH 17:
  batch 50 loss: 0.634183794260025
  batch 100 loss: 0.5907216942310334
  batch 150 loss: 0.6225481683015823
  batch 200 loss: 0.5937403100728988
  batch 250 loss: 0.6185500407218933
  batch 300 loss: 0.6290301942825317
  batch 350 loss: 0.6486240220069885
  batch 400 loss: 0.6490399003028869
  batch 450 loss: 0.6250358653068543
  batch 500 loss: 0.6493480783700943
  batch 550 loss: 0.6287390458583831
  batch 600 loss: 0.6875386220216751
  batch 650 loss: 0.6466302895545959
  batch 700 loss: 0.6668261861801148
  batch 750 loss: 0.6026830393075943
  batch 800 loss: 0.6538562732934952
  batch 850 loss: 0.6399721550941467
  batch 900 loss: 0.6261819195747376
LOSS train 0.62618 valid 0.73855, valid PER 22.77%
loss updated
EPOCH 18:
  batch 50 loss: 0.5865751332044602
  batch 100 loss: 0.606637892127037
  batch 150 loss: 0.6493185490369797
  batch 200 loss: 0.635685116648674
  batch 250 loss: 0.5759588384628296
  batch 300 loss: 0.6154977935552597
  batch 350 loss: 0.5832134103775024
  batch 400 loss: 0.5927083241939545
  batch 450 loss: 0.6048784017562866
  batch 500 loss: 0.636563059091568
  batch 550 loss: 0.6490650308132172
  batch 600 loss: 0.6240046411752701
  batch 650 loss: 0.6185705941915512
  batch 700 loss: 0.5989587068557739
  batch 750 loss: 0.6327615183591843
  batch 800 loss: 0.6504811823368073
  batch 850 loss: 0.6551601177453995
  batch 900 loss: 0.6408320593833924
LOSS train 0.64083 valid 0.75056, valid PER 23.19%
EPOCH 19:
  batch 50 loss: 0.5963959079980851
  batch 100 loss: 0.6095278865098953
  batch 150 loss: 0.5615275585651398
  batch 200 loss: 0.5642789030075073
  batch 250 loss: 0.6196196490526199
  batch 300 loss: 0.6459176176786423
  batch 350 loss: 0.6255402487516403
  batch 400 loss: 0.5977389526367187
  batch 450 loss: 0.5872764092683792
  batch 500 loss: 0.5852357292175293
  batch 550 loss: 0.6613071751594544
  batch 600 loss: 0.6121046298742294
  batch 650 loss: 0.6336897677183151
  batch 700 loss: 0.6202144038677215
  batch 750 loss: 0.5935036498308182
  batch 800 loss: 0.589290719628334
  batch 850 loss: 0.5904182505607605
  batch 900 loss: 0.608127030134201
LOSS train 0.60813 valid 0.72488, valid PER 22.36%
loss updated
EPOCH 20:
  batch 50 loss: 0.5540980738401413
  batch 100 loss: 0.5673025739192963
  batch 150 loss: 0.5667303788661957
  batch 200 loss: 0.5526488572359085
  batch 250 loss: 0.5725668197870255
  batch 300 loss: 0.5799119418859482
  batch 350 loss: 0.5403612834215165
  batch 400 loss: 0.568995772600174
  batch 450 loss: 0.5531660461425781
  batch 500 loss: 0.6130580419301986
  batch 550 loss: 0.5869821262359619
  batch 600 loss: 0.5757681131362915
  batch 650 loss: 0.5942784869670867
  batch 700 loss: 0.6020839405059815
  batch 750 loss: 0.5708225750923157
  batch 800 loss: 0.6204407274723053
  batch 850 loss: 0.6098650979995728
  batch 900 loss: 0.5883081585168839
LOSS train 0.58831 valid 0.72877, valid PER 22.16%
[2.059456140995026, 1.437880470752716, 1.2339453494548798, 1.132095409631729, 1.0230845367908479, 1.0034867465496062, 0.9317586874961853, 0.8806695342063904, 0.8130955177545548, 0.8003015732765197, 0.8052548301219941, 0.7749762666225434, 0.7308588421344757, 0.7195013725757599, 0.6488336533308029, 0.6915787947177887, 0.6261819195747376, 0.6408320593833924, 0.608127030134201, 0.5883081585168839]
[2.0194106101989746, 1.385169506072998, 1.1923105716705322, 1.0739247798919678, 1.0195282697677612, 0.9352229833602905, 0.931921124458313, 0.8708469271659851, 0.8354374766349792, 0.8520129919052124, 0.8176174759864807, 0.8181858062744141, 0.7792344689369202, 0.7915617823600769, 0.7774263024330139, 0.7418095469474792, 0.7385510206222534, 0.7505561709403992, 0.724884569644928, 0.7287715077400208]
Training finished in 28.0 minutes.
Model saved to checkpoints/20230125_221344/model_19
Loading model from checkpoints/20230125_221344/model_19
SUB: 15.92%, DEL: 6.31%, INS: 2.22%, COR: 77.77%, PER: 24.45%
