Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=4, fbank_dims=23, model_dims=256, concat=1, lr=0.001, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.4, beta1=0.9, beta2=0.999, exp=0.5)
Total number of model parameters is 5326888
EPOCH 1:
  batch 50 loss: 5.15173837184906
  batch 100 loss: 3.3063014221191405
  batch 150 loss: 3.282339696884155
  batch 200 loss: 3.267542519569397
  batch 250 loss: 3.204602823257446
  batch 300 loss: 3.027867555618286
  batch 350 loss: 2.8470751285552978
  batch 400 loss: 2.7545829582214356
  batch 450 loss: 2.6635090112686157
  batch 500 loss: 2.5307755422592164
  batch 550 loss: 2.38611620426178
  batch 600 loss: 2.302476167678833
  batch 650 loss: 2.1661748123168945
  batch 700 loss: 2.0729784989356994
  batch 750 loss: 1.9537541341781617
  batch 800 loss: 1.926480505466461
  batch 850 loss: 1.8290816640853882
  batch 900 loss: 1.7531890273094177
LOSS train 1.75319 valid 1.68434, valid PER 51.09%
loss updated
EPOCH 2:
  batch 50 loss: 1.7056023097038269
  batch 100 loss: 1.6735213923454284
  batch 150 loss: 1.5808466744422913
  batch 200 loss: 1.5339196467399596
  batch 250 loss: 1.4807744526863098
  batch 300 loss: 1.4595526194572448
  batch 350 loss: 1.4720524168014526
  batch 400 loss: 1.4147886538505554
  batch 450 loss: 1.374457960128784
  batch 500 loss: 1.3814088463783265
  batch 550 loss: 1.314432398080826
  batch 600 loss: 1.3003216528892516
  batch 650 loss: 1.2841705727577208
  batch 700 loss: 1.2746548104286193
  batch 750 loss: 1.2823087573051453
  batch 800 loss: 1.2212861156463624
  batch 850 loss: 1.2118503522872925
  batch 900 loss: 1.1760436749458314
LOSS train 1.17604 valid 1.14377, valid PER 36.37%
loss updated
EPOCH 3:
  batch 50 loss: 1.1183303892612457
  batch 100 loss: 1.1847799134254455
  batch 150 loss: 1.1972113001346587
  batch 200 loss: 1.10838649392128
  batch 250 loss: 1.0970391607284546
  batch 300 loss: 1.1060518634319305
  batch 350 loss: 1.1176979768276214
  batch 400 loss: 1.0916129171848297
  batch 450 loss: 1.0839902663230896
  batch 500 loss: 1.0733664953708648
  batch 550 loss: 1.0625845289230347
  batch 600 loss: 1.0017678570747375
  batch 650 loss: 1.0366512429714203
  batch 700 loss: 1.0413134539127349
  batch 750 loss: 1.0466922843456268
  batch 800 loss: 1.061720095872879
  batch 850 loss: 1.0129301130771637
  batch 900 loss: 1.0546502673625946
LOSS train 1.05465 valid 1.00416, valid PER 31.76%
loss updated
EPOCH 4:
  batch 50 loss: 1.0002771461009978
  batch 100 loss: 0.9564669132232666
  batch 150 loss: 0.9851931166648865
  batch 200 loss: 0.9574887430667878
  batch 250 loss: 0.9532869231700897
  batch 300 loss: 0.9632127237319946
  batch 350 loss: 0.9431133580207824
  batch 400 loss: 0.9173501420021057
  batch 450 loss: 0.9329122126102447
  batch 500 loss: 1.0123098075389863
  batch 550 loss: 0.9449444651603699
  batch 600 loss: 0.9253993487358093
  batch 650 loss: 0.9630397999286652
  batch 700 loss: 0.9541547083854676
  batch 750 loss: 0.9596349608898163
  batch 800 loss: 0.9489538657665253
  batch 850 loss: 0.9400439846515656
  batch 900 loss: 0.9306057596206665
LOSS train 0.93061 valid 0.94104, valid PER 29.02%
loss updated
EPOCH 5:
  batch 50 loss: 0.8860379695892334
  batch 100 loss: 0.8674109375476837
  batch 150 loss: 0.9233905041217804
  batch 200 loss: 0.9228910064697265
  batch 250 loss: 0.8607965445518494
  batch 300 loss: 0.9022561275959015
  batch 350 loss: 0.8730348038673401
  batch 400 loss: 0.8308709156513214
  batch 450 loss: 0.8563480305671692
  batch 500 loss: 0.8176377671957016
  batch 550 loss: 0.907041574716568
  batch 600 loss: 0.895399044752121
  batch 650 loss: 0.9108408558368682
  batch 700 loss: 0.8665074479579925
  batch 750 loss: 0.8487398743629455
  batch 800 loss: 0.8927555084228516
  batch 850 loss: 0.8779782116413116
  batch 900 loss: 0.8872402739524842
LOSS train 0.88724 valid 0.90913, valid PER 28.28%
loss updated
EPOCH 6:
  batch 50 loss: 0.8261884093284607
  batch 100 loss: 0.8172771942615509
  batch 150 loss: 0.809761723279953
  batch 200 loss: 0.78784852206707
  batch 250 loss: 0.7858857429027557
  batch 300 loss: 0.8454253530502319
  batch 350 loss: 0.8348549199104309
  batch 400 loss: 0.8106061697006226
  batch 450 loss: 0.8286477136611938
  batch 500 loss: 0.7754906928539276
  batch 550 loss: 0.814268765449524
  batch 600 loss: 0.7904755151271821
  batch 650 loss: 0.7702481627464295
  batch 700 loss: 0.7708380317687988
  batch 750 loss: 0.7783084380626678
  batch 800 loss: 0.805973128080368
  batch 850 loss: 0.8091760659217835
  batch 900 loss: 0.7963894987106324
LOSS train 0.79639 valid 0.83167, valid PER 26.33%
loss updated
EPOCH 7:
  batch 50 loss: 0.758576979637146
  batch 100 loss: 0.7836722481250763
  batch 150 loss: 0.7124968165159226
  batch 200 loss: 0.7291830670833588
  batch 250 loss: 0.7763550174236298
  batch 300 loss: 0.7293165814876557
  batch 350 loss: 0.7674522602558136
  batch 400 loss: 0.7411967432498932
  batch 450 loss: 0.7357096511125565
  batch 500 loss: 0.7370313000679016
  batch 550 loss: 0.7435122776031494
  batch 600 loss: 0.7505127954483032
  batch 650 loss: 0.7338056814670563
  batch 700 loss: 0.7841347402334213
  batch 750 loss: 0.7303268218040466
  batch 800 loss: 0.7272629207372665
  batch 850 loss: 0.722597383260727
  batch 900 loss: 0.7193248242139816
LOSS train 0.71932 valid 0.80521, valid PER 24.98%
loss updated
EPOCH 8:
  batch 50 loss: 0.7067851173877716
  batch 100 loss: 0.6598619657754898
  batch 150 loss: 0.698104008436203
  batch 200 loss: 0.6727983778715134
  batch 250 loss: 0.6923382043838501
  batch 300 loss: 0.6848553973436355
  batch 350 loss: 0.6912719994783402
  batch 400 loss: 0.7064541268348694
  batch 450 loss: 0.7447640043497086
  batch 500 loss: 0.6956707376241684
  batch 550 loss: 0.7019515061378478
  batch 600 loss: 0.6714014029502868
  batch 650 loss: 0.7093685650825501
  batch 700 loss: 0.7069560015201568
  batch 750 loss: 0.7056338173151017
  batch 800 loss: 0.7117851865291596
  batch 850 loss: 0.6954837185144425
  batch 900 loss: 0.7008009052276611
LOSS train 0.70080 valid 0.76929, valid PER 23.74%
loss updated
EPOCH 9:
  batch 50 loss: 0.6561764192581176
  batch 100 loss: 0.6056131106615067
  batch 150 loss: 0.6383427238464355
  batch 200 loss: 0.6484949868917466
  batch 250 loss: 0.6133065235614776
  batch 300 loss: 0.6551109802722931
  batch 350 loss: 0.5997341120243073
  batch 400 loss: 0.6680419838428497
  batch 450 loss: 0.687802100777626
  batch 500 loss: 0.6483239501714706
  batch 550 loss: 0.6393229061365128
  batch 600 loss: 0.7048702430725098
  batch 650 loss: 0.6925005179643631
  batch 700 loss: 0.6888830316066742
  batch 750 loss: 0.6773916012048722
  batch 800 loss: 0.6944473916292191
  batch 850 loss: 0.679409749507904
  batch 900 loss: 0.6441331291198731
LOSS train 0.64413 valid 0.78615, valid PER 24.36%
EPOCH 10:
  batch 50 loss: 0.6489926707744599
  batch 100 loss: 0.6332158893346786
  batch 150 loss: 0.6450627589225769
  batch 200 loss: 0.6342707294225692
  batch 250 loss: 0.6302169537544251
  batch 300 loss: 0.609088476896286
  batch 350 loss: 0.6178767049312591
  batch 400 loss: 0.5979603815078736
  batch 450 loss: 0.6051809388399124
  batch 500 loss: 0.5967998576164245
  batch 550 loss: 0.6435973823070527
  batch 600 loss: 0.6296881818771363
  batch 650 loss: 0.6503604573011398
  batch 700 loss: 0.6135179990530014
  batch 750 loss: 0.6432383960485458
  batch 800 loss: 0.6401904004812241
  batch 850 loss: 0.6383616983890533
  batch 900 loss: 0.6417692792415619
LOSS train 0.64177 valid 0.72035, valid PER 22.30%
loss updated
EPOCH 11:
  batch 50 loss: 0.5894267386198044
  batch 100 loss: 0.5668469512462616
  batch 150 loss: 0.5807143878936768
  batch 200 loss: 0.5306043916940689
  batch 250 loss: 0.5587853002548218
  batch 300 loss: 0.568589539527893
  batch 350 loss: 0.6004455864429474
  batch 400 loss: 0.5786314988136292
  batch 450 loss: 0.5564875966310501
  batch 500 loss: 0.5716072022914886
  batch 550 loss: 0.6248623251914978
  batch 600 loss: 0.5916070806980133
  batch 650 loss: 0.6085130566358566
  batch 700 loss: 0.6428439879417419
  batch 750 loss: 0.5718001186847687
  batch 800 loss: 0.6188887417316437
  batch 850 loss: 0.6080918890237809
  batch 900 loss: 0.641538895368576
LOSS train 0.64154 valid 0.70835, valid PER 22.08%
loss updated
EPOCH 12:
  batch 50 loss: 0.5524731254577637
  batch 100 loss: 0.5455639910697937
  batch 150 loss: 0.5561707216501236
  batch 200 loss: 0.5757536625862122
  batch 250 loss: 0.5593035006523133
  batch 300 loss: 0.5773639017343521
  batch 350 loss: 0.5409549582004547
  batch 400 loss: 0.5785657215118408
  batch 450 loss: 0.5553329706192016
  batch 500 loss: 0.5681197607517242
  batch 550 loss: 0.5629734346270561
  batch 600 loss: 0.5472522097826004
  batch 650 loss: 0.5720061051845551
  batch 700 loss: 0.5576133424043656
  batch 750 loss: 0.5699933934211731
  batch 800 loss: 0.5430557602643966
  batch 850 loss: 0.5646762788295746
  batch 900 loss: 0.5789766710996628
LOSS train 0.57898 valid 0.72510, valid PER 22.51%
EPOCH 13:
  batch 50 loss: 0.5022720336914063
  batch 100 loss: 0.5044384908676147
  batch 150 loss: 0.5313269340991974
  batch 200 loss: 0.5191078698635101
  batch 250 loss: 0.520786982178688
  batch 300 loss: 0.5436047118902206
  batch 350 loss: 0.514999937415123
  batch 400 loss: 0.5284346175193787
  batch 450 loss: 0.5369626086950302
  batch 500 loss: 0.5176335066556931
  batch 550 loss: 0.5523028194904327
  batch 600 loss: 0.5442259842157364
  batch 650 loss: 0.5313314110040664
  batch 700 loss: 0.5409046471118927
  batch 750 loss: 0.5258398228883743
  batch 800 loss: 0.5307748204469681
  batch 850 loss: 0.5192279934883117
  batch 900 loss: 0.5618058443069458
LOSS train 0.56181 valid 0.70576, valid PER 21.70%
loss updated
EPOCH 14:
  batch 50 loss: 0.48493549883365633
  batch 100 loss: 0.4732721483707428
  batch 150 loss: 0.49257741689682005
  batch 200 loss: 0.48286895275115965
  batch 250 loss: 0.48297120332717897
  batch 300 loss: 0.48052594244480135
  batch 350 loss: 0.5015268582105636
  batch 400 loss: 0.5158014684915543
  batch 450 loss: 0.4761217400431633
  batch 500 loss: 0.4987323695421219
  batch 550 loss: 0.5229388958215714
  batch 600 loss: 0.49757236659526827
  batch 650 loss: 0.5110243362188339
  batch 700 loss: 0.5351719516515732
  batch 750 loss: 0.5484039890766144
  batch 800 loss: 0.518323754966259
  batch 850 loss: 0.5500714313983918
  batch 900 loss: 0.5294679969549179
LOSS train 0.52947 valid 0.70268, valid PER 21.12%
loss updated
EPOCH 15:
  batch 50 loss: 0.4520100170373917
  batch 100 loss: 0.46024378031492236
  batch 150 loss: 0.5186799710988999
  batch 200 loss: 0.49513416171073915
  batch 250 loss: 0.5108404827117919
  batch 300 loss: 0.5073354637622833
  batch 350 loss: 0.5144754093885422
  batch 400 loss: 0.5024667370319367
  batch 450 loss: 0.4702827894687653
  batch 500 loss: 0.47497537642717363
  batch 550 loss: 0.5015908914804459
  batch 600 loss: 0.542027605175972
  batch 650 loss: 0.5126075965166091
  batch 700 loss: 0.4791564518213272
  batch 750 loss: 0.5073021084070206
  batch 800 loss: 0.4877300018072128
  batch 850 loss: 0.5017179262638092
  batch 900 loss: 0.4700914257764816
LOSS train 0.47009 valid 0.67759, valid PER 20.29%
loss updated
EPOCH 16:
  batch 50 loss: 0.43510480493307113
  batch 100 loss: 0.43340209007263186
  batch 150 loss: 0.43179877519607546
  batch 200 loss: 0.4623395252227783
  batch 250 loss: 0.4504325371980667
  batch 300 loss: 0.4494896721839905
  batch 350 loss: 0.4633243703842163
  batch 400 loss: 0.49427844226360323
  batch 450 loss: 0.4909109252691269
  batch 500 loss: 0.4528052455186844
  batch 550 loss: 0.4468614375591278
  batch 600 loss: 0.46968692541122437
  batch 650 loss: 0.46975145876407626
  batch 700 loss: 0.4370402494072914
  batch 750 loss: 0.45880893886089325
  batch 800 loss: 0.4463690751791
  batch 850 loss: 0.4497780603170395
  batch 900 loss: 0.45083710134029387
LOSS train 0.45084 valid 0.67183, valid PER 20.37%
loss updated
EPOCH 17:
  batch 50 loss: 0.42036171644926074
  batch 100 loss: 0.3866613358259201
  batch 150 loss: 0.43617783695459367
  batch 200 loss: 0.4335471248626709
  batch 250 loss: 0.45072316825389863
  batch 300 loss: 0.42876229763031004
  batch 350 loss: 0.45609661847352984
  batch 400 loss: 0.4660628372430801
  batch 450 loss: 0.42436846435070036
  batch 500 loss: 0.4426277333498001
  batch 550 loss: 0.42625553250312803
  batch 600 loss: 0.47192494332790375
  batch 650 loss: 0.44012998282909394
  batch 700 loss: 0.4462816625833511
  batch 750 loss: 0.4391975349187851
  batch 800 loss: 0.44418082118034363
  batch 850 loss: 0.45469951927661895
  batch 900 loss: 0.45761516034603117
LOSS train 0.45762 valid 0.68924, valid PER 20.49%
EPOCH 18:
  batch 50 loss: 0.377565504014492
  batch 100 loss: 0.3878363731503487
  batch 150 loss: 0.4305757075548172
  batch 200 loss: 0.39512785017490387
  batch 250 loss: 0.38733612358570096
  batch 300 loss: 0.38888628512620926
  batch 350 loss: 0.39043417483568194
  batch 400 loss: 0.4004906716942787
  batch 450 loss: 0.4273550224304199
  batch 500 loss: 0.4079649966955185
  batch 550 loss: 0.4314247366786003
  batch 600 loss: 0.4265639024972916
  batch 650 loss: 0.4099066635966301
  batch 700 loss: 0.41655152797698974
  batch 750 loss: 0.4363411659002304
  batch 800 loss: 0.4804125648736954
  batch 850 loss: 0.4682499647140503
  batch 900 loss: 0.4591269081830978
LOSS train 0.45913 valid 0.70396, valid PER 20.18%
EPOCH 19:
  batch 50 loss: 0.3881852513551712
  batch 100 loss: 0.38514469474554064
  batch 150 loss: 0.3571337845921516
  batch 200 loss: 0.36452949047088623
  batch 250 loss: 0.42937168657779695
  batch 300 loss: 0.4107900172472
  batch 350 loss: 0.4103475403785706
  batch 400 loss: 0.377874293923378
  batch 450 loss: 0.37020208925008774
  batch 500 loss: 0.36042032778263094
  batch 550 loss: 0.40237996459007264
  batch 600 loss: 0.4122530907392502
  batch 650 loss: 0.40945939391851427
  batch 700 loss: 0.40243206679821014
  batch 750 loss: 0.4064414489269257
  batch 800 loss: 0.3998559755086899
  batch 850 loss: 0.41034959495067597
  batch 900 loss: 0.4313844001293182
LOSS train 0.43138 valid 0.70788, valid PER 20.55%
EPOCH 20:
  batch 50 loss: 0.37780976921319964
  batch 100 loss: 0.36318414032459256
  batch 150 loss: 0.36322919398546216
  batch 200 loss: 0.37965008974075315
  batch 250 loss: 0.3711351242661476
  batch 300 loss: 0.3811856111884117
  batch 350 loss: 0.3546370604634285
  batch 400 loss: 0.388631252348423
  batch 450 loss: 0.3616070982813835
  batch 500 loss: 0.39111324310302736
  batch 550 loss: 0.38697439461946487
  batch 600 loss: 0.3783786988258362
  batch 650 loss: 0.3863975206017494
  batch 700 loss: 0.3837110948562622
  batch 750 loss: 0.3960696229338646
  batch 800 loss: 0.4181444752216339
  batch 850 loss: 0.4282048481702805
  batch 900 loss: 0.4070316529273987
LOSS train 0.40703 valid 0.70627, valid PER 20.43%
[1.7531890273094177, 1.1760436749458314, 1.0546502673625946, 0.9306057596206665, 0.8872402739524842, 0.7963894987106324, 0.7193248242139816, 0.7008009052276611, 0.6441331291198731, 0.6417692792415619, 0.641538895368576, 0.5789766710996628, 0.5618058443069458, 0.5294679969549179, 0.4700914257764816, 0.45083710134029387, 0.45761516034603117, 0.4591269081830978, 0.4313844001293182, 0.4070316529273987]
[1.6843427419662476, 1.1437681913375854, 1.0041604042053223, 0.9410442113876343, 0.9091255068778992, 0.8316718935966492, 0.8052061796188354, 0.7692945003509521, 0.7861506342887878, 0.7203541398048401, 0.7083538770675659, 0.725103497505188, 0.7057631611824036, 0.7026760578155518, 0.6775944232940674, 0.6718268990516663, 0.6892445087432861, 0.7039614915847778, 0.7078754901885986, 0.7062730193138123]
Training finished in 24.0 minutes.
Model saved to checkpoints/20230125_225125/model_16
Loading model from checkpoints/20230125_225125/model_16
SUB: 14.22%, DEL: 5.91%, INS: 2.25%, COR: 79.87%, PER: 22.38%
