Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=4, fbank_dims=23, model_dims=256, concat=1, lr=0.002, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.4, beta1=0.9, beta2=0.999)
Total number of model parameters is 5326888
EPOCH 1:
  batch 50 loss: 4.523953099250793
  batch 100 loss: 3.3124428033828734
  batch 150 loss: 3.2808419990539552
  batch 200 loss: 3.277275314331055
  batch 250 loss: 3.1916519165039063
  batch 300 loss: 2.9687841796875
  batch 350 loss: 2.847771100997925
  batch 400 loss: 2.7524522829055784
  batch 450 loss: 2.6926225566864015
  batch 500 loss: 2.5784162998199465
  batch 550 loss: 2.413899736404419
  batch 600 loss: 2.3426790475845336
  batch 650 loss: 2.2230725073814392
  batch 700 loss: 2.1525536584854126
  batch 750 loss: 1.9911569571495056
  batch 800 loss: 1.9624348521232604
  batch 850 loss: 1.8922987914085387
  batch 900 loss: 1.8322360825538635
LOSS train 1.83224 valid 1.74684, valid PER 53.85%
EPOCH 2:
  batch 50 loss: 1.7680349588394164
  batch 100 loss: 1.7197199249267578
  batch 150 loss: 1.5953663516044616
  batch 200 loss: 1.5769705915451049
  batch 250 loss: 1.5382951593399048
  batch 300 loss: 1.4686756706237794
  batch 350 loss: 1.5088997864723206
  batch 400 loss: 1.4533836269378662
  batch 450 loss: 1.4057371616363525
  batch 500 loss: 1.449486711025238
  batch 550 loss: 1.4118102383613587
  batch 600 loss: 1.36481436252594
  batch 650 loss: 1.3226887583732605
  batch 700 loss: 1.3505708837509156
  batch 750 loss: 1.2974752652645112
  batch 800 loss: 1.2839349842071532
  batch 850 loss: 1.2669244229793548
  batch 900 loss: 1.2342918229103088
LOSS train 1.23429 valid 1.19014, valid PER 38.19%
EPOCH 3:
  batch 50 loss: 1.1759515738487243
  batch 100 loss: 1.239193994998932
  batch 150 loss: 1.2774187970161437
  batch 200 loss: 1.1735761964321136
  batch 250 loss: 1.174642605781555
  batch 300 loss: 1.1788815712928773
  batch 350 loss: 1.1913023686408997
  batch 400 loss: 1.1579435443878174
  batch 450 loss: 1.15891233086586
  batch 500 loss: 1.1518214762210846
  batch 550 loss: 1.1559773755073548
  batch 600 loss: 1.0855025351047516
  batch 650 loss: 1.0968464052677154
  batch 700 loss: 1.094912657737732
  batch 750 loss: 1.1271159064769745
  batch 800 loss: 1.141571091413498
  batch 850 loss: 1.097454957962036
  batch 900 loss: 1.1144323480129241
LOSS train 1.11443 valid 1.07328, valid PER 33.78%
EPOCH 4:
  batch 50 loss: 1.1053211688995361
  batch 100 loss: 1.0316610324382782
  batch 150 loss: 1.0438463807106018
  batch 200 loss: 1.0467594528198243
  batch 250 loss: 1.0612242209911347
  batch 300 loss: 1.0223570346832276
  batch 350 loss: 1.03287500500679
  batch 400 loss: 1.0264536452293396
  batch 450 loss: 1.0061755216121673
  batch 500 loss: 1.082078583240509
  batch 550 loss: 0.9792609286308288
  batch 600 loss: 1.00377308011055
  batch 650 loss: 1.0750920057296753
  batch 700 loss: 1.0658953726291656
  batch 750 loss: 1.0325950956344605
  batch 800 loss: 1.0099832546710967
  batch 850 loss: 1.0059611105918884
  batch 900 loss: 1.0452309262752533
LOSS train 1.04523 valid 1.00571, valid PER 31.12%
EPOCH 5:
  batch 50 loss: 0.9630301773548127
  batch 100 loss: 0.9599323201179505
  batch 150 loss: 0.9810616242885589
  batch 200 loss: 1.004770120382309
  batch 250 loss: 0.9361495685577392
  batch 300 loss: 0.9878145039081574
  batch 350 loss: 0.9207836282253266
  batch 400 loss: 0.9238830506801605
  batch 450 loss: 0.9519092273712159
  batch 500 loss: 0.9180851995944976
  batch 550 loss: 0.9518557906150817
  batch 600 loss: 0.9661187589168548
  batch 650 loss: 1.032746880054474
  batch 700 loss: 0.994930317401886
  batch 750 loss: 0.9220100712776184
  batch 800 loss: 0.9700504326820374
  batch 850 loss: 0.9801099717617034
  batch 900 loss: 0.9292941462993621
LOSS train 0.92929 valid 0.94231, valid PER 28.92%
EPOCH 6:
  batch 50 loss: 0.9055508744716644
  batch 100 loss: 0.912553437948227
  batch 150 loss: 0.8840396070480346
  batch 200 loss: 0.8623007607460021
  batch 250 loss: 0.906834682226181
  batch 300 loss: 0.9368224585056305
  batch 350 loss: 0.9108094835281372
  batch 400 loss: 0.8816476118564606
  batch 450 loss: 0.9169082129001618
  batch 500 loss: 0.8847095429897308
  batch 550 loss: 0.9220889902114868
  batch 600 loss: 0.8823714971542358
  batch 650 loss: 0.8774287211894989
  batch 700 loss: 0.8805092680454254
  batch 750 loss: 0.9079530847072601
  batch 800 loss: 0.9192153632640838
  batch 850 loss: 0.9332570564746857
  batch 900 loss: 0.9532029390335083
LOSS train 0.95320 valid 0.89734, valid PER 28.06%
EPOCH 7:
  batch 50 loss: 0.8412010741233825
  batch 100 loss: 0.8895278775691986
  batch 150 loss: 0.8374273669719696
  batch 200 loss: 0.8556047463417054
  batch 250 loss: 0.9108536314964294
  batch 300 loss: 0.8195618855953216
  batch 350 loss: 0.8876702296733856
  batch 400 loss: 0.8578993451595306
  batch 450 loss: 0.8738225543498993
  batch 500 loss: 0.866957105398178
  batch 550 loss: 0.8407168078422547
  batch 600 loss: 0.8606768226623536
  batch 650 loss: 0.8434741234779358
  batch 700 loss: 0.9025319910049439
  batch 750 loss: 0.8741650795936584
  batch 800 loss: 0.8586245238780975
  batch 850 loss: 0.8466602921485901
  batch 900 loss: 0.8672251510620117
LOSS train 0.86723 valid 0.86856, valid PER 27.48%
EPOCH 8:
  batch 50 loss: 0.8188435912132264
  batch 100 loss: 0.8001690208911896
  batch 150 loss: 0.8237969934940338
  batch 200 loss: 0.8453786706924439
  batch 250 loss: 0.8397480177879334
  batch 300 loss: 0.8032105779647827
  batch 350 loss: 0.7819504296779632
  batch 400 loss: 0.7829823118448257
  batch 450 loss: 0.8428264236450196
  batch 500 loss: 0.8295058989524842
  batch 550 loss: 0.8181242430210114
  batch 600 loss: 0.7785982596874237
  batch 650 loss: 0.8247173464298249
  batch 700 loss: 0.8279852044582366
  batch 750 loss: 0.8123897063732147
  batch 800 loss: 0.8167817652225494
  batch 850 loss: 0.7918650078773498
  batch 900 loss: 0.8036429822444916
LOSS train 0.80364 valid 0.84133, valid PER 26.09%
EPOCH 9:
  batch 50 loss: 0.7673014545440674
  batch 100 loss: 0.7610271728038788
  batch 150 loss: 0.7845934963226319
  batch 200 loss: 0.7728442132472992
  batch 250 loss: 0.7587776005268096
  batch 300 loss: 0.7677650332450867
  batch 350 loss: 0.7558796846866608
  batch 400 loss: 0.8207943499088287
  batch 450 loss: 0.8181204581260682
  batch 500 loss: 0.7857196295261383
  batch 550 loss: 0.778258032798767
  batch 600 loss: 0.8350080144405365
  batch 650 loss: 0.7954464948177338
  batch 700 loss: 0.8175035035610199
  batch 750 loss: 0.8172162675857544
  batch 800 loss: 0.8396839082241059
  batch 850 loss: 0.79709299325943
  batch 900 loss: 0.7945478010177612
LOSS train 0.79455 valid 0.82412, valid PER 25.86%
EPOCH 10:
  batch 50 loss: 0.7437550246715545
  batch 100 loss: 0.7419799649715424
  batch 150 loss: 0.7706563878059387
  batch 200 loss: 0.7599544167518616
  batch 250 loss: 0.7630560052394867
  batch 300 loss: 0.7392232668399811
  batch 350 loss: 0.7509400129318238
  batch 400 loss: 0.7299806863069535
  batch 450 loss: 0.7229967164993286
  batch 500 loss: 0.7447593474388122
  batch 550 loss: 0.7617079496383667
  batch 600 loss: 0.7420562064647674
  batch 650 loss: 0.7422144293785096
  batch 700 loss: 0.7617826956510544
  batch 750 loss: 0.76290567278862
  batch 800 loss: 0.7614629387855529
  batch 850 loss: 0.7468655610084534
  batch 900 loss: 0.7298051434755325
LOSS train 0.72981 valid 0.81516, valid PER 25.10%
EPOCH 11:
  batch 50 loss: 0.7163750982284546
  batch 100 loss: 0.7084228563308715
  batch 150 loss: 0.7143886655569076
  batch 200 loss: 0.6768962073326111
  batch 250 loss: 0.6947426640987396
  batch 300 loss: 0.7026859349012375
  batch 350 loss: 0.7236608111858368
  batch 400 loss: 0.711248687505722
  batch 450 loss: 0.6912256550788879
  batch 500 loss: 0.7052287673950195
  batch 550 loss: 0.7233868074417115
  batch 600 loss: 0.7069901657104493
  batch 650 loss: 0.7268695056438446
  batch 700 loss: 0.7810762083530426
  batch 750 loss: 0.709751695394516
  batch 800 loss: 0.7534984809160232
  batch 850 loss: 0.7080564951896667
  batch 900 loss: 0.740609472990036
LOSS train 0.74061 valid 0.80400, valid PER 24.25%
EPOCH 12:
  batch 50 loss: 0.6704952609539032
  batch 100 loss: 0.668999131321907
  batch 150 loss: 0.7027497673034668
  batch 200 loss: 0.6909585285186768
  batch 250 loss: 0.7030760538578034
  batch 300 loss: 0.7754799836874008
  batch 350 loss: 0.7064178299903869
  batch 400 loss: 0.7247518426179886
  batch 450 loss: 0.704374514222145
  batch 500 loss: 0.7333629512786866
  batch 550 loss: 0.6955543756484985
  batch 600 loss: 0.702455712556839
  batch 650 loss: 0.7089111888408661
  batch 700 loss: 0.7125076860189438
  batch 750 loss: 0.7217138797044754
  batch 800 loss: 0.6669150757789611
  batch 850 loss: 0.7182072824239731
  batch 900 loss: 0.6942028379440308
LOSS train 0.69420 valid 0.78117, valid PER 23.60%
EPOCH 13:
  batch 50 loss: 0.632700667977333
  batch 100 loss: 0.6525151789188385
  batch 150 loss: 0.6936368387937546
  batch 200 loss: 0.6585668951272965
  batch 250 loss: 0.6905514127016068
  batch 300 loss: 0.6923019659519195
  batch 350 loss: 0.6622260922193527
  batch 400 loss: 0.6654407787322998
  batch 450 loss: 0.6664500039815903
  batch 500 loss: 0.6487980151176452
  batch 550 loss: 0.6880608075857162
  batch 600 loss: 0.6686122512817383
  batch 650 loss: 0.686191258430481
  batch 700 loss: 0.7196579813957215
  batch 750 loss: 0.6838278079032898
  batch 800 loss: 0.6665239888429642
  batch 850 loss: 0.6723297941684723
  batch 900 loss: 0.7196770530939102
LOSS train 0.71968 valid 0.78831, valid PER 24.20%
EPOCH 14:
  batch 50 loss: 0.6499349796772003
  batch 100 loss: 0.651667304635048
  batch 150 loss: 0.6411439645290374
  batch 200 loss: 0.6588043975830078
  batch 250 loss: 0.6637407594919205
  batch 300 loss: 0.641188383102417
  batch 350 loss: 0.6376180619001388
  batch 400 loss: 0.6604058295488358
  batch 450 loss: 0.6049802327156066
  batch 500 loss: 0.6552527952194214
  batch 550 loss: 0.6491960859298707
  batch 600 loss: 0.6316014635562897
  batch 650 loss: 0.6614971023797989
  batch 700 loss: 0.6384420698881149
  batch 750 loss: 0.6415163093805313
  batch 800 loss: 0.6598540592193604
  batch 850 loss: 0.7076626574993133
  batch 900 loss: 0.6702759963274002
LOSS train 0.67028 valid 0.75455, valid PER 23.38%
EPOCH 15:
  batch 50 loss: 0.6021572667360305
  batch 100 loss: 0.5984982854127884
  batch 150 loss: 0.5985020971298218
  batch 200 loss: 0.6315294969081878
  batch 250 loss: 0.6419177877902985
  batch 300 loss: 0.6452796512842178
  batch 350 loss: 0.6078385192155839
  batch 400 loss: 0.6249430799484252
  batch 450 loss: 0.6382741671800614
  batch 500 loss: 0.6314822435379028
  batch 550 loss: 0.6909829658269883
  batch 600 loss: 0.6794261574745178
  batch 650 loss: 0.6147793740034103
  batch 700 loss: 0.6209675288200378
  batch 750 loss: 0.6474354660511017
  batch 800 loss: 0.6205670291185379
  batch 850 loss: 0.6258843749761581
  batch 900 loss: 0.6182771468162537
LOSS train 0.61828 valid 0.75036, valid PER 23.36%
EPOCH 16:
  batch 50 loss: 0.632089307308197
  batch 100 loss: 0.5805621993541717
  batch 150 loss: 0.6136607366800308
  batch 200 loss: 0.6361523133516311
  batch 250 loss: 0.6164423733949661
  batch 300 loss: 0.600631052851677
  batch 350 loss: 0.6090509009361267
  batch 400 loss: 0.6050177294015885
  batch 450 loss: 0.590206663608551
  batch 500 loss: 0.6075342696905136
  batch 550 loss: 0.5814700561761856
  batch 600 loss: 0.6457542693614959
  batch 650 loss: 0.6392093503475189
  batch 700 loss: 0.6192786777019501
  batch 750 loss: 0.6117863124608993
  batch 800 loss: 0.625397977232933
  batch 850 loss: 0.6227050191164016
  batch 900 loss: 0.6545297282934189
LOSS train 0.65453 valid 0.73635, valid PER 22.54%
EPOCH 17:
  batch 50 loss: 0.574177463054657
  batch 100 loss: 0.5630617302656173
  batch 150 loss: 0.6276948946714401
  batch 200 loss: 0.5662861347198487
  batch 250 loss: 0.5875556087493896
  batch 300 loss: 0.5683100247383117
  batch 350 loss: 0.6107388937473297
  batch 400 loss: 0.6506181037425995
  batch 450 loss: 0.5906170731782914
  batch 500 loss: 0.631602446436882
  batch 550 loss: 0.5895855301618576
  batch 600 loss: 0.6275960463285446
  batch 650 loss: 0.5940661841630935
  batch 700 loss: 0.6360136514902115
  batch 750 loss: 0.6142671519517898
  batch 800 loss: 0.6426159137487412
  batch 850 loss: 0.6291144871711731
  batch 900 loss: 0.6049904626607895
LOSS train 0.60499 valid 0.72630, valid PER 22.13%
EPOCH 18:
  batch 50 loss: 0.5404769515991211
  batch 100 loss: 0.5437939584255218
  batch 150 loss: 0.6090167182683944
  batch 200 loss: 0.5709100466966629
  batch 250 loss: 0.5526815056800842
  batch 300 loss: 0.5621009802818299
  batch 350 loss: 0.5441442942619323
  batch 400 loss: 0.575026575922966
  batch 450 loss: 0.5836721807718277
  batch 500 loss: 0.5772723776102066
  batch 550 loss: 0.6007945662736893
  batch 600 loss: 0.5874720150232315
  batch 650 loss: 0.577128244638443
  batch 700 loss: 0.5533317160606385
  batch 750 loss: 0.5737602084875106
  batch 800 loss: 0.5942346006631851
  batch 850 loss: 0.5897022151947021
  batch 900 loss: 0.5832093590497971
LOSS train 0.58321 valid 0.73641, valid PER 21.96%
EPOCH 19:
  batch 50 loss: 0.5649534583091735
  batch 100 loss: 0.554956887960434
  batch 150 loss: 0.5168994826078415
  batch 200 loss: 0.5164531034231186
  batch 250 loss: 0.5537997788190842
  batch 300 loss: 0.5713226515054702
  batch 350 loss: 0.571553965806961
  batch 400 loss: 0.5386319380998611
  batch 450 loss: 0.5378724324703217
  batch 500 loss: 0.5502490043640137
  batch 550 loss: 0.5905544954538345
  batch 600 loss: 0.5809727483987808
  batch 650 loss: 0.5851327782869339
  batch 700 loss: 0.6222158885002136
  batch 750 loss: 0.5743799102306366
  batch 800 loss: 0.5501650404930115
  batch 850 loss: 0.5454327255487442
  batch 900 loss: 0.5741004258394241
LOSS train 0.57410 valid 0.72764, valid PER 21.51%
EPOCH 20:
  batch 50 loss: 0.5013752454519271
  batch 100 loss: 0.5513619905710221
  batch 150 loss: 0.5384939604997635
  batch 200 loss: 0.5481196135282517
  batch 250 loss: 0.5349135315418243
  batch 300 loss: 0.5197802400588989
  batch 350 loss: 0.5127781307697297
  batch 400 loss: 0.5178173476457596
  batch 450 loss: 0.49704995572566985
  batch 500 loss: 0.5486645889282227
  batch 550 loss: 0.5618080931901932
  batch 600 loss: 0.5448174756765366
  batch 650 loss: 0.5600893408060074
  batch 700 loss: 0.5323961818218231
  batch 750 loss: 0.5201767724752426
  batch 800 loss: 0.5523181289434433
  batch 850 loss: 0.5427059876918793
  batch 900 loss: 0.5362995278835296
LOSS train 0.53630 valid 0.73015, valid PER 21.44%
[1.8322360825538635, 1.2342918229103088, 1.1144323480129241, 1.0452309262752533, 0.9292941462993621, 0.9532029390335083, 0.8672251510620117, 0.8036429822444916, 0.7945478010177612, 0.7298051434755325, 0.740609472990036, 0.6942028379440308, 0.7196770530939102, 0.6702759963274002, 0.6182771468162537, 0.6545297282934189, 0.6049904626607895, 0.5832093590497971, 0.5741004258394241, 0.5362995278835296]
[1.7468408346176147, 1.1901437044143677, 1.0732789039611816, 1.0057095289230347, 0.9423147439956665, 0.897344708442688, 0.8685603141784668, 0.841329038143158, 0.8241166472434998, 0.8151559829711914, 0.8039982318878174, 0.7811683416366577, 0.7883115410804749, 0.7545483708381653, 0.7503631711006165, 0.7363486289978027, 0.7262965440750122, 0.7364096641540527, 0.7276408076286316, 0.7301537990570068]
Training finished in 26.0 minutes.
Model saved to checkpoints/20230125_110733/model_17
Loading model from checkpoints/20230125_110733/model_17
SUB: 15.40%, DEL: 5.87%, INS: 2.61%, COR: 78.73%, PER: 23.88%
