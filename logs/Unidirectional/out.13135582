Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.001, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.2, beta1=0.9, beta2=0.999)
Total number of model parameters is 83496
EPOCH 1:
  batch 50 loss: 10.314202132225036
  batch 100 loss: 3.337176284790039
  batch 150 loss: 3.3114083766937257
  batch 200 loss: 3.291543984413147
  batch 250 loss: 3.25989550113678
  batch 300 loss: 3.2228790712356568
  batch 350 loss: 3.166677780151367
  batch 400 loss: 3.1006416273117066
  batch 450 loss: 2.991227140426636
  batch 500 loss: 2.868099012374878
  batch 550 loss: 2.771282205581665
  batch 600 loss: 2.694200439453125
  batch 650 loss: 2.6248177003860476
  batch 700 loss: 2.5966594648361205
  batch 750 loss: 2.5414496088027954
  batch 800 loss: 2.5139312028884886
  batch 850 loss: 2.479139904975891
  batch 900 loss: 2.438112025260925
LOSS train 2.43811 valid 2.39839, valid PER 84.03%
EPOCH 2:
  batch 50 loss: 2.425482358932495
  batch 100 loss: 2.378295373916626
  batch 150 loss: 2.29144540309906
  batch 200 loss: 2.2945349836349487
  batch 250 loss: 2.305821099281311
  batch 300 loss: 2.2606987380981445
  batch 350 loss: 2.2482218837738035
  batch 400 loss: 2.2086589789390563
  batch 450 loss: 2.2092953586578368
  batch 500 loss: 2.1895059633255003
  batch 550 loss: 2.184029541015625
  batch 600 loss: 2.143733696937561
  batch 650 loss: 2.1057103300094604
  batch 700 loss: 2.104897713661194
  batch 750 loss: 2.0857055163383484
  batch 800 loss: 2.054045193195343
  batch 850 loss: 2.0723569869995115
  batch 900 loss: 2.0246157169342043
LOSS train 2.02462 valid 2.01032, valid PER 78.05%
EPOCH 3:
  batch 50 loss: 1.9834049773216247
  batch 100 loss: 2.017247896194458
  batch 150 loss: 2.009770197868347
  batch 200 loss: 1.9362355637550355
  batch 250 loss: 1.9361237406730651
  batch 300 loss: 1.9215580248832702
  batch 350 loss: 1.9381840205192566
  batch 400 loss: 1.91654456615448
  batch 450 loss: 1.8921467518806458
  batch 500 loss: 1.8608842253684998
  batch 550 loss: 1.8544504475593566
  batch 600 loss: 1.818266680240631
  batch 650 loss: 1.8286911273002624
  batch 700 loss: 1.810546998977661
  batch 750 loss: 1.8432420921325683
  batch 800 loss: 1.8456619358062745
  batch 850 loss: 1.7992174482345582
  batch 900 loss: 1.7885396695137024
LOSS train 1.78854 valid 1.77930, valid PER 69.02%
EPOCH 4:
  batch 50 loss: 1.7845994806289673
  batch 100 loss: 1.7273995804786682
  batch 150 loss: 1.7548866653442383
  batch 200 loss: 1.7398026728630065
  batch 250 loss: 1.7695582437515258
  batch 300 loss: 1.7396256232261658
  batch 350 loss: 1.7167676162719727
  batch 400 loss: 1.7025635623931885
  batch 450 loss: 1.7125062990188598
  batch 500 loss: 1.7315977215766907
  batch 550 loss: 1.6787316513061523
  batch 600 loss: 1.666825385093689
  batch 650 loss: 1.723085699081421
  batch 700 loss: 1.7334586238861085
  batch 750 loss: 1.6379961013793944
  batch 800 loss: 1.6642064094543456
  batch 850 loss: 1.6338437342643737
  batch 900 loss: 1.6442851972579957
LOSS train 1.64429 valid 1.62064, valid PER 60.51%
EPOCH 5:
  batch 50 loss: 1.645035617351532
  batch 100 loss: 1.6077262663841247
  batch 150 loss: 1.638673846721649
  batch 200 loss: 1.637970633506775
  batch 250 loss: 1.5999475812911987
  batch 300 loss: 1.616108877658844
  batch 350 loss: 1.5750876784324646
  batch 400 loss: 1.5520468473434448
  batch 450 loss: 1.5641925358772277
  batch 500 loss: 1.5372000432014465
  batch 550 loss: 1.5895743227005006
  batch 600 loss: 1.5981838059425355
  batch 650 loss: 1.5795101857185363
  batch 700 loss: 1.587397632598877
  batch 750 loss: 1.5245253562927246
  batch 800 loss: 1.594858181476593
  batch 850 loss: 1.563427906036377
  batch 900 loss: 1.5101500296592711
LOSS train 1.51015 valid 1.52137, valid PER 53.99%
EPOCH 6:
  batch 50 loss: 1.5184824633598328
  batch 100 loss: 1.5291058039665222
  batch 150 loss: 1.5199155712127685
  batch 200 loss: 1.479088809490204
  batch 250 loss: 1.5370607161521912
  batch 300 loss: 1.5366161417961122
  batch 350 loss: 1.5251607227325439
  batch 400 loss: 1.496722433567047
  batch 450 loss: 1.5108156704902649
  batch 500 loss: 1.487213830947876
  batch 550 loss: 1.5054407024383545
  batch 600 loss: 1.4713152503967286
  batch 650 loss: 1.442829818725586
  batch 700 loss: 1.44965726852417
  batch 750 loss: 1.4945000362396241
  batch 800 loss: 1.4688800692558288
  batch 850 loss: 1.4881819629669188
  batch 900 loss: 1.4877533030509948
LOSS train 1.48775 valid 1.43026, valid PER 49.69%
EPOCH 7:
  batch 50 loss: 1.4512618160247803
  batch 100 loss: 1.5110774636268616
  batch 150 loss: 1.4222913098335266
  batch 200 loss: 1.4093652868270874
  batch 250 loss: 1.477639000415802
  batch 300 loss: 1.4347977232933045
  batch 350 loss: 1.459740993976593
  batch 400 loss: 1.416601278781891
  batch 450 loss: 1.444626097679138
  batch 500 loss: 1.413096911907196
  batch 550 loss: 1.4055417680740356
  batch 600 loss: 1.4114778661727905
  batch 650 loss: 1.3658275771141053
  batch 700 loss: 1.4465247464179993
  batch 750 loss: 1.3997378277778625
  batch 800 loss: 1.4062542986869813
  batch 850 loss: 1.3849560308456421
  batch 900 loss: 1.4139719104766846
LOSS train 1.41397 valid 1.39748, valid PER 47.61%
EPOCH 8:
  batch 50 loss: 1.3979225945472717
  batch 100 loss: 1.3758500719070434
  batch 150 loss: 1.4153300166130065
  batch 200 loss: 1.3905328273773194
  batch 250 loss: 1.3835113191604613
  batch 300 loss: 1.3424402117729186
  batch 350 loss: 1.3525096487998962
  batch 400 loss: 1.3370893800258636
  batch 450 loss: 1.4082718420028686
  batch 500 loss: 1.3885784792900084
  batch 550 loss: 1.3768762683868407
  batch 600 loss: 1.3437543439865112
  batch 650 loss: 1.350405020713806
  batch 700 loss: 1.388011863231659
  batch 750 loss: 1.3785569536685944
  batch 800 loss: 1.3690294480323792
  batch 850 loss: 1.3454304337501526
  batch 900 loss: 1.3650605273246765
LOSS train 1.36506 valid 1.34016, valid PER 44.54%
EPOCH 9:
  batch 50 loss: 1.3341868543624877
  batch 100 loss: 1.3283317613601684
  batch 150 loss: 1.3421443057060243
  batch 200 loss: 1.2926533246040344
  batch 250 loss: 1.2992205333709717
  batch 300 loss: 1.3265102815628051
  batch 350 loss: 1.313079309463501
  batch 400 loss: 1.3556479978561402
  batch 450 loss: 1.3339588379859924
  batch 500 loss: 1.327846384048462
  batch 550 loss: 1.306906864643097
  batch 600 loss: 1.366874530315399
  batch 650 loss: 1.3374199330806733
  batch 700 loss: 1.323051495552063
  batch 750 loss: 1.3115957057476044
  batch 800 loss: 1.354880781173706
  batch 850 loss: 1.3231420874595643
  batch 900 loss: 1.2924992060661316
LOSS train 1.29250 valid 1.27815, valid PER 41.55%
EPOCH 10:
  batch 50 loss: 1.2823332953453064
  batch 100 loss: 1.3317985224723816
  batch 150 loss: 1.3397493934631348
  batch 200 loss: 1.277666790485382
  batch 250 loss: 1.2892577481269836
  batch 300 loss: 1.2941927433013916
  batch 350 loss: 1.2920776116847992
  batch 400 loss: 1.268162977695465
  batch 450 loss: 1.2544402694702148
  batch 500 loss: 1.2819081890583037
  batch 550 loss: 1.2859253656864167
  batch 600 loss: 1.2752524280548097
  batch 650 loss: 1.3007715094089507
  batch 700 loss: 1.320139856338501
  batch 750 loss: 1.307514840364456
  batch 800 loss: 1.2969362151622772
  batch 850 loss: 1.2938059735298157
  batch 900 loss: 1.2780080318450928
LOSS train 1.27801 valid 1.28187, valid PER 41.25%
EPOCH 11:
  batch 50 loss: 1.301409729719162
  batch 100 loss: 1.2606473028659821
  batch 150 loss: 1.2349754309654235
  batch 200 loss: 1.2531086468696595
  batch 250 loss: 1.2371953427791595
  batch 300 loss: 1.2617901408672332
  batch 350 loss: 1.2984381127357483
  batch 400 loss: 1.2416833531856537
  batch 450 loss: 1.2512639689445495
  batch 500 loss: 1.2481684041023255
  batch 550 loss: 1.2816103374958039
  batch 600 loss: 1.2540791654586791
  batch 650 loss: 1.25944433927536
  batch 700 loss: 1.333492374420166
  batch 750 loss: 1.24898304104805
  batch 800 loss: 1.2600338077545166
  batch 850 loss: 1.2483955252170562
  batch 900 loss: 1.280561044216156
LOSS train 1.28056 valid 1.26461, valid PER 40.31%
EPOCH 12:
  batch 50 loss: 1.2499263346195222
  batch 100 loss: 1.2045398151874542
  batch 150 loss: 1.2363600206375123
  batch 200 loss: 1.2596369111537933
  batch 250 loss: 1.2302505731582642
  batch 300 loss: 1.2540995287895202
  batch 350 loss: 1.2204853582382202
  batch 400 loss: 1.2412912368774414
  batch 450 loss: 1.237006003856659
  batch 500 loss: 1.2407925605773926
  batch 550 loss: 1.249254035949707
  batch 600 loss: 1.2318618559837342
  batch 650 loss: 1.2479998207092284
  batch 700 loss: 1.240453803539276
  batch 750 loss: 1.2535089945793152
  batch 800 loss: 1.2175280117988587
  batch 850 loss: 1.2227630341053009
  batch 900 loss: 1.207187750339508
LOSS train 1.20719 valid 1.22050, valid PER 38.77%
EPOCH 13:
  batch 50 loss: 1.2093417263031006
  batch 100 loss: 1.2223437762260436
  batch 150 loss: 1.245886652469635
  batch 200 loss: 1.192317247390747
  batch 250 loss: 1.1827108705043792
  batch 300 loss: 1.2267042875289917
  batch 350 loss: 1.2071957886219025
  batch 400 loss: 1.2190488958358765
  batch 450 loss: 1.2029238152503967
  batch 500 loss: 1.1913630080223083
  batch 550 loss: 1.2507270359992981
  batch 600 loss: 1.2164106225967408
  batch 650 loss: 1.2026514446735381
  batch 700 loss: 1.2423738157749176
  batch 750 loss: 1.1728237140178681
  batch 800 loss: 1.2227438688278198
  batch 850 loss: 1.2045041847229003
  batch 900 loss: 1.1869223093986512
LOSS train 1.18692 valid 1.19881, valid PER 38.40%
EPOCH 14:
  batch 50 loss: 1.1861500906944276
  batch 100 loss: 1.1724885034561157
  batch 150 loss: 1.1936325895786286
  batch 200 loss: 1.1886718690395355
  batch 250 loss: 1.1908288156986238
  batch 300 loss: 1.1745477283000947
  batch 350 loss: 1.1869218611717225
  batch 400 loss: 1.2013678026199341
  batch 450 loss: 1.1821563196182252
  batch 500 loss: 1.2026366865634919
  batch 550 loss: 1.1929722213745118
  batch 600 loss: 1.1808796179294587
  batch 650 loss: 1.2002758252620698
  batch 700 loss: 1.2481467187404633
  batch 750 loss: 1.2084945917129517
  batch 800 loss: 1.204323606491089
  batch 850 loss: 1.23316100358963
  batch 900 loss: 1.1804657971858978
LOSS train 1.18047 valid 1.18753, valid PER 38.21%
EPOCH 15:
  batch 50 loss: 1.1553733551502228
  batch 100 loss: 1.1946288204193116
  batch 150 loss: 1.1702783918380737
  batch 200 loss: 1.1960009849071502
  batch 250 loss: 1.1936737275123597
  batch 300 loss: 1.1724295544624328
  batch 350 loss: 1.1518009281158448
  batch 400 loss: 1.1655749678611755
  batch 450 loss: 1.1840024924278258
  batch 500 loss: 1.1949895167350768
  batch 550 loss: 1.2008748602867128
  batch 600 loss: 1.2030469024181365
  batch 650 loss: 1.1536997985839843
  batch 700 loss: 1.1731369233131408
  batch 750 loss: 1.207521333694458
  batch 800 loss: 1.1625505113601684
  batch 850 loss: 1.1369236862659455
  batch 900 loss: 1.1147495520114898
LOSS train 1.11475 valid 1.18315, valid PER 38.23%
EPOCH 16:
  batch 50 loss: 1.1382502400875092
  batch 100 loss: 1.144027544260025
  batch 150 loss: 1.1482166576385497
  batch 200 loss: 1.191712534427643
  batch 250 loss: 1.1685665237903595
  batch 300 loss: 1.213971838951111
  batch 350 loss: 1.1806989872455598
  batch 400 loss: 1.1439044713973998
  batch 450 loss: 1.141741806268692
  batch 500 loss: 1.1602178049087524
  batch 550 loss: 1.1519439721107483
  batch 600 loss: 1.1577245151996614
  batch 650 loss: 1.1743609702587128
  batch 700 loss: 1.1785031747817993
  batch 750 loss: 1.1589013254642486
  batch 800 loss: 1.147556473016739
  batch 850 loss: 1.167968192100525
  batch 900 loss: 1.173584201335907
LOSS train 1.17358 valid 1.16410, valid PER 37.57%
EPOCH 17:
  batch 50 loss: 1.134180543422699
  batch 100 loss: 1.113043247461319
  batch 150 loss: 1.1755336260795592
  batch 200 loss: 1.1285992765426636
  batch 250 loss: 1.1442661201953888
  batch 300 loss: 1.1717677462100982
  batch 350 loss: 1.1454455029964448
  batch 400 loss: 1.1652405774593353
  batch 450 loss: 1.1480244648456575
  batch 500 loss: 1.1624651753902435
  batch 550 loss: 1.1279192519187928
  batch 600 loss: 1.1687107515335082
  batch 650 loss: 1.1238870525360107
  batch 700 loss: 1.1391005718708038
  batch 750 loss: 1.099270786046982
  batch 800 loss: 1.1500814497470855
  batch 850 loss: 1.1471190810203553
  batch 900 loss: 1.1472480905056
LOSS train 1.14725 valid 1.14499, valid PER 36.88%
EPOCH 18:
  batch 50 loss: 1.1330654323101044
  batch 100 loss: 1.1313539600372315
  batch 150 loss: 1.154516419172287
  batch 200 loss: 1.116308436393738
  batch 250 loss: 1.1115129160881043
  batch 300 loss: 1.2001907193660737
  batch 350 loss: 1.153405293226242
  batch 400 loss: 1.1105967235565186
  batch 450 loss: 1.1335246074199676
  batch 500 loss: 1.138579590320587
  batch 550 loss: 1.1701436161994934
  batch 600 loss: 1.133934472799301
  batch 650 loss: 1.0799559473991394
  batch 700 loss: 1.0943800103664398
  batch 750 loss: 1.1376295578479767
  batch 800 loss: 1.1394472408294678
  batch 850 loss: 1.1322639310359954
  batch 900 loss: 1.1266386330127716
LOSS train 1.12664 valid 1.15209, valid PER 37.32%
EPOCH 19:
  batch 50 loss: 1.1351611053943633
  batch 100 loss: 1.1280609822273255
  batch 150 loss: 1.092641931772232
  batch 200 loss: 1.101168372631073
  batch 250 loss: 1.1554808437824249
  batch 300 loss: 1.1258233439922334
  batch 350 loss: 1.1529354465007782
  batch 400 loss: 1.108818508386612
  batch 450 loss: 1.0657728159427642
  batch 500 loss: 1.1027190685272217
  batch 550 loss: 1.1301403093338012
  batch 600 loss: 1.1625698375701905
  batch 650 loss: 1.1366022312641144
  batch 700 loss: 1.1306035375595094
  batch 750 loss: 1.0781834173202514
  batch 800 loss: 1.0728559958934785
  batch 850 loss: 1.1131411957740784
  batch 900 loss: 1.10846088886261
LOSS train 1.10846 valid 1.11950, valid PER 35.96%
EPOCH 20:
  batch 50 loss: 1.050316582918167
  batch 100 loss: 1.1448927688598634
  batch 150 loss: 1.1358948397636413
  batch 200 loss: 1.098214373588562
  batch 250 loss: 1.1454108369350433
  batch 300 loss: 1.100961972475052
  batch 350 loss: 1.116954482793808
  batch 400 loss: 1.0897188889980316
  batch 450 loss: 1.0967223024368287
  batch 500 loss: 1.1049691724777222
  batch 550 loss: 1.1071442210674285
  batch 600 loss: 1.077354462146759
  batch 650 loss: 1.1200359511375426
  batch 700 loss: 1.1062304878234863
  batch 750 loss: 1.0944813978672028
  batch 800 loss: 1.1161539900302886
  batch 850 loss: 1.114054675102234
  batch 900 loss: 1.092357907295227
LOSS train 1.09236 valid 1.11680, valid PER 35.78%
[2.438112025260925, 2.0246157169342043, 1.7885396695137024, 1.6442851972579957, 1.5101500296592711, 1.4877533030509948, 1.4139719104766846, 1.3650605273246765, 1.2924992060661316, 1.2780080318450928, 1.280561044216156, 1.207187750339508, 1.1869223093986512, 1.1804657971858978, 1.1147495520114898, 1.173584201335907, 1.1472480905056, 1.1266386330127716, 1.10846088886261, 1.092357907295227]
[tensor(2.3984, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.0103, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.7793, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.6206, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.5214, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.4303, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.3975, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.3402, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.2782, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.2819, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.2646, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.2205, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.1988, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.1875, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.1831, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.1641, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.1450, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.1521, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.1195, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.1168, device='cuda:0', grad_fn=<DivBackward0>)]
Training finished in 3.0 minutes.
Model saved to checkpoints/20230123_231438/model_20
Loading model from checkpoints/20230123_231438/model_20
SUB: 20.48%, DEL: 15.14%, INS: 1.93%, COR: 64.38%, PER: 37.55%
