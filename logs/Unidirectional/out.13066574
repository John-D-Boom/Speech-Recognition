Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=128, concat=1, lr=0.5, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.2, beta1=0.9, beta2=0.999)
Total number of model parameters is 83496
EPOCH 1:
  batch 50 loss: 20.293206329345704
  batch 100 loss: 17.503995113372802
  batch 150 loss: 14.235924415588379
  batch 200 loss: 9.531366519927978
  batch 250 loss: 7.664192838668823
  batch 300 loss: 15.016223373413085
  batch 350 loss: 17.00246156692505
  batch 400 loss: 20.806487674713136
  batch 450 loss: 16.195086708068846
  batch 500 loss: 10.311052522659303
  batch 550 loss: 12.040383434295654
  batch 600 loss: 9.404054288864137
  batch 650 loss: 8.207687301635742
  batch 700 loss: 8.2998814868927
  batch 750 loss: 9.018911333084107
  batch 800 loss: 7.879744777679443
  batch 850 loss: 6.370802307128907
  batch 900 loss: 6.538977775573731
LOSS train 6.53898 valid 7.32816, valid PER 90.73%
EPOCH 2:
  batch 50 loss: 6.121059093475342
  batch 100 loss: 7.285569801330566
  batch 150 loss: 18.882939491271973
  batch 200 loss: 17.156649036407472
  batch 250 loss: 21.786119079589845
  batch 300 loss: 18.73368921279907
  batch 350 loss: 27.496547870635986
  batch 400 loss: 27.831396331787108
  batch 450 loss: 43.4786141204834
  batch 500 loss: 30.306910858154296
  batch 550 loss: 21.6674715423584
  batch 600 loss: 21.367087478637696
  batch 650 loss: 14.658766326904297
  batch 700 loss: 13.562198734283447
  batch 750 loss: 13.286198539733887
  batch 800 loss: 11.257294387817383
  batch 850 loss: 16.307130069732665
  batch 900 loss: 9.82231595993042
LOSS train 9.82232 valid 14.02343, valid PER 78.28%
EPOCH 3:
  batch 50 loss: 11.654850425720214
  batch 100 loss: 13.150648097991944
  batch 150 loss: 12.812810840606689
  batch 200 loss: 15.055463829040526
  batch 250 loss: 10.525616245269775
  batch 300 loss: 11.287872009277343
  batch 350 loss: 9.363788270950318
  batch 400 loss: 6.49271180152893
  batch 450 loss: 9.234509239196777
  batch 500 loss: 7.72115119934082
  batch 550 loss: 6.044934930801392
  batch 600 loss: 4.935590310096741
  batch 650 loss: 5.186635322570801
  batch 700 loss: 5.874890985488892
  batch 750 loss: 7.040308408737182
  batch 800 loss: 7.176367778778076
  batch 850 loss: 8.877823219299316
  batch 900 loss: 7.7776182270050045
LOSS train 7.77762 valid 6.61174, valid PER 87.93%
EPOCH 4:
  batch 50 loss: 7.613656148910523
  batch 100 loss: 9.001227407455444
  batch 150 loss: 7.56672553062439
  batch 200 loss: 8.037536325454711
  batch 250 loss: 7.283018636703491
  batch 300 loss: 7.215167512893677
  batch 350 loss: 6.6671301364898685
  batch 400 loss: 6.8003793048858645
  batch 450 loss: 6.517428770065307
  batch 500 loss: 6.993561248779297
  batch 550 loss: 5.240455579757691
  batch 600 loss: 5.250803527832031
  batch 650 loss: 5.43906753540039
  batch 700 loss: 5.493992357254029
  batch 750 loss: 5.145921125411987
  batch 800 loss: 5.808285350799561
  batch 850 loss: 5.962889385223389
  batch 900 loss: 6.080533084869384
LOSS train 6.08053 valid 11.48783, valid PER 75.36%
EPOCH 5:
  batch 50 loss: 12.12669900894165
  batch 100 loss: 14.945181198120117
  batch 150 loss: 30.546727104187013
  batch 200 loss: 33.20252063751221
  batch 250 loss: 27.012503223419188
  batch 300 loss: 20.675965309143066
  batch 350 loss: 22.10039134979248
  batch 400 loss: 25.190159683227538
  batch 450 loss: 30.43585060119629
  batch 500 loss: 27.343307609558106
  batch 550 loss: 36.47252456665039
  batch 600 loss: 35.35975242614746
  batch 650 loss: 30.55682201385498
  batch 700 loss: 48.53628654479981
  batch 750 loss: 51.01773422241211
  batch 800 loss: 44.77200927734375
  batch 850 loss: 37.39744529724121
  batch 900 loss: 29.870565185546877
LOSS train 29.87057 valid 40.80455, valid PER 80.91%
EPOCH 6:
  batch 50 loss: 47.75664112091064
  batch 100 loss: 63.7714501953125
  batch 150 loss: 39.29849407196045
  batch 200 loss: 32.980174179077146
  batch 250 loss: 41.54151908874512
  batch 300 loss: 44.88643398284912
  batch 350 loss: 43.66748176574707
  batch 400 loss: 35.0835888671875
  batch 450 loss: 27.091269760131837
  batch 500 loss: 48.88888450622559
  batch 550 loss: 47.237851524353026
  batch 600 loss: 32.194599075317385
  batch 650 loss: 29.60481254577637
  batch 700 loss: 26.399364471435547
  batch 750 loss: 29.897882537841795
  batch 800 loss: 27.032732887268068
  batch 850 loss: 48.056787643432614
  batch 900 loss: 38.27246223449707
LOSS train 38.27246 valid 28.00113, valid PER 80.56%
EPOCH 7:
  batch 50 loss: 39.45909339904785
  batch 100 loss: 193.855652923584
  batch 150 loss: 101.44978790283203
  batch 200 loss: 44.9541219329834
  batch 250 loss: 51.342413101196286
  batch 300 loss: 55.19348243713379
  batch 350 loss: 41.43740406036377
  batch 400 loss: 35.33012443542481
  batch 450 loss: 26.902762908935546
  batch 500 loss: 26.313408660888673
  batch 550 loss: 24.71543561935425
  batch 600 loss: 22.847932510375976
  batch 650 loss: 26.223224182128906
  batch 700 loss: 26.64311882019043
  batch 750 loss: 26.1324479675293
  batch 800 loss: 22.09784404754639
  batch 850 loss: 18.63391819000244
  batch 900 loss: 17.64725570678711
LOSS train 17.64726 valid 32.35900, valid PER 86.26%
EPOCH 8:
  batch 50 loss: 25.588503379821777
  batch 100 loss: 29.959521293640137
  batch 150 loss: 29.064285430908203
  batch 200 loss: 33.566672744750974
  batch 250 loss: 21.272835330963133
  batch 300 loss: 24.766625938415526
  batch 350 loss: 22.19234588623047
  batch 400 loss: 27.39842514038086
  batch 450 loss: 25.650826377868654
  batch 500 loss: 25.833492698669435
  batch 550 loss: 25.994025535583496
  batch 600 loss: 20.125469036102295
  batch 650 loss: 20.561145668029784
  batch 700 loss: 22.03377286911011
  batch 750 loss: 20.365888023376463
  batch 800 loss: 17.284774646759033
  batch 850 loss: 19.489814643859862
  batch 900 loss: 16.852427768707276
LOSS train 16.85243 valid 12.00945, valid PER 78.40%
EPOCH 9:
  batch 50 loss: 12.511372394561768
  batch 100 loss: 13.474462051391601
  batch 150 loss: 14.607683296203612
  batch 200 loss: 8.921297826766967
  batch 250 loss: 9.255853338241577
  batch 300 loss: 11.389598016738892
  batch 350 loss: 10.668849716186523
  batch 400 loss: 10.86858673095703
  batch 450 loss: 13.0284854888916
  batch 500 loss: 11.53308713912964
  batch 550 loss: 10.774494829177856
  batch 600 loss: 10.820127010345459
  batch 650 loss: 10.741145143508911
  batch 700 loss: 13.164056968688964
  batch 750 loss: 11.917378005981446
  batch 800 loss: 12.669497737884521
  batch 850 loss: 14.954999389648437
  batch 900 loss: 14.860867252349854
LOSS train 14.86087 valid 12.69516, valid PER 86.85%
EPOCH 10:
  batch 50 loss: 12.070880794525147
  batch 100 loss: 12.46502618789673
  batch 150 loss: 8.705163564682007
  batch 200 loss: 9.48058970451355
  batch 250 loss: 9.312662839889526
  batch 300 loss: 8.743250312805175
  batch 350 loss: 7.80610610961914
  batch 400 loss: 8.767980813980103
  batch 450 loss: 7.470710363388061
  batch 500 loss: 7.7844442653656
  batch 550 loss: 6.894129610061645
  batch 600 loss: 7.296218061447144
  batch 650 loss: 7.990984725952148
  batch 700 loss: 7.9277853012084964
  batch 750 loss: 7.736286849975586
  batch 800 loss: 6.417780981063843
  batch 850 loss: 6.585521907806396
  batch 900 loss: 9.40379638671875
LOSS train 9.40380 valid 10.17456, valid PER 92.00%
EPOCH 11:
  batch 50 loss: 8.328313856124877
  batch 100 loss: 7.847830867767334
  batch 150 loss: 7.852298393249511
  batch 200 loss: 12.676355800628663
  batch 250 loss: 9.981139402389527
  batch 300 loss: 8.712988157272338
  batch 350 loss: 8.57704730987549
  batch 400 loss: 8.270495796203614
  batch 450 loss: 8.484957752227784
  batch 500 loss: 9.433390855789185
  batch 550 loss: 8.348275051116943
  batch 600 loss: 7.043970508575439
  batch 650 loss: 7.058463945388794
  batch 700 loss: 8.834555501937865
  batch 750 loss: 9.680238065719605
  batch 800 loss: 8.945716800689697
  batch 850 loss: 7.983162488937378
  batch 900 loss: 7.768910627365113
LOSS train 7.76891 valid 11.66874, valid PER 86.01%
EPOCH 12:
  batch 50 loss: 10.464011507034302
  batch 100 loss: 9.150340518951417
  batch 150 loss: 8.934544897079467
  batch 200 loss: 8.931668367385864
  batch 250 loss: 9.270050830841065
  batch 300 loss: 10.64894458770752
  batch 350 loss: 9.304850883483887
  batch 400 loss: 11.372682666778564
  batch 450 loss: 12.878378963470459
  batch 500 loss: 10.565567684173583
  batch 550 loss: 11.933699588775635
  batch 600 loss: 9.742289113998414
  batch 650 loss: 10.551624240875244
  batch 700 loss: 9.833604946136475
  batch 750 loss: 12.597303371429444
  batch 800 loss: 12.738756275177002
  batch 850 loss: 11.125129346847535
  batch 900 loss: 14.585148868560792
LOSS train 14.58515 valid 11.48486, valid PER 90.30%
EPOCH 13:
  batch 50 loss: 12.565070533752442
  batch 100 loss: 12.94892026901245
  batch 150 loss: 12.918203620910644
  batch 200 loss: 13.323590507507324
  batch 250 loss: 10.946363096237183
  batch 300 loss: 11.584356822967528
  batch 350 loss: 10.366875381469727
  batch 400 loss: 10.615552196502685
  batch 450 loss: 9.099558210372924
  batch 500 loss: 8.495139055252075
  batch 550 loss: 8.230984535217285
  batch 600 loss: 10.740801019668579
  batch 650 loss: 11.950908279418945
  batch 700 loss: 10.439017534255981
  batch 750 loss: 8.952335472106933
  batch 800 loss: 9.088926372528077
  batch 850 loss: 9.465358896255493
  batch 900 loss: 11.361829357147217
LOSS train 11.36183 valid 13.18308, valid PER 80.26%
EPOCH 14:
  batch 50 loss: 13.154710063934326
  batch 100 loss: 12.238743705749512
  batch 150 loss: 11.596201448440551
  batch 200 loss: 12.922029647827149
  batch 250 loss: 14.492968006134033
  batch 300 loss: 16.05190971374512
  batch 350 loss: 14.561142082214355
  batch 400 loss: 13.662153778076172
  batch 450 loss: 11.257994174957275
  batch 500 loss: 12.996595077514648
  batch 550 loss: 12.990105152130127
  batch 600 loss: 16.61217845916748
  batch 650 loss: 12.431856594085694
  batch 700 loss: 18.658320713043214
  batch 750 loss: 33.04691318511963
  batch 800 loss: 21.86175748825073
  batch 850 loss: 16.612432651519775
  batch 900 loss: 16.62722631454468
LOSS train 16.62723 valid 18.37808, valid PER 85.24%
EPOCH 15:
  batch 50 loss: 16.00489860534668
  batch 100 loss: 24.622700881958007
  batch 150 loss: 16.835631790161134
  batch 200 loss: 16.297524032592772
  batch 250 loss: 20.348138828277587
  batch 300 loss: 16.132240028381347
  batch 350 loss: 17.031399536132813
  batch 400 loss: 14.652313423156738
  batch 450 loss: 15.276028079986572
  batch 500 loss: 15.544606914520264
  batch 550 loss: 15.476882572174071
  batch 600 loss: 13.743215198516845
  batch 650 loss: 14.592701244354249
  batch 700 loss: 14.674357299804688
  batch 750 loss: 13.456916885375977
  batch 800 loss: 12.506511287689209
  batch 850 loss: 17.30586051940918
  batch 900 loss: 12.85885934829712
LOSS train 12.85886 valid 14.53382, valid PER 103.03%
EPOCH 16:
  batch 50 loss: 18.6089701461792
  batch 100 loss: 14.805253715515137
  batch 150 loss: 19.609644050598146
  batch 200 loss: 16.185328845977782
  batch 250 loss: 19.382252788543703
  batch 300 loss: 17.256611709594726
  batch 350 loss: 20.46784074783325
  batch 400 loss: 15.05787405014038
  batch 450 loss: 16.41499599456787
  batch 500 loss: 21.466192893981933
  batch 550 loss: 19.917222270965578
  batch 600 loss: 24.01164093017578
  batch 650 loss: 25.795116119384765
  batch 700 loss: 23.326363105773925
  batch 750 loss: 30.307993125915527
  batch 800 loss: 24.155097351074218
  batch 850 loss: 18.338192825317382
  batch 900 loss: 17.912053871154786
LOSS train 17.91205 valid 18.63367, valid PER 77.68%
EPOCH 17:
  batch 50 loss: 17.366687297821045
  batch 100 loss: 16.338275394439698
  batch 150 loss: 24.31413288116455
  batch 200 loss: 24.419502487182616
  batch 250 loss: 22.24099624633789
  batch 300 loss: 18.115030460357666
  batch 350 loss: 17.597743911743166
  batch 400 loss: 21.522277545928954
  batch 450 loss: 13.751023979187012
  batch 500 loss: 17.22656805038452
  batch 550 loss: 19.26217582702637
  batch 600 loss: 15.899448108673095
  batch 650 loss: 17.955802364349367
  batch 700 loss: 16.68581214904785
  batch 750 loss: 11.311694374084473
  batch 800 loss: 16.592699031829834
  batch 850 loss: 12.218048706054688
  batch 900 loss: 11.606994171142578
LOSS train 11.60699 valid 11.30918, valid PER 77.58%
EPOCH 18:
  batch 50 loss: 18.36865140914917
  batch 100 loss: 19.13982437133789
  batch 150 loss: 21.627816009521485
  batch 200 loss: 16.679264183044435
  batch 250 loss: 15.561402072906494
  batch 300 loss: 23.6083713722229
  batch 350 loss: 15.370254383087158
  batch 400 loss: 16.14348606109619
  batch 450 loss: 16.953332920074462
  batch 500 loss: 20.26953861236572
  batch 550 loss: 18.622403888702394
  batch 600 loss: 15.843258686065674
  batch 650 loss: 20.11966817855835
  batch 700 loss: 21.441124782562255
  batch 750 loss: 17.76473949432373
  batch 800 loss: 17.913203468322752
  batch 850 loss: 21.759059066772462
  batch 900 loss: 17.27870761871338
LOSS train 17.27871 valid 12.00264, valid PER 78.77%
EPOCH 19:
  batch 50 loss: 14.225520858764648
  batch 100 loss: 17.597971057891847
  batch 150 loss: 16.85554588317871
  batch 200 loss: 17.15661479949951
  batch 250 loss: 13.819216518402099
  batch 300 loss: 19.05518434524536
  batch 350 loss: 19.776333084106444
  batch 400 loss: 14.731856346130371
  batch 450 loss: 13.980648822784424
  batch 500 loss: 16.19664649963379
  batch 550 loss: 12.248333702087402
  batch 600 loss: 16.68649709701538
  batch 650 loss: 17.81313732147217
  batch 700 loss: 19.365555152893066
  batch 750 loss: 15.141702842712402
  batch 800 loss: 16.865945911407472
  batch 850 loss: 21.04326856613159
  batch 900 loss: 17.62384527206421
LOSS train 17.62385 valid 13.30231, valid PER 80.20%
EPOCH 20:
  batch 50 loss: 16.786242237091063
  batch 100 loss: 16.180356559753417
  batch 150 loss: 15.396023979187012
  batch 200 loss: 11.25033314704895
  batch 250 loss: 13.880353937149048
  batch 300 loss: 13.514331245422364
  batch 350 loss: 13.079815044403077
  batch 400 loss: 12.207787418365479
  batch 450 loss: 10.033779287338257
  batch 500 loss: 15.820214538574218
  batch 550 loss: 13.291203804016114
  batch 600 loss: 14.656796054840088
  batch 650 loss: 12.677737693786622
  batch 700 loss: 11.070080375671386
  batch 750 loss: 11.019691505432128
  batch 800 loss: 11.354001197814942
  batch 850 loss: 10.615203428268433
  batch 900 loss: 11.06531156539917
LOSS train 11.06531 valid 10.44234, valid PER 85.56%
[6.538977775573731, 9.82231595993042, 7.7776182270050045, 6.080533084869384, 29.870565185546877, 38.27246223449707, 17.64725570678711, 16.852427768707276, 14.860867252349854, 9.40379638671875, 7.768910627365113, 14.585148868560792, 11.361829357147217, 16.62722631454468, 12.85885934829712, 17.912053871154786, 11.606994171142578, 17.27870761871338, 17.62384527206421, 11.06531156539917]
[tensor(7.3282, device='cuda:0', grad_fn=<DivBackward0>), tensor(14.0234, device='cuda:0', grad_fn=<DivBackward0>), tensor(6.6117, device='cuda:0', grad_fn=<DivBackward0>), tensor(11.4878, device='cuda:0', grad_fn=<DivBackward0>), tensor(40.8046, device='cuda:0', grad_fn=<DivBackward0>), tensor(28.0011, device='cuda:0', grad_fn=<DivBackward0>), tensor(32.3590, device='cuda:0', grad_fn=<DivBackward0>), tensor(12.0094, device='cuda:0', grad_fn=<DivBackward0>), tensor(12.6952, device='cuda:0', grad_fn=<DivBackward0>), tensor(10.1746, device='cuda:0', grad_fn=<DivBackward0>), tensor(11.6687, device='cuda:0', grad_fn=<DivBackward0>), tensor(11.4849, device='cuda:0', grad_fn=<DivBackward0>), tensor(13.1831, device='cuda:0', grad_fn=<DivBackward0>), tensor(18.3781, device='cuda:0', grad_fn=<DivBackward0>), tensor(14.5338, device='cuda:0', grad_fn=<DivBackward0>), tensor(18.6337, device='cuda:0', grad_fn=<DivBackward0>), tensor(11.3092, device='cuda:0', grad_fn=<DivBackward0>), tensor(12.0026, device='cuda:0', grad_fn=<DivBackward0>), tensor(13.3023, device='cuda:0', grad_fn=<DivBackward0>), tensor(10.4423, device='cuda:0', grad_fn=<DivBackward0>)]
Training finished in 8.0 minutes.
Model saved to checkpoints/20230123_135734/model_3
Loading model from checkpoints/20230123_135734/model_3
SUB: 8.61%, DEL: 77.74%, INS: 0.08%, COR: 13.65%, PER: 86.43%
