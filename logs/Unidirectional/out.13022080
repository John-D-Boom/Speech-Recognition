Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=256, concat=1, lr=0.002, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.2, beta1=0.9, beta2=0.999)
Total number of model parameters is 298024
EPOCH 1:
  batch 50 loss: 5.749932351112366
  batch 100 loss: 3.217753872871399
  batch 150 loss: 3.1160718107223513
  batch 200 loss: 3.554549899101257
  batch 250 loss: 3.5322619104385375
  batch 300 loss: 3.1850853061676023
  batch 350 loss: 3.0302638387680054
  batch 400 loss: 2.966423544883728
  batch 450 loss: 2.913295545578003
  batch 500 loss: 2.8378421115875243
  batch 550 loss: 2.758067021369934
  batch 600 loss: 2.7039749240875244
  batch 650 loss: 2.599987812042236
  batch 700 loss: 2.5229759311676023
  batch 750 loss: 2.459768867492676
  batch 800 loss: 2.4366431856155395
  batch 850 loss: 2.394511957168579
  batch 900 loss: 2.3219129419326783
LOSS train 2.32191 valid 2.33255, valid PER 78.02%
EPOCH 2:
  batch 50 loss: 2.301703577041626
  batch 100 loss: 2.21112685918808
  batch 150 loss: 2.081148111820221
  batch 200 loss: 2.0528440165519712
  batch 250 loss: 2.0604268431663515
  batch 300 loss: 1.9929171919822692
  batch 350 loss: 1.9701096606254578
  batch 400 loss: 1.9178278875350951
  batch 450 loss: 1.9114361786842347
  batch 500 loss: 1.894865381717682
  batch 550 loss: 1.8647517919540406
  batch 600 loss: 1.830557107925415
  batch 650 loss: 1.7819249892234803
  batch 700 loss: 1.8005345797538757
  batch 750 loss: 1.7705024075508118
  batch 800 loss: 1.731745789051056
  batch 850 loss: 1.7307639503479004
  batch 900 loss: 1.6837088394165038
LOSS train 1.68371 valid 1.68457, valid PER 59.55%
EPOCH 3:
  batch 50 loss: 1.6305481100082397
  batch 100 loss: 1.6687729716300965
  batch 150 loss: 1.70306969165802
  batch 200 loss: 1.5819088196754456
  batch 250 loss: 1.5980400133132935
  batch 300 loss: 1.6017995071411133
  batch 350 loss: 1.6134234690666198
  batch 400 loss: 1.5731870532035828
  batch 450 loss: 1.5578867721557617
  batch 500 loss: 1.54176331281662
  batch 550 loss: 1.5293711042404174
  batch 600 loss: 1.473849856853485
  batch 650 loss: 1.5245810413360597
  batch 700 loss: 1.4956441497802735
  batch 750 loss: 1.5134578132629395
  batch 800 loss: 1.5364006900787353
  batch 850 loss: 1.4840233135223388
  batch 900 loss: 1.507832179069519
LOSS train 1.50783 valid 1.44974, valid PER 47.20%
EPOCH 4:
  batch 50 loss: 1.4959441661834716
  batch 100 loss: 1.4165626764297485
  batch 150 loss: 1.4443702483177185
  batch 200 loss: 1.435979404449463
  batch 250 loss: 1.4713714575767518
  batch 300 loss: 1.421050055027008
  batch 350 loss: 1.434525616168976
  batch 400 loss: 1.3808916568756104
  batch 450 loss: 1.4235811972618102
  batch 500 loss: 1.4264263892173767
  batch 550 loss: 1.3751257681846618
  batch 600 loss: 1.3543578147888184
  batch 650 loss: 1.4522959351539613
  batch 700 loss: 1.4307459306716919
  batch 750 loss: 1.3855084991455078
  batch 800 loss: 1.3434442710876464
  batch 850 loss: 1.3452966141700744
  batch 900 loss: 1.3869334769248962
LOSS train 1.38693 valid 1.33494, valid PER 42.21%
EPOCH 5:
  batch 50 loss: 1.3365927481651305
  batch 100 loss: 1.3479845333099365
  batch 150 loss: 1.3482294130325316
  batch 200 loss: 1.3652182912826538
  batch 250 loss: 1.3017338562011718
  batch 300 loss: 1.3662075185775757
  batch 350 loss: 1.31107323884964
  batch 400 loss: 1.2747854018211364
  batch 450 loss: 1.29141055226326
  batch 500 loss: 1.2932928681373597
  batch 550 loss: 1.3370595645904542
  batch 600 loss: 1.358572223186493
  batch 650 loss: 1.3730856847763062
  batch 700 loss: 1.3447000992298126
  batch 750 loss: 1.3422030925750732
  batch 800 loss: 1.3737303161621093
  batch 850 loss: 1.3495314359664916
  batch 900 loss: 1.2890571606159211
LOSS train 1.28906 valid 1.27691, valid PER 41.11%
EPOCH 6:
  batch 50 loss: 1.27668523311615
  batch 100 loss: 1.312762222290039
  batch 150 loss: 1.3032625770568849
  batch 200 loss: 1.2651594972610474
  batch 250 loss: 1.295985131263733
  batch 300 loss: 1.291390256881714
  batch 350 loss: 1.3217329645156861
  batch 400 loss: 1.3228588235378265
  batch 450 loss: 1.3164289343357085
  batch 500 loss: 1.2602539336681366
  batch 550 loss: 1.287924156188965
  batch 600 loss: 1.2395370626449584
  batch 650 loss: 1.264858329296112
  batch 700 loss: 1.228385430574417
  batch 750 loss: 1.2595379805564881
  batch 800 loss: 1.2406714475154876
  batch 850 loss: 1.2897304117679596
  batch 900 loss: 1.2820456326007843
LOSS train 1.28205 valid 1.23198, valid PER 39.20%
EPOCH 7:
  batch 50 loss: 1.2526175165176392
  batch 100 loss: 1.2840224170684815
  batch 150 loss: 1.212493087053299
  batch 200 loss: 1.242456431388855
  batch 250 loss: 1.2618077421188354
  batch 300 loss: 1.22298792719841
  batch 350 loss: 1.245321171283722
  batch 400 loss: 1.224814145565033
  batch 450 loss: 1.2225587368011475
  batch 500 loss: 1.203880923986435
  batch 550 loss: 1.213785125017166
  batch 600 loss: 1.2221409285068512
  batch 650 loss: 1.1842986536026001
  batch 700 loss: 1.2413059234619142
  batch 750 loss: 1.201305296421051
  batch 800 loss: 1.2220485377311707
  batch 850 loss: 1.192746660709381
  batch 900 loss: 1.2272559571266175
LOSS train 1.22726 valid 1.23074, valid PER 40.01%
EPOCH 8:
  batch 50 loss: 1.2140750694274902
  batch 100 loss: 1.19911825299263
  batch 150 loss: 1.2260331153869628
  batch 200 loss: 1.238969954252243
  batch 250 loss: 1.1673687303066254
  batch 300 loss: 1.1637974965572357
  batch 350 loss: 1.1591958248615264
  batch 400 loss: 1.1711811876296998
  batch 450 loss: 1.236058497428894
  batch 500 loss: 1.1954273855686188
  batch 550 loss: 1.2027666389942169
  batch 600 loss: 1.1725995552539825
  batch 650 loss: 1.1795541882514953
  batch 700 loss: 1.2027553737163543
  batch 750 loss: 1.1982827639579774
  batch 800 loss: 1.218735908269882
  batch 850 loss: 1.1606883811950683
  batch 900 loss: 1.1723523938655853
LOSS train 1.17235 valid 1.17326, valid PER 38.38%
EPOCH 9:
  batch 50 loss: 1.1460909867286682
  batch 100 loss: 1.129939796924591
  batch 150 loss: 1.1524013674259186
  batch 200 loss: 1.1309280216693878
  batch 250 loss: 1.1156099426746369
  batch 300 loss: 1.1795603811740876
  batch 350 loss: 1.1553274154663087
  batch 400 loss: 1.1694325637817382
  batch 450 loss: 1.1839356803894043
  batch 500 loss: 1.1572399842739105
  batch 550 loss: 1.1621123254299164
  batch 600 loss: 1.207402334213257
  batch 650 loss: 1.1921299064159394
  batch 700 loss: 1.1646108496189118
  batch 750 loss: 1.1690726089477539
  batch 800 loss: 1.19974240899086
  batch 850 loss: 1.175511385202408
  batch 900 loss: 1.1211593544483185
LOSS train 1.12116 valid 1.15691, valid PER 37.08%
EPOCH 10:
  batch 50 loss: 1.1438388693332673
  batch 100 loss: 1.1920638215541839
  batch 150 loss: 1.1764105319976808
  batch 200 loss: 1.1319170022010803
  batch 250 loss: 1.142250406742096
  batch 300 loss: 1.1232180154323579
  batch 350 loss: 1.139335765838623
  batch 400 loss: 1.0997995114326478
  batch 450 loss: 1.1199815690517425
  batch 500 loss: 1.1662785255908965
  batch 550 loss: 1.177922810316086
  batch 600 loss: 1.123075098991394
  batch 650 loss: 1.1293065071105957
  batch 700 loss: 1.1773374450206757
  batch 750 loss: 1.3117693769931793
  batch 800 loss: 1.1782051193714143
  batch 850 loss: 1.134921795129776
  batch 900 loss: 1.0975308740139007
LOSS train 1.09753 valid 1.14974, valid PER 36.49%
EPOCH 11:
  batch 50 loss: 1.1197237265110016
  batch 100 loss: 1.099465823173523
  batch 150 loss: 1.1254463946819306
  batch 200 loss: 1.1040390598773957
  batch 250 loss: 1.1511242413520812
  batch 300 loss: 1.108078716993332
  batch 350 loss: 1.1517904162406922
  batch 400 loss: 1.1043979215621949
  batch 450 loss: 1.085078228712082
  batch 500 loss: 1.0908434748649598
  batch 550 loss: 1.1401708579063417
  batch 600 loss: 1.1170677149295807
  batch 650 loss: 1.1227718389034271
  batch 700 loss: 1.1940573525428773
  batch 750 loss: 1.100028921365738
  batch 800 loss: 1.1369067585468293
  batch 850 loss: 1.1570734095573425
  batch 900 loss: 1.1384438371658325
LOSS train 1.13844 valid 1.12953, valid PER 35.58%
EPOCH 12:
  batch 50 loss: 1.0595695495605468
  batch 100 loss: 1.0432736027240752
  batch 150 loss: 1.0959776949882507
  batch 200 loss: 1.1297765207290649
  batch 250 loss: 1.1136910176277162
  batch 300 loss: 1.1250859904289245
  batch 350 loss: 1.1005471289157867
  batch 400 loss: 1.0811083447933196
  batch 450 loss: 1.090853888988495
  batch 500 loss: 1.1185164034366608
  batch 550 loss: 1.0909671437740327
  batch 600 loss: 1.098600914478302
  batch 650 loss: 1.1353642857074737
  batch 700 loss: 1.135457090139389
  batch 750 loss: 1.0910160374641418
  batch 800 loss: 1.0620425486564635
  batch 850 loss: 1.106365967988968
  batch 900 loss: 1.1088216245174407
LOSS train 1.10882 valid 1.11542, valid PER 35.88%
EPOCH 13:
  batch 50 loss: 1.0816571855545043
  batch 100 loss: 1.078514713048935
  batch 150 loss: 1.092204097509384
  batch 200 loss: 1.0863374400138854
  batch 250 loss: 1.0659476184844972
  batch 300 loss: 1.0767893981933594
  batch 350 loss: 1.0669001150131225
  batch 400 loss: 1.1094441270828248
  batch 450 loss: 1.125861200094223
  batch 500 loss: 1.1072456204891206
  batch 550 loss: 1.129838160276413
  batch 600 loss: 1.078089590072632
  batch 650 loss: 1.0709362971782683
  batch 700 loss: 1.0887537753582002
  batch 750 loss: 1.0467697954177857
  batch 800 loss: 1.0863697838783264
  batch 850 loss: 1.0804537880420684
  batch 900 loss: 1.0690138387680053
LOSS train 1.06901 valid 1.11334, valid PER 36.06%
EPOCH 14:
  batch 50 loss: 1.0499194836616517
  batch 100 loss: 1.042767323255539
  batch 150 loss: 1.0894101810455323
  batch 200 loss: 1.1006713616847992
  batch 250 loss: 1.1051321721076965
  batch 300 loss: 1.0571353733539581
  batch 350 loss: 1.0824886536598206
  batch 400 loss: 1.1238169360160828
  batch 450 loss: 1.0175320053100585
  batch 500 loss: 1.0381894659996034
  batch 550 loss: 1.0443289685249328
  batch 600 loss: 1.0414684176445008
  batch 650 loss: 1.1046148443222046
  batch 700 loss: 1.0939046847820282
  batch 750 loss: 1.065689936876297
  batch 800 loss: 1.0548141479492188
  batch 850 loss: 1.1102257907390594
  batch 900 loss: 1.0807076835632323
LOSS train 1.08071 valid 1.10529, valid PER 35.76%
EPOCH 15:
  batch 50 loss: 1.0634032201766968
  batch 100 loss: 1.2388349270820618
  batch 150 loss: 1.0870048260688783
  batch 200 loss: 1.1151943290233612
  batch 250 loss: 1.1239645195007324
  batch 300 loss: 1.1361361384391784
  batch 350 loss: 1.0500953996181488
  batch 400 loss: 1.0690126287937165
  batch 450 loss: 1.0662995994091033
  batch 500 loss: 1.0776694226264953
  batch 550 loss: 1.083747891187668
  batch 600 loss: 1.0836906349658966
  batch 650 loss: 1.0482708930969238
  batch 700 loss: 1.0398605394363403
  batch 750 loss: 1.0809001517295838
  batch 800 loss: 1.0314706575870514
  batch 850 loss: 1.0268903481960296
  batch 900 loss: 1.0457059502601624
LOSS train 1.04571 valid 1.08410, valid PER 34.70%
EPOCH 16:
  batch 50 loss: 1.0258111429214478
  batch 100 loss: 1.039146852493286
  batch 150 loss: 1.0450281727313995
  batch 200 loss: 1.0692568576335908
  batch 250 loss: 1.0678695213794709
  batch 300 loss: 1.0869783544540406
  batch 350 loss: 1.0623609972000123
  batch 400 loss: 1.0260611033439637
  batch 450 loss: 1.0234969389438628
  batch 500 loss: 1.0258910989761352
  batch 550 loss: 1.034738428592682
  batch 600 loss: 1.0597283005714417
  batch 650 loss: 1.0732356238365173
  batch 700 loss: 1.0523039054870607
  batch 750 loss: 1.0471866297721864
  batch 800 loss: 1.0116167449951172
  batch 850 loss: 1.0138856112957
  batch 900 loss: 1.0136722552776336
LOSS train 1.01367 valid 1.04254, valid PER 33.60%
EPOCH 17:
  batch 50 loss: 0.9831992661952973
  batch 100 loss: 0.9760375773906708
  batch 150 loss: 1.0711087954044343
  batch 200 loss: 1.0516748869419097
  batch 250 loss: 1.034545934200287
  batch 300 loss: 1.0379751694202424
  batch 350 loss: 1.0350068366527558
  batch 400 loss: 1.0395795822143554
  batch 450 loss: 1.02335502743721
  batch 500 loss: 1.054117282629013
  batch 550 loss: 1.013721525669098
  batch 600 loss: 1.0460240197181703
  batch 650 loss: 1.026390631198883
  batch 700 loss: 1.0454700648784638
  batch 750 loss: 0.9961036813259124
  batch 800 loss: 1.0012003469467163
  batch 850 loss: 0.9911500918865204
  batch 900 loss: 1.0457822132110595
LOSS train 1.04578 valid 1.10590, valid PER 34.34%
EPOCH 18:
  batch 50 loss: 1.0385330355167388
  batch 100 loss: 0.9965299534797668
  batch 150 loss: 1.0471397161483764
  batch 200 loss: 0.9668160438537597
  batch 250 loss: 1.0306948006153107
  batch 300 loss: 1.0282146334648132
  batch 350 loss: 1.012011684179306
  batch 400 loss: 1.0210502791404723
  batch 450 loss: 1.0335605239868164
  batch 500 loss: 1.020885750055313
  batch 550 loss: 1.056910719871521
  batch 600 loss: 1.019509037733078
  batch 650 loss: 1.0083081364631652
  batch 700 loss: 1.0259658074378968
  batch 750 loss: 1.0209176170825958
  batch 800 loss: 1.0678468501567842
  batch 850 loss: 1.0481078457832336
  batch 900 loss: 1.0294475865364074
LOSS train 1.02945 valid 1.06921, valid PER 34.34%
EPOCH 19:
  batch 50 loss: 0.9943558144569397
  batch 100 loss: 0.9751078081130982
  batch 150 loss: 1.075142641067505
  batch 200 loss: 1.053325206041336
  batch 250 loss: 1.0653159594535828
  batch 300 loss: 1.0175749719142915
  batch 350 loss: 1.0298825240135192
  batch 400 loss: 1.0441062617301942
  batch 450 loss: 0.9777038323879242
  batch 500 loss: 0.9824921536445618
  batch 550 loss: 1.0437972164154052
  batch 600 loss: 1.0715760385990143
  batch 650 loss: 1.0500126779079437
  batch 700 loss: 1.0707636392116546
  batch 750 loss: 0.9925452828407287
  batch 800 loss: 0.9846681642532349
  batch 850 loss: 1.0034263265132903
  batch 900 loss: 0.9819544696807861
LOSS train 0.98195 valid 1.04219, valid PER 33.51%
EPOCH 20:
  batch 50 loss: 0.9443644189834595
  batch 100 loss: 1.0100242567062379
  batch 150 loss: 1.0257589709758759
  batch 200 loss: 0.9914678382873535
  batch 250 loss: 1.0153053784370423
  batch 300 loss: 1.004287838935852
  batch 350 loss: 0.9841463446617127
  batch 400 loss: 0.9731732320785522
  batch 450 loss: 1.0049542784690857
  batch 500 loss: 1.0355017709732055
  batch 550 loss: 1.0138196313381196
  batch 600 loss: 0.9810955250263214
  batch 650 loss: 0.9823027801513672
  batch 700 loss: 1.0045548450946808
  batch 750 loss: 0.9930274498462677
  batch 800 loss: 1.0446022355556488
  batch 850 loss: 1.0573460781574249
  batch 900 loss: 0.9999578654766083
LOSS train 0.99996 valid 1.05984, valid PER 33.94%
[2.3219129419326783, 1.6837088394165038, 1.507832179069519, 1.3869334769248962, 1.2890571606159211, 1.2820456326007843, 1.2272559571266175, 1.1723523938655853, 1.1211593544483185, 1.0975308740139007, 1.1384438371658325, 1.1088216245174407, 1.0690138387680053, 1.0807076835632323, 1.0457059502601624, 1.0136722552776336, 1.0457822132110595, 1.0294475865364074, 0.9819544696807861, 0.9999578654766083]
[tensor(2.3326, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.6846, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.4497, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.3349, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.2769, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.2320, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.2307, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.1733, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.1569, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.1497, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.1295, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.1154, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.1133, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.1053, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.0841, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.0425, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.1059, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.0692, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.0422, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.0598, device='cuda:0', grad_fn=<DivBackward0>)]
Training finished in 6.0 minutes.
Model saved to checkpoints/20230122_154913/model_19
Loading model from checkpoints/20230122_154913/model_19
SUB: 21.08%, DEL: 11.62%, INS: 1.88%, COR: 67.30%, PER: 34.58%
