Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=4, fbank_dims=23, model_dims=128, concat=1, lr=0.002, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.2, beta1=0.9, beta2=0.999)
Total number of model parameters is 479784
EPOCH 1:
  batch 50 loss: 5.76026831150055
  batch 100 loss: 3.3466657829284667
  batch 150 loss: 3.329188680648804
  batch 200 loss: 3.2927105712890623
  batch 250 loss: 3.2607651710510255
  batch 300 loss: 3.225566420555115
  batch 350 loss: 3.2068318223953245
  batch 400 loss: 3.193342351913452
  batch 450 loss: 3.0968237590789793
  batch 500 loss: 3.078087778091431
  batch 550 loss: 3.026478147506714
  batch 600 loss: 3.0147902536392213
  batch 650 loss: 2.9930184984207155
  batch 700 loss: 2.9803125429153443
  batch 750 loss: 2.909036259651184
  batch 800 loss: 2.8794055461883543
  batch 850 loss: 2.8429464435577394
  batch 900 loss: 2.8056920051574705
LOSS train 2.80569 valid 2.82519, valid PER 79.86%
EPOCH 2:
  batch 50 loss: 2.831238660812378
  batch 100 loss: 2.788854818344116
  batch 150 loss: 2.715489091873169
  batch 200 loss: 2.7108678674697875
  batch 250 loss: 2.723678398132324
  batch 300 loss: 2.6774027967453002
  batch 350 loss: 2.6673278045654296
  batch 400 loss: 2.638696541786194
  batch 450 loss: 2.634298539161682
  batch 500 loss: 2.6257595109939573
  batch 550 loss: 2.5896655225753786
  batch 600 loss: 2.572355017662048
  batch 650 loss: 2.547164134979248
  batch 700 loss: 2.5219404077529908
  batch 750 loss: 2.5004475688934327
  batch 800 loss: 2.453148889541626
  batch 850 loss: 2.4368967533111574
  batch 900 loss: 2.3979599142074584
LOSS train 2.39796 valid 2.37674, valid PER 79.21%
EPOCH 3:
  batch 50 loss: 2.346282138824463
  batch 100 loss: 2.3704232931137086
  batch 150 loss: 2.387531251907349
  batch 200 loss: 2.2827804040908815
  batch 250 loss: 2.2958444118499757
  batch 300 loss: 2.270329999923706
  batch 350 loss: 2.2917940855026244
  batch 400 loss: 2.2460751581192016
  batch 450 loss: 2.2138201570510865
  batch 500 loss: 2.166491520404816
  batch 550 loss: 2.1295075154304506
  batch 600 loss: 2.0761921286582945
  batch 650 loss: 2.0472576880455016
  batch 700 loss: 2.0068775248527526
  batch 750 loss: 2.050624566078186
  batch 800 loss: 2.0333352661132813
  batch 850 loss: 2.0012322568893435
  batch 900 loss: 1.9814942979812622
LOSS train 1.98149 valid 1.92991, valid PER 64.32%
EPOCH 4:
  batch 50 loss: 1.9605130648612976
  batch 100 loss: 1.8856643009185792
  batch 150 loss: 1.9043379163742065
  batch 200 loss: 1.893246862888336
  batch 250 loss: 1.9381574034690856
  batch 300 loss: 1.8791490387916565
  batch 350 loss: 1.8407420992851258
  batch 400 loss: 1.8193566823005676
  batch 450 loss: 1.828394546508789
  batch 500 loss: 1.8400920128822327
  batch 550 loss: 1.7663968396186829
  batch 600 loss: 1.7768862175941467
  batch 650 loss: 1.8080363941192628
  batch 700 loss: 1.8158006024360658
  batch 750 loss: 1.7314684104919433
  batch 800 loss: 1.7599221777915954
  batch 850 loss: 1.7380341863632203
  batch 900 loss: 1.7211083912849425
LOSS train 1.72111 valid 1.71138, valid PER 56.21%
EPOCH 5:
  batch 50 loss: 1.7176332998275756
  batch 100 loss: 1.673868887424469
  batch 150 loss: 1.6905972313880921
  batch 200 loss: 1.6803906917572022
  batch 250 loss: 1.6696220445632934
  batch 300 loss: 1.67740638256073
  batch 350 loss: 1.6412591576576232
  batch 400 loss: 1.5640196371078492
  batch 450 loss: 1.5808428168296813
  batch 500 loss: 1.5652494144439697
  batch 550 loss: 1.6036468172073364
  batch 600 loss: 1.6042977595329284
  batch 650 loss: 1.6009904289245604
  batch 700 loss: 1.5631678628921508
  batch 750 loss: 1.5621640419960021
  batch 800 loss: 1.5605765891075134
  batch 850 loss: 1.554722261428833
  batch 900 loss: 1.4756883883476257
LOSS train 1.47569 valid 1.48147, valid PER 45.27%
EPOCH 6:
  batch 50 loss: 1.4688207054138183
  batch 100 loss: 1.5432065653800964
  batch 150 loss: 1.4978872728347778
  batch 200 loss: 1.4389300799369813
  batch 250 loss: 1.4823980188369752
  batch 300 loss: 1.4738525748252869
  batch 350 loss: 1.4529603004455567
  batch 400 loss: 1.447860884666443
  batch 450 loss: 1.4638838648796082
  batch 500 loss: 1.4160538339614868
  batch 550 loss: 1.4517991328239441
  batch 600 loss: 1.3875785779953003
  batch 650 loss: 1.3634466814994812
  batch 700 loss: 1.3442624092102051
  batch 750 loss: 1.40989684343338
  batch 800 loss: 1.4015803575515746
  batch 850 loss: 1.428849973678589
  batch 900 loss: 1.4182968258857727
LOSS train 1.41830 valid 1.33801, valid PER 40.61%
EPOCH 7:
  batch 50 loss: 1.3363556361198425
  batch 100 loss: 1.4183929991722106
  batch 150 loss: 1.3291857647895813
  batch 200 loss: 1.3313015437126159
  batch 250 loss: 1.3879916977882385
  batch 300 loss: 1.3322368025779725
  batch 350 loss: 1.3590685105323792
  batch 400 loss: 1.3462956285476684
  batch 450 loss: 1.3746117973327636
  batch 500 loss: 1.3364512920379639
  batch 550 loss: 1.3175528836250305
  batch 600 loss: 1.3448259806632996
  batch 650 loss: 1.2914988994598389
  batch 700 loss: 1.3581337308883668
  batch 750 loss: 1.3077846360206604
  batch 800 loss: 1.2921756458282472
  batch 850 loss: 1.2811650490760804
  batch 900 loss: 1.3005389380455017
LOSS train 1.30054 valid 1.30135, valid PER 39.73%
EPOCH 8:
  batch 50 loss: 1.2953106129169465
  batch 100 loss: 1.2595768117904662
  batch 150 loss: 1.2830849266052247
  batch 200 loss: 1.2783510899543762
  batch 250 loss: 1.2461032843589783
  batch 300 loss: 1.2424001348018647
  batch 350 loss: 1.23925062417984
  batch 400 loss: 1.2128887379169464
  batch 450 loss: 1.2841958618164062
  batch 500 loss: 1.3077493810653686
  batch 550 loss: 1.2663452565670013
  batch 600 loss: 1.2359748601913452
  batch 650 loss: 1.25750812292099
  batch 700 loss: 1.2611734807491302
  batch 750 loss: 1.2266150331497192
  batch 800 loss: 1.2495028734207154
  batch 850 loss: 1.2211406779289247
  batch 900 loss: 1.2274419045448304
LOSS train 1.22744 valid 1.20375, valid PER 36.61%
EPOCH 9:
  batch 50 loss: 1.2264043474197388
  batch 100 loss: 1.1962062203884125
  batch 150 loss: 1.2226856875419616
  batch 200 loss: 1.192815157175064
  batch 250 loss: 1.2091218519210816
  batch 300 loss: 1.2199510061740875
  batch 350 loss: 1.2082735049724578
  batch 400 loss: 1.224521975517273
  batch 450 loss: 1.236128044128418
  batch 500 loss: 1.1869796693325043
  batch 550 loss: 1.203113614320755
  batch 600 loss: 1.2577364265918731
  batch 650 loss: 1.1876737225055694
  batch 700 loss: 1.1890070354938507
  batch 750 loss: 1.1712397372722625
  batch 800 loss: 1.2197614109516144
  batch 850 loss: 1.196596451997757
  batch 900 loss: 1.1669914722442627
LOSS train 1.16699 valid 1.14656, valid PER 34.58%
EPOCH 10:
  batch 50 loss: 1.1799595987796783
  batch 100 loss: 1.1721029937267304
  batch 150 loss: 1.1966798937320708
  batch 200 loss: 1.2006882202625275
  batch 250 loss: 1.1723486816883086
  batch 300 loss: 1.1739364302158355
  batch 350 loss: 1.155089783668518
  batch 400 loss: 1.1227264964580537
  batch 450 loss: 1.1458753407001496
  batch 500 loss: 1.1559625732898713
  batch 550 loss: 1.1552699506282806
  batch 600 loss: 1.1542230236530304
  batch 650 loss: 1.160626220703125
  batch 700 loss: 1.159815363883972
  batch 750 loss: 1.1534353077411652
  batch 800 loss: 1.2841775834560394
  batch 850 loss: 1.1616898798942565
  batch 900 loss: 1.1345589280128479
LOSS train 1.13456 valid 1.14586, valid PER 34.26%
EPOCH 11:
  batch 50 loss: 1.1437140369415284
  batch 100 loss: 1.1195090126991272
  batch 150 loss: 1.1114075982570648
  batch 200 loss: 1.1152703166007996
  batch 250 loss: 1.0976159811019897
  batch 300 loss: 1.1320253169536592
  batch 350 loss: 1.1715260827541352
  batch 400 loss: 1.0828623402118682
  batch 450 loss: 1.0992106664180756
  batch 500 loss: 1.097421556711197
  batch 550 loss: 1.1397977185249328
  batch 600 loss: 1.115685818195343
  batch 650 loss: 1.1474760258197785
  batch 700 loss: 1.219360409975052
  batch 750 loss: 1.0885772812366485
  batch 800 loss: 1.1085233461856843
  batch 850 loss: 1.0864717638492585
  batch 900 loss: 1.1259733462333679
LOSS train 1.12597 valid 1.08763, valid PER 33.38%
EPOCH 12:
  batch 50 loss: 1.0555579960346222
  batch 100 loss: 1.0775888347625733
  batch 150 loss: 1.0980656802654267
  batch 200 loss: 1.1133465576171875
  batch 250 loss: 1.0988879632949828
  batch 300 loss: 1.1407915234565735
  batch 350 loss: 1.0806191897392272
  batch 400 loss: 1.1247692382335663
  batch 450 loss: 1.0702876794338225
  batch 500 loss: 1.1119324696063995
  batch 550 loss: 1.1068965446949006
  batch 600 loss: 1.107043867111206
  batch 650 loss: 1.1112505900859833
  batch 700 loss: 1.0903245151042937
  batch 750 loss: 1.0961709666252135
  batch 800 loss: 1.0541830956935883
  batch 850 loss: 1.086074720621109
  batch 900 loss: 1.0557694339752197
LOSS train 1.05577 valid 1.14893, valid PER 34.76%
EPOCH 13:
  batch 50 loss: 1.0796466231346131
  batch 100 loss: 1.0796887135505677
  batch 150 loss: 1.0716635370254517
  batch 200 loss: 1.0402123165130615
  batch 250 loss: 1.0443176531791687
  batch 300 loss: 1.0908275938034058
  batch 350 loss: 1.0414885127544402
  batch 400 loss: 1.0789275419712068
  batch 450 loss: 1.0548355424404143
  batch 500 loss: 1.0249528312683105
  batch 550 loss: 1.1043682157993318
  batch 600 loss: 1.0719714987277984
  batch 650 loss: 1.0686328423023224
  batch 700 loss: 1.1134693217277527
  batch 750 loss: 1.0176473104953765
  batch 800 loss: 1.0855735123157502
  batch 850 loss: 1.070895664691925
  batch 900 loss: 1.0650029718875884
LOSS train 1.06500 valid 1.04526, valid PER 31.39%
EPOCH 14:
  batch 50 loss: 1.0359423780441284
  batch 100 loss: 1.0195061993598937
  batch 150 loss: 1.04982346534729
  batch 200 loss: 1.0379360938072204
  batch 250 loss: 1.0258551025390625
  batch 300 loss: 1.017938016653061
  batch 350 loss: 1.0408521747589112
  batch 400 loss: 1.0714730370044707
  batch 450 loss: 1.0287266266345978
  batch 500 loss: 1.049833059310913
  batch 550 loss: 1.0945254516601564
  batch 600 loss: 1.052134222984314
  batch 650 loss: 1.0676546239852904
  batch 700 loss: 1.0554649889469148
  batch 750 loss: 1.0417684185504914
  batch 800 loss: 1.0788868629932404
  batch 850 loss: 1.128693289756775
  batch 900 loss: 1.077231945991516
LOSS train 1.07723 valid 1.05551, valid PER 31.88%
EPOCH 15:
  batch 50 loss: 1.0234220206737519
  batch 100 loss: 1.0184397327899932
  batch 150 loss: 1.0545491087436676
  batch 200 loss: 1.0583932840824126
  batch 250 loss: 1.0511146700382232
  batch 300 loss: 1.0214040076732636
  batch 350 loss: 1.0100109386444092
  batch 400 loss: 1.0382560217380523
  batch 450 loss: 1.0207577574253082
  batch 500 loss: 1.0306786036491393
  batch 550 loss: 1.0756357669830323
  batch 600 loss: 1.0559984505176545
  batch 650 loss: 1.0377888679504395
  batch 700 loss: 1.0077077674865722
  batch 750 loss: 1.0569096195697785
  batch 800 loss: 1.0452076923847198
  batch 850 loss: 1.0017089092731475
  batch 900 loss: 0.9882389497756958
LOSS train 0.98824 valid 1.10109, valid PER 33.57%
EPOCH 16:
  batch 50 loss: 1.024738336801529
  batch 100 loss: 1.0008024752140046
  batch 150 loss: 1.0391475296020507
  batch 200 loss: 1.0497101640701294
  batch 250 loss: 1.0615545451641082
  batch 300 loss: 1.0236490535736085
  batch 350 loss: 1.0123195457458496
  batch 400 loss: 1.01369762301445
  batch 450 loss: 1.0118611454963684
  batch 500 loss: 1.0049725699424743
  batch 550 loss: 0.9964679253101348
  batch 600 loss: 1.0078446054458619
  batch 650 loss: 1.024570688009262
  batch 700 loss: 0.9985388076305389
  batch 750 loss: 1.0333738934993744
  batch 800 loss: 1.0790985023975372
  batch 850 loss: 1.042082464694977
  batch 900 loss: 1.032467224597931
LOSS train 1.03247 valid 1.05786, valid PER 31.87%
EPOCH 17:
  batch 50 loss: 0.987106761932373
  batch 100 loss: 0.9616243064403533
  batch 150 loss: 1.0351914536952973
  batch 200 loss: 0.9643277442455291
  batch 250 loss: 1.0051601076126098
  batch 300 loss: 0.9692863619327545
  batch 350 loss: 0.9828458797931671
  batch 400 loss: 1.011540597677231
  batch 450 loss: 1.0093839466571808
  batch 500 loss: 1.01922057390213
  batch 550 loss: 0.9857501196861267
  batch 600 loss: 1.0286729264259338
  batch 650 loss: 0.9784070789813996
  batch 700 loss: 0.9917506849765778
  batch 750 loss: 0.9394781291484833
  batch 800 loss: 0.9968242514133453
  batch 850 loss: 0.9812891817092896
  batch 900 loss: 0.9693696010112762
LOSS train 0.96937 valid 1.00750, valid PER 30.45%
EPOCH 18:
  batch 50 loss: 0.9677305209636688
  batch 100 loss: 0.9514566111564636
  batch 150 loss: 1.0007071161270142
  batch 200 loss: 0.9945424711704254
  batch 250 loss: 0.9375930500030517
  batch 300 loss: 1.0131441164016723
  batch 350 loss: 0.9796449303627014
  batch 400 loss: 0.9723852694034576
  batch 450 loss: 1.0138901996612548
  batch 500 loss: 0.9885829865932465
  batch 550 loss: 1.0280251002311707
  batch 600 loss: 0.9890043568611145
  batch 650 loss: 0.938726544380188
  batch 700 loss: 0.9342542862892151
  batch 750 loss: 0.9816756892204285
  batch 800 loss: 1.0091927659511566
  batch 850 loss: 0.9786592769622803
  batch 900 loss: 0.9687570583820343
LOSS train 0.96876 valid 1.00402, valid PER 30.52%
EPOCH 19:
  batch 50 loss: 0.9667910718917847
  batch 100 loss: 0.9594351518154144
  batch 150 loss: 0.9249712550640106
  batch 200 loss: 0.9245755755901337
  batch 250 loss: 0.9824944067001343
  batch 300 loss: 0.9776334583759307
  batch 350 loss: 1.0072718024253846
  batch 400 loss: 0.9743297719955444
  batch 450 loss: 0.9130196571350098
  batch 500 loss: 0.9224576735496521
  batch 550 loss: 0.964159619808197
  batch 600 loss: 0.9832511532306671
  batch 650 loss: 0.9803389024734497
  batch 700 loss: 1.0060949730873108
  batch 750 loss: 0.9542230021953583
  batch 800 loss: 0.9604342722892761
  batch 850 loss: 0.9671772921085358
  batch 900 loss: 0.937837108373642
LOSS train 0.93784 valid 0.98514, valid PER 30.18%
EPOCH 20:
  batch 50 loss: 0.8968968224525452
  batch 100 loss: 0.9185955965518952
  batch 150 loss: 0.9361796259880066
  batch 200 loss: 0.9287436044216156
  batch 250 loss: 0.974552983045578
  batch 300 loss: 0.9480400800704956
  batch 350 loss: 0.9519338309764862
  batch 400 loss: 0.9394458818435669
  batch 450 loss: 0.9525245213508606
  batch 500 loss: 0.9679737901687622
  batch 550 loss: 0.9532495760917663
  batch 600 loss: 0.9173374938964843
  batch 650 loss: 0.9559339785575867
  batch 700 loss: 0.9505864834785461
  batch 750 loss: 0.9286844146251678
  batch 800 loss: 0.9483119475841523
  batch 850 loss: 0.9609036016464233
  batch 900 loss: 0.9322293436527253
LOSS train 0.93223 valid 0.99761, valid PER 29.96%
[2.8056920051574705, 2.3979599142074584, 1.9814942979812622, 1.7211083912849425, 1.4756883883476257, 1.4182968258857727, 1.3005389380455017, 1.2274419045448304, 1.1669914722442627, 1.1345589280128479, 1.1259733462333679, 1.0557694339752197, 1.0650029718875884, 1.077231945991516, 0.9882389497756958, 1.032467224597931, 0.9693696010112762, 0.9687570583820343, 0.937837108373642, 0.9322293436527253]
[tensor(2.8252, device='cuda:0', grad_fn=<DivBackward0>), tensor(2.3767, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.9299, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.7114, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.4815, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.3380, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.3014, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.2037, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.1466, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.1459, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.0876, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.1489, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.0453, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.0555, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.1011, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.0579, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.0075, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.0040, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9851, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9976, device='cuda:0', grad_fn=<DivBackward0>)]
Training finished in 5.0 minutes.
Model saved to checkpoints/20230122_154913/model_19
Loading model from checkpoints/20230122_154913/model_19
SUB: 20.30%, DEL: 8.65%, INS: 2.85%, COR: 71.06%, PER: 31.79%
