Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=256, concat=1, lr=0.001, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.2, beta1=0.9, beta2=0.999)
Total number of model parameters is 596008
EPOCH 1:
  batch 50 loss: 6.795436215400696
  batch 100 loss: 3.2282364797592162
  batch 150 loss: 3.0960841274261472
  batch 200 loss: 2.979775586128235
  batch 250 loss: 2.8585914182662964
  batch 300 loss: 2.7415875387191773
  batch 350 loss: 2.5522303438186644
  batch 400 loss: 2.4503572511672975
  batch 450 loss: 2.35872368812561
  batch 500 loss: 2.247073791027069
  batch 550 loss: 2.097889380455017
  batch 600 loss: 2.011753406524658
  batch 650 loss: 1.9203029227256776
  batch 700 loss: 1.9065855264663696
  batch 750 loss: 1.8297295451164246
  batch 800 loss: 1.7927576804161072
  batch 850 loss: 1.7580013632774354
  batch 900 loss: 1.7187734270095825
LOSS train 1.71877 valid 1.69640, valid PER 66.84%
EPOCH 2:
  batch 50 loss: 1.6909591603279113
  batch 100 loss: 1.6704090094566346
  batch 150 loss: 1.584979109764099
  batch 200 loss: 1.577738311290741
  batch 250 loss: 1.6413431119918824
  batch 300 loss: 1.5430112814903258
  batch 350 loss: 1.551141369342804
  batch 400 loss: 1.5262806415557861
  batch 450 loss: 1.5063821625709535
  batch 500 loss: 1.4985012245178222
  batch 550 loss: 1.4674357080459595
  batch 600 loss: 1.4667369389533997
  batch 650 loss: 1.426917564868927
  batch 700 loss: 1.436226305961609
  batch 750 loss: 1.427319917678833
  batch 800 loss: 1.3736718702316284
  batch 850 loss: 1.390165376663208
  batch 900 loss: 1.3657810258865357
LOSS train 1.36578 valid 1.33367, valid PER 46.48%
EPOCH 3:
  batch 50 loss: 1.2958411264419556
  batch 100 loss: 1.3560658943653108
  batch 150 loss: 1.334468228816986
  batch 200 loss: 1.2934553599357606
  batch 250 loss: 1.2957954025268554
  batch 300 loss: 1.2761134886741639
  batch 350 loss: 1.3211490893363953
  batch 400 loss: 1.2878647089004516
  batch 450 loss: 1.2562344896793365
  batch 500 loss: 1.2344782149791718
  batch 550 loss: 1.2514818978309632
  batch 600 loss: 1.1826108169555665
  batch 650 loss: 1.2033457374572754
  batch 700 loss: 1.2193446350097656
  batch 750 loss: 1.252297637462616
  batch 800 loss: 1.256325008869171
  batch 850 loss: 1.2114416921138764
  batch 900 loss: 1.207088531255722
LOSS train 1.20709 valid 1.18011, valid PER 38.56%
EPOCH 4:
  batch 50 loss: 1.194795433282852
  batch 100 loss: 1.1164537632465363
  batch 150 loss: 1.1515300357341767
  batch 200 loss: 1.1332910609245301
  batch 250 loss: 1.1599080896377563
  batch 300 loss: 1.1649296391010284
  batch 350 loss: 1.1472929728031158
  batch 400 loss: 1.0945967543125152
  batch 450 loss: 1.1112031209468842
  batch 500 loss: 1.1838768815994263
  batch 550 loss: 1.0926542258262635
  batch 600 loss: 1.0671476089954377
  batch 650 loss: 1.1437054777145386
  batch 700 loss: 1.1456530725955962
  batch 750 loss: 1.1120194053649903
  batch 800 loss: 1.1134285855293273
  batch 850 loss: 1.0929852318763733
  batch 900 loss: 1.0973011541366577
LOSS train 1.09730 valid 1.10441, valid PER 34.20%
EPOCH 5:
  batch 50 loss: 1.0514563012123108
  batch 100 loss: 1.047497352361679
  batch 150 loss: 1.0777166998386383
  batch 200 loss: 1.07835413813591
  batch 250 loss: 1.0292942118644715
  batch 300 loss: 1.0636036956310273
  batch 350 loss: 1.027557383775711
  batch 400 loss: 1.0240620827674867
  batch 450 loss: 1.0311968207359314
  batch 500 loss: 1.0174846076965331
  batch 550 loss: 1.0429732859134675
  batch 600 loss: 1.0604960560798644
  batch 650 loss: 1.0395435559749604
  batch 700 loss: 1.0268572735786439
  batch 750 loss: 1.0152231872081756
  batch 800 loss: 1.0468229007720948
  batch 850 loss: 1.0482703065872192
  batch 900 loss: 1.0203302943706511
LOSS train 1.02033 valid 1.04225, valid PER 32.59%
EPOCH 6:
  batch 50 loss: 1.0025055730342864
  batch 100 loss: 0.9869980311393738
  batch 150 loss: 0.9832956898212433
  batch 200 loss: 0.9541181421279907
  batch 250 loss: 0.976267374753952
  batch 300 loss: 0.9899318253993988
  batch 350 loss: 1.008422666788101
  batch 400 loss: 0.9848951172828674
  batch 450 loss: 0.9935530269145966
  batch 500 loss: 0.9459292411804199
  batch 550 loss: 0.9873218095302582
  batch 600 loss: 0.9596055245399475
  batch 650 loss: 0.9357493197917939
  batch 700 loss: 0.9606440556049347
  batch 750 loss: 0.9779802525043487
  batch 800 loss: 0.9725983095169067
  batch 850 loss: 0.9844747245311737
  batch 900 loss: 0.9841096270084381
LOSS train 0.98411 valid 1.00176, valid PER 30.96%
EPOCH 7:
  batch 50 loss: 0.9118738639354705
  batch 100 loss: 0.9638043987751007
  batch 150 loss: 0.890620025396347
  batch 200 loss: 0.9194796884059906
  batch 250 loss: 0.9826490890979767
  batch 300 loss: 0.9175997340679168
  batch 350 loss: 0.9421606278419494
  batch 400 loss: 0.9007003808021545
  batch 450 loss: 0.915468944311142
  batch 500 loss: 0.9146311068534851
  batch 550 loss: 0.9087968683242797
  batch 600 loss: 0.9162604463100433
  batch 650 loss: 0.8952780783176422
  batch 700 loss: 0.9549955475330353
  batch 750 loss: 0.9382172417640686
  batch 800 loss: 0.9268185555934906
  batch 850 loss: 0.893696700334549
  batch 900 loss: 0.9318891930580139
LOSS train 0.93189 valid 0.98779, valid PER 30.61%
EPOCH 8:
  batch 50 loss: 0.8920475542545319
  batch 100 loss: 0.8522510123252869
  batch 150 loss: 0.8963950598239898
  batch 200 loss: 0.8672223746776581
  batch 250 loss: 0.8930353689193725
  batch 300 loss: 0.8487866997718811
  batch 350 loss: 0.8678221702575684
  batch 400 loss: 0.8607887125015259
  batch 450 loss: 0.8848731291294097
  batch 500 loss: 0.8851046538352967
  batch 550 loss: 0.8927540957927704
  batch 600 loss: 0.8593617284297943
  batch 650 loss: 0.8578972494602204
  batch 700 loss: 0.8798988652229309
  batch 750 loss: 0.915668842792511
  batch 800 loss: 0.9161823463439941
  batch 850 loss: 0.8695917129516602
  batch 900 loss: 0.8637890899181366
LOSS train 0.86379 valid 0.94920, valid PER 28.90%
EPOCH 9:
  batch 50 loss: 0.816479731798172
  batch 100 loss: 0.8033032715320587
  batch 150 loss: 0.8183914136886596
  batch 200 loss: 0.8091093575954438
  batch 250 loss: 0.8208604478836059
  batch 300 loss: 0.8408929634094239
  batch 350 loss: 0.8384625089168548
  batch 400 loss: 0.8644571304321289
  batch 450 loss: 0.8724962294101715
  batch 500 loss: 0.8400262904167175
  batch 550 loss: 0.8512781918048858
  batch 600 loss: 0.859225914478302
  batch 650 loss: 0.8678932440280914
  batch 700 loss: 0.8506003904342652
  batch 750 loss: 0.8710787725448609
  batch 800 loss: 0.8766753888130188
  batch 850 loss: 0.8887637507915497
  batch 900 loss: 0.8242377781867981
LOSS train 0.82424 valid 0.92308, valid PER 28.44%
EPOCH 10:
  batch 50 loss: 0.7808677154779434
  batch 100 loss: 0.7945931315422058
  batch 150 loss: 0.8338632535934448
  batch 200 loss: 0.773738820552826
  batch 250 loss: 0.7989732336997986
  batch 300 loss: 0.805164053440094
  batch 350 loss: 0.8068309164047242
  batch 400 loss: 0.7712576061487197
  batch 450 loss: 0.789913330078125
  batch 500 loss: 0.7791446542739868
  batch 550 loss: 0.802427523136139
  batch 600 loss: 0.8080560088157653
  batch 650 loss: 0.8254943263530731
  batch 700 loss: 0.8030857062339782
  batch 750 loss: 0.8321835386753083
  batch 800 loss: 0.8223646128177643
  batch 850 loss: 0.8010690391063691
  batch 900 loss: 0.817747802734375
LOSS train 0.81775 valid 0.91365, valid PER 27.90%
EPOCH 11:
  batch 50 loss: 0.7569698011875152
  batch 100 loss: 0.741338176727295
  batch 150 loss: 0.7582785624265671
  batch 200 loss: 0.7341878616809845
  batch 250 loss: 0.7539550173282623
  batch 300 loss: 0.7589231717586518
  batch 350 loss: 0.7910756582021713
  batch 400 loss: 0.7541645729541778
  batch 450 loss: 0.7513881629705429
  batch 500 loss: 0.7616895818710328
  batch 550 loss: 0.784574248790741
  batch 600 loss: 0.7774278545379638
  batch 650 loss: 0.7926734840869903
  batch 700 loss: 0.8617063164710999
  batch 750 loss: 0.7704986560344697
  batch 800 loss: 0.7863045477867127
  batch 850 loss: 0.786277095079422
  batch 900 loss: 0.8056319546699524
LOSS train 0.80563 valid 0.90609, valid PER 28.16%
EPOCH 12:
  batch 50 loss: 0.7196169710159301
  batch 100 loss: 0.7145598989725113
  batch 150 loss: 0.7272035276889801
  batch 200 loss: 0.7464616310596466
  batch 250 loss: 0.7324949061870575
  batch 300 loss: 0.7682450246810913
  batch 350 loss: 0.744083793759346
  batch 400 loss: 0.7374506640434265
  batch 450 loss: 0.7415127861499786
  batch 500 loss: 0.7612500858306884
  batch 550 loss: 0.753439849615097
  batch 600 loss: 0.7582349920272827
  batch 650 loss: 0.749151172041893
  batch 700 loss: 0.7583754169940948
  batch 750 loss: 0.7657691037654877
  batch 800 loss: 0.7180622911453247
  batch 850 loss: 0.7529112493991852
  batch 900 loss: 0.7614062702655793
LOSS train 0.76141 valid 0.90284, valid PER 27.34%
EPOCH 13:
  batch 50 loss: 0.6775017786026001
  batch 100 loss: 0.7010215890407562
  batch 150 loss: 0.702343437075615
  batch 200 loss: 0.6583384770154953
  batch 250 loss: 0.7080558520555497
  batch 300 loss: 0.7463044881820678
  batch 350 loss: 0.6819074034690857
  batch 400 loss: 0.7091931539773941
  batch 450 loss: 0.7047662174701691
  batch 500 loss: 0.7097402393817902
  batch 550 loss: 0.745594642162323
  batch 600 loss: 0.7147093790769578
  batch 650 loss: 0.7118757510185242
  batch 700 loss: 0.7231636440753937
  batch 750 loss: 0.693902643918991
  batch 800 loss: 0.7276192510128021
  batch 850 loss: 0.7053478026390075
  batch 900 loss: 0.7217465960979461
LOSS train 0.72175 valid 0.90193, valid PER 27.64%
EPOCH 14:
  batch 50 loss: 0.6767327284812927
  batch 100 loss: 0.6632817435264587
  batch 150 loss: 0.6908304566144943
  batch 200 loss: 0.6756107956171036
  batch 250 loss: 0.6726694232225419
  batch 300 loss: 0.6612940949201583
  batch 350 loss: 0.7035909473896027
  batch 400 loss: 0.698505112528801
  batch 450 loss: 0.6595478397607804
  batch 500 loss: 0.6816159582138062
  batch 550 loss: 0.6882801228761672
  batch 600 loss: 0.6866831582784653
  batch 650 loss: 0.7181146746873855
  batch 700 loss: 0.6915632742643356
  batch 750 loss: 0.6895517498254776
  batch 800 loss: 0.6641288650035858
  batch 850 loss: 0.7076367664337159
  batch 900 loss: 0.7029290962219238
LOSS train 0.70293 valid 0.89135, valid PER 26.60%
EPOCH 15:
  batch 50 loss: 0.5968145072460175
  batch 100 loss: 0.632557515501976
  batch 150 loss: 0.6290638053417206
  batch 200 loss: 0.6653520548343659
  batch 250 loss: 0.6717329704761505
  batch 300 loss: 0.6520320266485214
  batch 350 loss: 0.6343122500181199
  batch 400 loss: 0.6625814789533615
  batch 450 loss: 0.6528885269165039
  batch 500 loss: 0.6282320320606232
  batch 550 loss: 0.6839371919631958
  batch 600 loss: 0.69211232483387
  batch 650 loss: 0.6656355500221253
  batch 700 loss: 0.6511653149127961
  batch 750 loss: 0.6810957843065262
  batch 800 loss: 0.6606871837377548
  batch 850 loss: 0.6746406090259552
  batch 900 loss: 0.6559247970581055
LOSS train 0.65592 valid 0.88396, valid PER 26.44%
EPOCH 16:
  batch 50 loss: 0.6177810269594193
  batch 100 loss: 0.6118047189712524
  batch 150 loss: 0.6061089086532593
  batch 200 loss: 0.6301775777339935
  batch 250 loss: 0.6179709458351135
  batch 300 loss: 0.6272343963384628
  batch 350 loss: 0.6209034347534179
  batch 400 loss: 0.6188850742578507
  batch 450 loss: 0.660897821187973
  batch 500 loss: 0.6373896634578705
  batch 550 loss: 0.623069759607315
  batch 600 loss: 0.6598260629177094
  batch 650 loss: 0.6505845016241074
  batch 700 loss: 0.603209770321846
  batch 750 loss: 0.635552603006363
  batch 800 loss: 0.6203513658046722
  batch 850 loss: 0.6523478019237519
  batch 900 loss: 0.6608150625228881
LOSS train 0.66082 valid 0.89537, valid PER 26.99%
EPOCH 17:
  batch 50 loss: 0.6267065358161926
  batch 100 loss: 0.5666000521183014
  batch 150 loss: 0.6119590306282043
  batch 200 loss: 0.5761335051059723
  batch 250 loss: 0.6126227390766144
  batch 300 loss: 0.5871111685037613
  batch 350 loss: 0.6189536702632904
  batch 400 loss: 0.620923285484314
  batch 450 loss: 0.602443636059761
  batch 500 loss: 0.6341701292991638
  batch 550 loss: 0.6081621223688125
  batch 600 loss: 0.6359599202871322
  batch 650 loss: 0.6104573142528534
  batch 700 loss: 0.6318255168199539
  batch 750 loss: 0.5980366492271423
  batch 800 loss: 0.6488326746225357
  batch 850 loss: 0.6295398503541947
  batch 900 loss: 0.6262232220172882
LOSS train 0.62622 valid 0.91671, valid PER 26.70%
EPOCH 18:
  batch 50 loss: 0.5632201033830643
  batch 100 loss: 0.550921658873558
  batch 150 loss: 0.6123081523180008
  batch 200 loss: 0.5745288312435151
  batch 250 loss: 0.5688198226690292
  batch 300 loss: 0.5496897339820862
  batch 350 loss: 0.5554127317667007
  batch 400 loss: 0.5553772181272507
  batch 450 loss: 0.6068630361557007
  batch 500 loss: 0.5873217636346817
  batch 550 loss: 0.6198772728443146
  batch 600 loss: 0.598385865688324
  batch 650 loss: 0.6139868760108947
  batch 700 loss: 0.6009392392635345
  batch 750 loss: 0.6089597064256668
  batch 800 loss: 0.6117893850803375
  batch 850 loss: 0.624545059800148
  batch 900 loss: 0.6076000326871872
LOSS train 0.60760 valid 0.90834, valid PER 26.46%
EPOCH 19:
  batch 50 loss: 0.542439547777176
  batch 100 loss: 0.5259123259782791
  batch 150 loss: 0.5288832718133927
  batch 200 loss: 0.5309065097570419
  batch 250 loss: 0.5618000739812851
  batch 300 loss: 0.5991074621677399
  batch 350 loss: 0.5870349597930908
  batch 400 loss: 0.5547467792034149
  batch 450 loss: 0.5460597515106201
  batch 500 loss: 0.5445542293787002
  batch 550 loss: 0.6096872586011887
  batch 600 loss: 0.5829072833061218
  batch 650 loss: 0.5996169346570969
  batch 700 loss: 0.6147161191701889
  batch 750 loss: 0.5768289911746979
  batch 800 loss: 0.5575099182128906
  batch 850 loss: 0.5762942385673523
  batch 900 loss: 0.558487000465393
LOSS train 0.55849 valid 0.90635, valid PER 25.81%
EPOCH 20:
  batch 50 loss: 0.5244629836082458
  batch 100 loss: 0.5426411384344101
  batch 150 loss: 0.5170170575380325
  batch 200 loss: 0.5173500269651413
  batch 250 loss: 0.5375902128219604
  batch 300 loss: 0.5590089356899262
  batch 350 loss: 0.49581621706485746
  batch 400 loss: 0.538888213634491
  batch 450 loss: 0.5369364446401597
  batch 500 loss: 0.5518076455593109
  batch 550 loss: 0.5721919471025467
  batch 600 loss: 0.5464333295822144
  batch 650 loss: 0.5728912156820297
  batch 700 loss: 0.5390991097688675
  batch 750 loss: 0.5471590566635132
  batch 800 loss: 0.576070853471756
  batch 850 loss: 0.5721934741735458
  batch 900 loss: 0.5781992161273957
LOSS train 0.57820 valid 0.92922, valid PER 26.02%
[1.7187734270095825, 1.3657810258865357, 1.207088531255722, 1.0973011541366577, 1.0203302943706511, 0.9841096270084381, 0.9318891930580139, 0.8637890899181366, 0.8242377781867981, 0.817747802734375, 0.8056319546699524, 0.7614062702655793, 0.7217465960979461, 0.7029290962219238, 0.6559247970581055, 0.6608150625228881, 0.6262232220172882, 0.6076000326871872, 0.558487000465393, 0.5781992161273957]
[tensor(1.6964, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.3337, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.1801, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.1044, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.0422, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.0018, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9878, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9492, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9231, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9136, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9061, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9028, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9019, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8913, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8840, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8954, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9167, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9083, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9064, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9292, device='cuda:0', grad_fn=<DivBackward0>)]
Training finished in 7.0 minutes.
Model saved to checkpoints/20230122_150737/model_15
Loading model from checkpoints/20230122_150737/model_15
SUB: 15.78%, DEL: 9.80%, INS: 2.39%, COR: 74.42%, PER: 27.97%
