Namespace(seed=123, train_json='train_fbank.json', val_json='dev_fbank.json', test_json='test_fbank.json', batch_size=4, num_layers=1, fbank_dims=23, model_dims=512, concat=1, lr=0.001, vocab='vocab_39.txt', report_interval=50, num_epochs=20, dropout=0.2, beta1=0.9, beta2=0.999)
Total number of model parameters is 2240552
EPOCH 1:
  batch 50 loss: 5.700879740715027
  batch 100 loss: 3.251281666755676
  batch 150 loss: 3.0693713188171388
  batch 200 loss: 2.825720582008362
  batch 250 loss: 2.5941024732589724
  batch 300 loss: 2.3899792766571046
  batch 350 loss: 2.218549590110779
  batch 400 loss: 2.160889148712158
  batch 450 loss: 2.037737534046173
  batch 500 loss: 1.9098049068450929
  batch 550 loss: 1.9430606484413147
  batch 600 loss: 1.876088855266571
  batch 650 loss: 1.7675820398330688
  batch 700 loss: 1.7444479703903197
  batch 750 loss: 1.6874826383590698
  batch 800 loss: 1.7290152716636658
  batch 850 loss: 1.6581533098220824
  batch 900 loss: 1.6123879504203797
LOSS train 1.61239 valid 1.62293, valid PER 50.34%
EPOCH 2:
  batch 50 loss: 1.6269025254249572
  batch 100 loss: 1.589079031944275
  batch 150 loss: 1.4982185173034668
  batch 200 loss: 1.4841714930534362
  batch 250 loss: 1.5288810682296754
  batch 300 loss: 1.4763441753387452
  batch 350 loss: 1.503029727935791
  batch 400 loss: 1.4208875513076782
  batch 450 loss: 1.4472171092033386
  batch 500 loss: 1.4208218455314636
  batch 550 loss: 1.4094457864761352
  batch 600 loss: 1.3742448711395263
  batch 650 loss: 1.341067979335785
  batch 700 loss: 1.3896823596954346
  batch 750 loss: 1.3464289939403533
  batch 800 loss: 1.322025842666626
  batch 850 loss: 1.3178732371330262
  batch 900 loss: 1.2895336747169495
LOSS train 1.28953 valid 1.26746, valid PER 40.57%
EPOCH 3:
  batch 50 loss: 1.219375410079956
  batch 100 loss: 1.2949059700965881
  batch 150 loss: 1.2902134537696839
  batch 200 loss: 1.219111179113388
  batch 250 loss: 1.2531924879550933
  batch 300 loss: 1.322675324678421
  batch 350 loss: 1.2485327911376953
  batch 400 loss: 1.2276033878326416
  batch 450 loss: 1.187577917575836
  batch 500 loss: 1.1880996143817901
  batch 550 loss: 1.1902129864692688
  batch 600 loss: 1.167968487739563
  batch 650 loss: 1.1956115329265595
  batch 700 loss: 1.2054563117027284
  batch 750 loss: 1.2167487525939942
  batch 800 loss: 1.2106237232685089
  batch 850 loss: 1.1374678218364715
  batch 900 loss: 1.164533772468567
LOSS train 1.16453 valid 1.13620, valid PER 35.54%
EPOCH 4:
  batch 50 loss: 1.1700461256504058
  batch 100 loss: 1.1644138157367707
  batch 150 loss: 1.1289249074459076
  batch 200 loss: 1.140852041244507
  batch 250 loss: 1.1665038287639617
  batch 300 loss: 1.1402354085445403
  batch 350 loss: 1.1722507441043855
  batch 400 loss: 1.1136787462234496
  batch 450 loss: 1.1364897179603577
  batch 500 loss: 1.2137596559524537
  batch 550 loss: 1.101688678264618
  batch 600 loss: 1.0498126804828645
  batch 650 loss: 1.1256352424621583
  batch 700 loss: 1.1240480315685273
  batch 750 loss: 1.1210632777214051
  batch 800 loss: 1.073241995573044
  batch 850 loss: 1.047804162502289
  batch 900 loss: 1.0856962096691132
LOSS train 1.08570 valid 1.09678, valid PER 34.20%
EPOCH 5:
  batch 50 loss: 1.02920254945755
  batch 100 loss: 1.0457721769809722
  batch 150 loss: 1.0750718009471893
  batch 200 loss: 1.062174369096756
  batch 250 loss: 1.0138256072998046
  batch 300 loss: 1.0381148147583008
  batch 350 loss: 0.9998235547542572
  batch 400 loss: 0.9745057332515716
  batch 450 loss: 0.9983645701408386
  batch 500 loss: 0.9836572742462159
  batch 550 loss: 1.021746108531952
  batch 600 loss: 0.9974124300479889
  batch 650 loss: 1.0176327812671662
  batch 700 loss: 1.0038577258586883
  batch 750 loss: 0.9714606142044068
  batch 800 loss: 1.038921571969986
  batch 850 loss: 1.0097617518901825
  batch 900 loss: 0.9710681390762329
LOSS train 0.97107 valid 1.03043, valid PER 32.35%
EPOCH 6:
  batch 50 loss: 0.9619601464271545
  batch 100 loss: 0.9799123287200928
  batch 150 loss: 0.9835699808597564
  batch 200 loss: 0.9506613349914551
  batch 250 loss: 0.9257587051391601
  batch 300 loss: 0.9641137373447418
  batch 350 loss: 0.9519446802139282
  batch 400 loss: 0.9261313915252686
  batch 450 loss: 0.9774183428287506
  batch 500 loss: 0.9158970904350281
  batch 550 loss: 0.9493174433708191
  batch 600 loss: 0.9413183891773224
  batch 650 loss: 0.923030344247818
  batch 700 loss: 0.900317701101303
  batch 750 loss: 0.9380429410934448
  batch 800 loss: 0.9247262668609619
  batch 850 loss: 0.9559336733818055
  batch 900 loss: 0.9438075983524322
LOSS train 0.94381 valid 0.93556, valid PER 29.56%
EPOCH 7:
  batch 50 loss: 0.8766493380069733
  batch 100 loss: 0.9301409339904785
  batch 150 loss: 0.8513601124286652
  batch 200 loss: 0.8593382239341736
  batch 250 loss: 0.9080190634727479
  batch 300 loss: 0.8742095112800599
  batch 350 loss: 0.8847526097297669
  batch 400 loss: 0.8633093810081482
  batch 450 loss: 0.8761966633796692
  batch 500 loss: 0.8740262508392334
  batch 550 loss: 0.8782870090007782
  batch 600 loss: 0.9050260269641877
  batch 650 loss: 0.8914862596988677
  batch 700 loss: 0.9168028271198273
  batch 750 loss: 0.8777901601791381
  batch 800 loss: 0.8828034663200378
  batch 850 loss: 0.8572957742214203
  batch 900 loss: 0.8700347208976745
LOSS train 0.87003 valid 0.94840, valid PER 29.82%
EPOCH 8:
  batch 50 loss: 0.8256989443302154
  batch 100 loss: 0.8383284783363343
  batch 150 loss: 0.8753622543811798
  batch 200 loss: 0.8605732977390289
  batch 250 loss: 0.8498868882656098
  batch 300 loss: 0.8502598869800567
  batch 350 loss: 0.8423191368579864
  batch 400 loss: 0.8608898031711578
  batch 450 loss: 0.954839459657669
  batch 500 loss: 0.8770733499526977
  batch 550 loss: 0.8609968316555023
  batch 600 loss: 0.8613419306278228
  batch 650 loss: 0.8722753918170929
  batch 700 loss: 0.9019253253936768
  batch 750 loss: 0.8855827879905701
  batch 800 loss: 0.8896125793457031
  batch 850 loss: 0.8765206229686737
  batch 900 loss: 0.8566973686218262
LOSS train 0.85670 valid 0.93340, valid PER 29.81%
EPOCH 9:
  batch 50 loss: 0.785932765007019
  batch 100 loss: 0.7892663085460663
  batch 150 loss: 0.7857171022891998
  batch 200 loss: 0.7783548700809478
  batch 250 loss: 0.7764172542095185
  batch 300 loss: 0.7903991031646729
  batch 350 loss: 0.7948297071456909
  batch 400 loss: 0.8101742815971374
  batch 450 loss: 0.8155194044113159
  batch 500 loss: 0.8028745102882385
  batch 550 loss: 0.7860970675945282
  batch 600 loss: 0.8182175636291504
  batch 650 loss: 0.8366433215141297
  batch 700 loss: 0.7879475688934326
  batch 750 loss: 0.8386253416538239
  batch 800 loss: 0.8729756283760071
  batch 850 loss: 0.8098797583580017
  batch 900 loss: 0.7996096050739289
LOSS train 0.79961 valid 0.89329, valid PER 28.36%
EPOCH 10:
  batch 50 loss: 0.7470932602882385
  batch 100 loss: 0.7464253747463226
  batch 150 loss: 0.7548556423187256
  batch 200 loss: 0.7232884043455123
  batch 250 loss: 0.7383264029026031
  batch 300 loss: 0.7392016756534576
  batch 350 loss: 0.7663187408447265
  batch 400 loss: 0.7310349857807159
  batch 450 loss: 0.7674861699342728
  batch 500 loss: 0.7529832547903061
  batch 550 loss: 0.7675423860549927
  batch 600 loss: 0.7456008243560791
  batch 650 loss: 0.807393182516098
  batch 700 loss: 0.8211676889657974
  batch 750 loss: 0.830620938539505
  batch 800 loss: 0.8183127212524414
  batch 850 loss: 0.7591450619697571
  batch 900 loss: 0.7531999516487121
LOSS train 0.75320 valid 0.89836, valid PER 27.82%
EPOCH 11:
  batch 50 loss: 0.6881512939929962
  batch 100 loss: 0.6901700949668884
  batch 150 loss: 0.6882319051027298
  batch 200 loss: 0.662814913392067
  batch 250 loss: 0.6919498598575592
  batch 300 loss: 0.6950677931308746
  batch 350 loss: 0.7272504907846451
  batch 400 loss: 0.6925969451665879
  batch 450 loss: 0.7204668945074082
  batch 500 loss: 0.7372041481733322
  batch 550 loss: 0.7453155374526977
  batch 600 loss: 0.7585630279779434
  batch 650 loss: 0.7377613294124603
  batch 700 loss: 0.7965900552272797
  batch 750 loss: 0.7358002835512161
  batch 800 loss: 0.7459107714891434
  batch 850 loss: 0.7312319588661194
  batch 900 loss: 0.749076042175293
LOSS train 0.74908 valid 0.88012, valid PER 27.74%
EPOCH 12:
  batch 50 loss: 0.6597930371761322
  batch 100 loss: 0.6493314331769944
  batch 150 loss: 0.6764419949054719
  batch 200 loss: 0.6640748691558838
  batch 250 loss: 0.6670628970861435
  batch 300 loss: 0.6928253585100174
  batch 350 loss: 0.6685016423463821
  batch 400 loss: 0.7145443868637085
  batch 450 loss: 0.6597451430559158
  batch 500 loss: 0.6971256393194198
  batch 550 loss: 0.6612573754787445
  batch 600 loss: 0.6954642707109451
  batch 650 loss: 0.6905041462182999
  batch 700 loss: 0.7032365345954895
  batch 750 loss: 0.6904727375507355
  batch 800 loss: 0.6668444812297821
  batch 850 loss: 0.6762057548761368
  batch 900 loss: 0.6977801847457886
LOSS train 0.69778 valid 0.87162, valid PER 27.00%
EPOCH 13:
  batch 50 loss: 0.5868306571245193
  batch 100 loss: 0.632749462723732
  batch 150 loss: 0.627229790687561
  batch 200 loss: 0.6059199124574661
  batch 250 loss: 0.6202199965715408
  batch 300 loss: 0.650668551325798
  batch 350 loss: 0.6195101284980774
  batch 400 loss: 0.6435753858089447
  batch 450 loss: 0.6785372447967529
  batch 500 loss: 0.6497130066156387
  batch 550 loss: 0.6637719666957855
  batch 600 loss: 0.6525667941570282
  batch 650 loss: 0.6447150725126266
  batch 700 loss: 0.6479298681020736
  batch 750 loss: 0.6219169563055038
  batch 800 loss: 0.6622450947761536
  batch 850 loss: 0.6334862536191941
  batch 900 loss: 0.6737011498212815
LOSS train 0.67370 valid 0.85295, valid PER 26.65%
EPOCH 14:
  batch 50 loss: 0.5804321998357773
  batch 100 loss: 0.5751151967048646
  batch 150 loss: 0.6136855113506318
  batch 200 loss: 0.6147000497579574
  batch 250 loss: 0.6076492756605149
  batch 300 loss: 0.5928606778383255
  batch 350 loss: 0.6255007761716843
  batch 400 loss: 0.6476677948236466
  batch 450 loss: 0.5914558243751525
  batch 500 loss: 0.6416533917188645
  batch 550 loss: 0.6363520401716233
  batch 600 loss: 0.6077711999416351
  batch 650 loss: 0.6325551128387451
  batch 700 loss: 0.6265422022342682
  batch 750 loss: 0.6303550094366074
  batch 800 loss: 0.6103550589084625
  batch 850 loss: 0.6290232956409454
  batch 900 loss: 0.6166478192806244
LOSS train 0.61665 valid 0.86097, valid PER 26.67%
EPOCH 15:
  batch 50 loss: 0.5497679388523102
  batch 100 loss: 0.5629602742195129
  batch 150 loss: 0.5772302901744842
  batch 200 loss: 0.5752941864728928
  batch 250 loss: 0.5698434334993362
  batch 300 loss: 0.5720563173294068
  batch 350 loss: 0.5721300971508027
  batch 400 loss: 0.5916921192407608
  batch 450 loss: 0.5575835907459259
  batch 500 loss: 0.5472277492284775
  batch 550 loss: 0.5927048122882843
  batch 600 loss: 0.5907170659303665
  batch 650 loss: 0.557059987783432
  batch 700 loss: 0.5671018826961517
  batch 750 loss: 0.6091014814376831
  batch 800 loss: 0.6051304775476456
  batch 850 loss: 0.5963255131244659
  batch 900 loss: 0.5592065608501434
LOSS train 0.55921 valid 0.88031, valid PER 26.95%
EPOCH 16:
  batch 50 loss: 0.5118634349107742
  batch 100 loss: 0.5034614783525467
  batch 150 loss: 0.5274373704195022
  batch 200 loss: 0.5457642883062362
  batch 250 loss: 0.543271045088768
  batch 300 loss: 0.5523967856168747
  batch 350 loss: 0.5366683954000473
  batch 400 loss: 0.5268116563558578
  batch 450 loss: 0.5504679840803146
  batch 500 loss: 0.5682419538497925
  batch 550 loss: 0.546438279747963
  batch 600 loss: 0.5746799647808075
  batch 650 loss: 0.562960854768753
  batch 700 loss: 0.5217508053779603
  batch 750 loss: 0.5559248560667038
  batch 800 loss: 0.5456335961818695
  batch 850 loss: 0.5649525505304337
  batch 900 loss: 0.5639276129007339
LOSS train 0.56393 valid 0.88478, valid PER 26.52%
EPOCH 17:
  batch 50 loss: 0.5204495453834533
  batch 100 loss: 0.4497168046236038
  batch 150 loss: 0.4937381047010422
  batch 200 loss: 0.4763265144824982
  batch 250 loss: 0.5134294545650482
  batch 300 loss: 0.5022282248735428
  batch 350 loss: 0.5334895718097686
  batch 400 loss: 0.543630011677742
  batch 450 loss: 0.5210141694545746
  batch 500 loss: 0.5414273273944855
  batch 550 loss: 0.5372786659002304
  batch 600 loss: 0.5909408497810363
  batch 650 loss: 0.5655958342552185
  batch 700 loss: 0.5482619762420654
  batch 750 loss: 0.537612971663475
  batch 800 loss: 0.5677906316518784
  batch 850 loss: 0.5397750508785247
  batch 900 loss: 0.5489065366983413
LOSS train 0.54891 valid 0.90132, valid PER 26.31%
EPOCH 18:
  batch 50 loss: 0.4591977399587631
  batch 100 loss: 0.433013442158699
  batch 150 loss: 0.5247265928983689
  batch 200 loss: 0.4867714625597
  batch 250 loss: 0.4524749273061752
  batch 300 loss: 0.46777220904827116
  batch 350 loss: 0.45929861307144165
  batch 400 loss: 0.46383927643299105
  batch 450 loss: 0.5161523252725602
  batch 500 loss: 0.5261749303340912
  batch 550 loss: 0.5248404002189636
  batch 600 loss: 0.5075567656755448
  batch 650 loss: 0.5001942348480225
  batch 700 loss: 0.49457139015197754
  batch 750 loss: 0.5306009781360627
  batch 800 loss: 0.549983924627304
  batch 850 loss: 0.5696402591466904
  batch 900 loss: 0.5226824876666069
LOSS train 0.52268 valid 0.91482, valid PER 26.48%
EPOCH 19:
  batch 50 loss: 0.42090749502182007
  batch 100 loss: 0.4256079179048538
  batch 150 loss: 0.43138524174690246
  batch 200 loss: 0.4159158992767334
  batch 250 loss: 0.44429720044136045
  batch 300 loss: 0.4663576698303223
  batch 350 loss: 0.46768021941184995
  batch 400 loss: 0.4575136119127274
  batch 450 loss: 0.43122657418251037
  batch 500 loss: 0.42648755490779877
  batch 550 loss: 0.4712514328956604
  batch 600 loss: 0.4731869897246361
  batch 650 loss: 0.48863502383232116
  batch 700 loss: 0.4933964121341705
  batch 750 loss: 0.4784667605161667
  batch 800 loss: 0.46962584733963014
  batch 850 loss: 0.4835582369565964
  batch 900 loss: 0.47265534043312074
LOSS train 0.47266 valid 0.92867, valid PER 26.50%
EPOCH 20:
  batch 50 loss: 0.38535100847482684
  batch 100 loss: 0.39678680211305617
  batch 150 loss: 0.41825008660554885
  batch 200 loss: 0.4143882590532303
  batch 250 loss: 0.4259013405442238
  batch 300 loss: 0.4316102129220962
  batch 350 loss: 0.40381340593099596
  batch 400 loss: 0.4334062761068344
  batch 450 loss: 0.4356128811836243
  batch 500 loss: 0.44735957860946657
  batch 550 loss: 0.47173895239830016
  batch 600 loss: 0.47207864105701447
  batch 650 loss: 0.4799039059877396
  batch 700 loss: 0.4529817092418671
  batch 750 loss: 0.42968767523765566
  batch 800 loss: 0.507187739610672
  batch 850 loss: 0.516389716565609
  batch 900 loss: 0.47274074733257293
LOSS train 0.47274 valid 0.93993, valid PER 27.04%
[1.6123879504203797, 1.2895336747169495, 1.164533772468567, 1.0856962096691132, 0.9710681390762329, 0.9438075983524322, 0.8700347208976745, 0.8566973686218262, 0.7996096050739289, 0.7531999516487121, 0.749076042175293, 0.6977801847457886, 0.6737011498212815, 0.6166478192806244, 0.5592065608501434, 0.5639276129007339, 0.5489065366983413, 0.5226824876666069, 0.47265534043312074, 0.47274074733257293]
[tensor(1.6229, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.2675, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.1362, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.0968, device='cuda:0', grad_fn=<DivBackward0>), tensor(1.0304, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9356, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9484, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9334, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8933, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8984, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8801, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8716, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8529, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8610, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8803, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.8848, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9013, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9148, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9287, device='cuda:0', grad_fn=<DivBackward0>), tensor(0.9399, device='cuda:0', grad_fn=<DivBackward0>)]
Training finished in 9.0 minutes.
Model saved to checkpoints/20230122_151059/model_13
Loading model from checkpoints/20230122_151059/model_13
SUB: 16.22%, DEL: 9.45%, INS: 2.17%, COR: 74.32%, PER: 27.85%
